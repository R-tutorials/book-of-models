# Data


### Feature & Target Transformations

Transforming variables can provide a few benefits in modeling, whether applied to the target, covariates, or both, and should regularly be used for most models. Some of these benefits include:

- Interpretable intercepts
- More comparable covariate effects
- Faster estimation
- Easier convergence
- Help with heteroscedasticity

For example, merely centering predictor variables, i.e. subtracting the mean, provides a more interpretable intercept that will fall within the actual range of the target variable, telling us what the value of the target variable is when the covariates are at their means (or reference value if categorical). But even if easier interpretation isn't a major concern, variable transformations can help with convergence and speed up estimation, so can always be of benefit.

#### Numeric variables

The following table shows the interpretation of some very common transformations applied to numeric variables- logging and standardization, (i.e. standardizing to mean zero, standard deviation one).


```{r}
#| echo: false
#| eval: true
#| label: tbl-transformation
#|
# gt doesn't seem to have an actual working answer to latex within cells, though kabel_extra and modelsummary appear to handle it as expected, at least if you just run it. Rendering seems to fail.
# tibble(
#     Target = c("y", "y", "log(y)", "log(y)", "y", "scale(y)", "scale(y)"),
#     Feature = c("x", "log(x)", "x", "log(x)", "scale(x)", "x", "scale(x)"),
#     Interpretation = c(
#         "$\\Delta y = \\beta\\Delta x$",
#         "$\\Delta y \\approx (\\beta/100)\\%\\Delta x$",
#         "$\\%\\Delta y \\approx 100\\cdot \\beta\\%\\Delta x$",
#         "$\\%\\Delta y = \\beta\\%\\Delta x$",
#         "$\\Delta y =  \\beta\\sigma\\Delta x$",
#         "$\\sigma\\Delta y =  \\beta\\Delta x$",
#         "$\\sigma\\Delta y =  \\beta\\sigma\\Delta x$"
#     ),
# ) |>
#     modelsummary::datasummary_df()
# kableExtra::kable()
# christ what year is this?
tibble(
    Target = c("y", "log(y)", "log(y)", "y", "scale(y)"),
    Feature = c("x", "x", "log(x)", "scale(x)", "scale(x)"),
    `Change in X` = c("1 unit", "1 unit", "1 % change", "1 standard deviation", "1 standard deviation"),
    `Change in Y` = c("\U03B2 unit", "100 * (exp(\U03B2) -1) %", "\U03B2 % change", "\U03B2 unit", "\U03B2 standard deviation"),
    Benefits = c("Interpretation", "Heteroscedasticity in y", "Interpretation, deal with feature extremes", "Interpretation, estimation", "Interpretation, estimation"),
) |>
    gt()

```

For example, it is very common to use **standardized** variables, also called **normalizing**, or simply scaling. If $y$ and $x$ are both standardized, a one unit (i.e. one standard deviation) change in $x$ leads to a $\beta$ standard deviation change in $y$. Again, if $\beta$ was .5, a standard deviation change in $x$ leads to a half standard deviation change in $y$. In general, there is nothing to lose by standardizing, so you should employ it often.

Another common transformation, particularly in machine learning, is the **min-max normalization**, changing variables to range from some minimum to some maximum, which is almost always zero to one.  This can make numeric and categorical indicators more comparable, and can help with convergence and speed up estimation. 

#### Categorical variables

A raw character string is not an analyzable unit, so character strings and labeled variables like factors must be converted for analysis to be conducted on them. For categorical variables, we can employ what is called **effects coding** to test for specific types of group differences.  Far and away the most common approach is called **dummy coding** or **one-hot encoding**. In these situations we create columns for each category, and the value of the column is 1 if the observation is in that category, and 0 otherwise.  Here is a one-hot encoded version of the `season` feature.

```{r}
#| echo: false

model.matrix(rating ~ -1 + season, df_reviews) |>
    as_tibble() |>
    bind_cols(df_reviews |> select(season)) |>
    head() |>
    gt()
```

:::{.callout-note}
When doing statistical models, all relevant information is incorporated in k-1 groups, so one category will be dropped. For example, in a linear model, the intercept is the mean of the target for the dropped category, and the coefficients for the other categories are the difference between the mean for the dropped, a.k.a. **reference**, category and the mean the category being considered. As an example, in the case of the `season` feature, if the dropped category is `winter`, the intercept tells us the mean rating for `winter`, and the coefficients for the other categories are the difference between the value for `winter` and the mean of the target for the included category. For other modeling approaches, all categories are included, and the model will learn the best way to use them, and may even only consider some or one of them at a particular iteration of estimation.
:::

 Another important way to encode categorical information is through an **embedding**.  This is a way of representing the categories as a vector of numbers, at which point the embedding feature is used in the model like anything else. The way to do this usually involves a model itself, one that learns the best way to represent the categories as numbers. This is a very common approach in deep learning, and can be done simultaneously with the rest of the model, but can potentially be used in any modeling situation as an initial data processing step.



