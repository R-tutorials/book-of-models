[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Models by Example",
    "section": "",
    "text": "1 Preface\nHi there, this is our book! It’s about models great and small, and we hope you’ll find some useful things here. Our main goal is to provide a resource for people who are learning about models, and we hope that it will be useful for people who are just starting out, as well as for people who are already familiar with models but want to learn from a different perspective. Whether you’re a machine learning master that would like a little bit more statistical nuance, or an academic trying to dive into the world of machine learning, we hope you’ll find it helpful.\nYou’ll definitely want tok have some familiarity with R or Python, and some basic knowledge of statistics will be helpful. We’ll try to explain things as we go, but we won’t be able to cover everything. If you’re looking for a good introduction to R, we recommend R for Data Science or the Python Data Science Handbook for Python. Beyond that, we’ll try to provide the context you need. We’re not here to make you an expert, just to help you get acquainted with the world of models."
  },
  {
    "objectID": "index.html#acknowledgments",
    "href": "index.html#acknowledgments",
    "title": "Models by Example",
    "section": "1.1 Acknowledgments",
    "text": "1.1 Acknowledgments\nA lot of people helped when writing the book…"
  },
  {
    "objectID": "introduction.html#what-is-this-book",
    "href": "introduction.html#what-is-this-book",
    "title": "1  Introduction",
    "section": "1.1 What Is This Book?",
    "text": "1.1 What Is This Book?\nThis book is a practical resource, something we hope you can refer to for a quick overview of a specific modeling technique, a reminder of something you’ve seen before, or perhaps a sneak peak into some modeling details. The text is focused on a few statistical and machine learning approaches that are widely employed, and specifically those which form the basis for most other models in use. Believe it or not, whether a lowly t-test or a complex neural network, there is a tie that binds. We hope to help you understand some of the core modeling principles, and how the simpler models can be extended and applied to a wide variety of data scenarios.\nOur approach here is first and foremost a practical one, as models themselves are just tools to help us reach a goal. If a model doesn’t work in the world, it’s not very useful. But modeling is often a delicate balance of interpretation and prediction, and each data situation is unique in some way, requiring a bespoke approach. What works well in one setting may be poor in another, and what may be the state of the art may only be marginally better than a notably simpler approach that is far more interpretable. In addition, complexities arise even in an otherwise deceptively simple application. However, if you have the core understanding of the techniques that lie at the heart of many models, you’ll automatically have many more tools at your disposal to tackle the problems you face, and be more comfortable with choosing the best for your needs."
  },
  {
    "objectID": "introduction.html#who-should-use-this-book",
    "href": "introduction.html#who-should-use-this-book",
    "title": "1  Introduction",
    "section": "1.2 Who Should Use This Book?",
    "text": "1.2 Who Should Use This Book?\nThis book is intended for every type of data dabbler, no matter what part of the data world you call home. If you consider yourself a data scientist, a business analyst, or a statistical hobbyist, you already know that the best part of a good dive into data is the modeling. But whatever your data persuasion, models give us the possibility to answer questions, make predictions, and understand what we’re interested in a little bit better. And no matter who you are, it isn’t always easy to understand how the models work. Even when you do get a good grasp of a modeling approach, it can still get complicated, and there are a lot of details to keep track of. In other cases, maybe you just have other things going on in your life and have forgotten a few things. We find that it’s always good to remind yourself of the basics! So if you’re just interested in data and hoping to understand it a little better, then it’s likely you’ll find something useful in this book!\nYour humble authors have struggled mightily themselves throughout the course of their data history, and still do! We were initially taught by those that weren’t exactly experts, and often found it difficult to get a good grasp of statistical modeling and machine learning. We’ve had to learn how to use the tools, how to interpret the results, and possibly the most difficult, how to explain what we’re doing to others! We’ve forgotten a lot, confused ourselves, and made some happy accidents in the process. That’s okay! Our goal here is to help you avoid some of those pitfalls, help you understand the basics of how models work, and get a sense of how most modeling endeavors have a lot of things in common.\nWhether you enthusiastically pour over formulas and code, or prefer to skip over them, we promise that you don’t need to memorize a formula to get a good understanding of modeling and related issues. We are the first to admit that we have long dumped the ability to pull formulas out of our brain folds1; however, knowing how those individual pieces work together only helps to deepen your understanding of the model. Typically using code puts the formula into more concrete terms that you can then use in different ways to solidify and expand your knowledge. Sometimes you just need a reminder or want to see what function you’d use. And often, the visualization will reveal even more about what’s going than the formula or the code. In short, there are a lot of tools at your disposal to help learn modeling in a way that works for you. We hope that anyone that would be interested in the book will find a way to learn things in a manner that suits them best.\nThere is bit of a caveat. We aren’t going to teach you basic statistics or how to program in R or Python. Although there is a good chance you will learn some of it here, you’ll have an easier time if you have a very basic understanding of statistics and some familiarity with coding. We will provide some resources for you to learn more about these topics, but we won’t be covering them in detail. The (appendix?) will provide some more information about prerequisites or just stuff that would be good to know. However, we really aren’t assuming a lot of conceptual knowledge, and are, if anything, assuming that whatever knowledge you have may be a bit loose or fuzzy. That’s okay!"
  },
  {
    "objectID": "introduction.html#what-can-you-expect",
    "href": "introduction.html#what-can-you-expect",
    "title": "1  Introduction",
    "section": "1.3 What Can You Expect?",
    "text": "1.3 What Can You Expect?\nFor each model that we cover, you can expect the following in terms of content:\n\nOverview\n\nWhy it’s useful\nConceptual example and interpretation\nWhere the model lies in the grand scheme of topics that we will cover\n\nKey ideas and concepts\n\nBrief summary and definition list of concepts or terms\n\nDemonstration with data, code, results, and visualizations\n\nThe data will often be simulations as that opens doors for further understanding, or a dataset that hopefully is a little more interesting than mtcars or iris.\nThe demonstrations will provide you the opportunity to get your hands as dirty as you wish. We will present the code in two ways:\n\nstandard functions (e.g., lm in R, ols in statsmodels for Python)\nthe steps to recreate the estimation process on your own (or at least something a little more hands-on)\n\n\nCommentary, cautions, and where to explore next\n\nWe are taking this approach for one reason: so that you can go as deep as you wish. If you are looking for a quick tutorial on helpful models, then you might not find yourself going any deeper than the standard functions (or even getting into the code at all). If you want to really dive into these models, then you might find yourself working through the complete steps. Another approach is to allow yourself some time between the standard functions and complete steps. You could work through the standard functions of every chapter, give it some time to marinate, and then work back through the complete steps. While we certainly recommend working through the chapters in order, we want to give you the flexibility to choose your own depth within each.\nWe hope that this book can serve as a “choose your own adventure” statistical reference. Whether you want a surface-level understanding, a deeper dive, or just want to be able to understand what the analysts in your organization are talking about, you will find value in this book. While we assume that you have a basic familiarity with coding, that doesn’t mean that you need to work through every line of code to understand the fundamental principles and use cases of every model."
  },
  {
    "objectID": "introduction.html#which-language",
    "href": "introduction.html#which-language",
    "title": "1  Introduction",
    "section": "1.4 Which Language?",
    "text": "1.4 Which Language?\nYou’ve probably noticed most books, blogs, and courses on data science choose R or Python. While many individuals often take an opinionated approach towards teaching and using one over the other, we eschew dogmatic approaches and language flame wars. R and Python are both great languages (and equally flawed in unique ways), and it is advantageous to be knowledgeable of both, even if you focus on one specifically, as they are the most popular languages for statistical modeling and machine learning. We use both extensively in our own work for teaching, personal use, and production level code, and have found both are up to whatever task you have in mind. Throughout this book, we will be presenting demonstrations in both R and Python, and you can use both or take your pick, but we want to leave that choice up to you. Our goal isn’t to obscure the ideas behind packages and specialty functions or tell you why one languages is superior to the other (they aren’t), but to show you the most basic functions behind big model ideas.\nWhile we want to provide you choice, we truly hope that displaying both languages can help people to “convert” from one to the other. We have spent countless hours, slumped over our computers, debugging errors and figuring things out. If we can take away one source of pain for you, that would be great! We’d like to consider this as a resource for the R user, who knows exactly what they want to do in R, but could use a little help translating their R knowledge to Python; we’d also like this book to be a resource for the Python user, who sees the value in R’s statistical modeling abilities."
  },
  {
    "objectID": "introduction.html#choose-your-own-adventure",
    "href": "introduction.html#choose-your-own-adventure",
    "title": "1  Introduction",
    "section": "1.5 Choose Your Own Adventure",
    "text": "1.5 Choose Your Own Adventure\nAs an example of how things will go, let’s look at the different ways we might express a relationship between two variables. If you want just a little bit of background, you can read through the Overview and Key Ideas. If you want to see the easy way to use the model, you can work through the Standard Function section. If you want to see the method from start to finish, feel free to work through the Roll Your Own section. If all of that sounds like a lot of work and you’d rather look at the pretty pictures, that’s fine too2, just jump to the Visualization section.\n\n1.5.1 Overview\nCorrelations provide a means of understanding how, and if, two or more things are related. For any two variables or features, we can estimate a single value that signals the strength and direction of the relationship between them. Despite limitations, correlations can be a great way to get a quick understanding of the relationship between two features. One variable could be temperature while another variable is the number of ice cream cones sold, or one is the number of hours spent studying and another variable is the grade on a test, maybe the number of hours spent watching TV versus the number of hours spent exercising. The correlation value will give us information that is similarly interpretable in each case.\n\n\n1.5.2 Key Ideas\nHere are key ideas to consider for understanding correlation.\n\nVariance: Two variables must vary if they are to co-vary\nCovariance: Joint variability of two variables, i.e. how they vary together\nInterpretation: Covariance is hard to interpret because the variables are typically on different scales\nCorrelation: Correlation is a standardized covariance, so it is easier to interpret\n\nAnother way to think about correlation is through the lens of variance covariance. Covariance is a measure of the joint variability of two variables, i.e. how they vary together. If two variables are highly correlated, then they have a high degree of covariance, i.e. they vary together in a meaningful way.\n\n\n1.5.3 Demonstration\nTo demonstrate correlation, we could start by formally defining it. Here is a formula for the Pearson-Product-Moment correlation coefficient. What everyone typically just calls “correlation” or Pearson’s r is actually the Pearson Product-Moment Correlation – that is a lot of words, though, so we will just go with correlation. As we mentioned earlier, we want to give you the choice to dive in as deep as you want.\n\\[\n\\rho = \\frac{\\Sigma(x_i-\\bar{x})(y_i-\\bar{y})}{\\sqrt{\\Sigma(x_i-\\bar{x})^2\\Sigma(y_i-\\bar{y})^2}}\n\\tag{1.1}\\]\nIn words, starting with the numerator, we are going to subtract the mean of x from every observation of x, do the same for y, multiply them together, and then sum them to produce a single value. If you’ve taken a statistics class before, you might have seen this part of the formula when talking about covariance – the joint variability of two variables. In fact, if you just divide the numerator by n-1 you’d get the covariance value. Covariance values, though, are not standardized; therefore, they are a bit tough to interpret. You will get a sign indicating relationship (just like a correlation) and the magnitude tells you the strength of the relationship, but it is hard to get your head around how much stronger a covariance of 4.00 is compared to a covariance of 0.28.\nThis is where the denominator comes into play. To put it into real terms, we will subtract the mean of x from every observation of x (i.e. get the deviations of x), square those values, and sum those values to produce a single value. We will do the same for y, multiply those results, and then take the square root of that product. The denominator might also look familiar to some. The individual pieces- \\(\\Sigma(x_i-\\bar{x})^2\\) and \\(\\Sigma(y_i-\\bar{y})^2\\) - are computing the variance of x and y. Taking the square root of variance gives us the standard deviation. Basically, we’re getting a combined standard deviation to scale the covariance. Using the standard deviation will mitigate the effects of x and y potentially being on different scales; in other words, we want to know how much two variables move together, even if the means and typical movement (deviation) might be very different.\nHere is a simplification, depicting correlation as a standardized covariance.\n\\[\n\\rho = \\frac{cov(x,y)}{\\sigma_x\\sigma_y}\n\\tag{1.2}\\]\nwhere \\(cov(x,y)\\) is the covariance between \\(x\\) and \\(y\\), and \\(\\sigma_x\\) and \\(\\sigma_y\\) are the standard deviations of \\(x\\) and \\(y\\), respectively. So putting it all together, we are simply looking at the ratio of raw covariance (i.e., how much two variables “move” together) and standard deviation (i.e., the amount of dispersion within a variable). Just as we mentioned earlier, simpler concepts often tie models together! What might look like a tricky formula, really just starts with means and deviations, variance and covariance!\nAs a reminder, this correlation will give us an idea about the linear relationship between two continuous variables; we will get a value between -1 and 1, with values closer to 0 indicating that there is no linear relationship between the two variables. As values get closer to 1, we have a positive correlation – as values for one variable increase, values for the other variable tend to increase along with it. As the correlation gets closer to -1, we have a negative correlation – as values for one variable increase, values for the other variable typically decrease. Correlations can be useful for quickly exploring linear relationships, but let’s not get too excited about it – they aren’t going to help you answer any big questions! We also don’t want to get too carried away with “statistical significance” yet – once samples get large, even small correlations become “significant”. Instead, just use correlations to explore the patterns within your data, start getting ideas about interesting relationships that you might find, and leave worries about significance for people with more time on their hands.\n\n1.5.3.1 Code\nIt’s often easier to understand a concept by seeing it in action. So let’s start by creating some data. We’ll create a variable x, then make a y that will have a linear relationship with it, but also have some random noise. We’ll then plot the two variables to see what they look like. To help your understanding, fiddle with the knobs noted.\n\nRPython\n\n\n\nset.seed(seed = 1001)\nN &lt;- 500\nx &lt;- rnorm(n = N, mean = 0, sd = 1)\n\n# Fiddle with the .5 and .75.\n# The first can also be negative if you like!\ny &lt;- .5 * x + rnorm(n = N, mean = 0, sd = .75)\n\n\n\n\nimport numpy as np\n\nnp.random.seed(seed = 1001)\nN = 500\nx = np.random.normal(loc = 0, scale = 1, size = N)\n\n# Fiddle with the .5 and .75. \n# The first can also be negative if you like!\ny = .5*x + np.random.normal(loc = 0, scale = .75, size = N) \n\n\n\n\nNow check out the plot of those two values:\n\n\n\n\n\nScatterplot of two variables.\n\n\n\n\nRemember that correlation is testing for the presence of a linear relationship, with -1 indicating a perfect negative relationship, 1 indicating a perfect positive relationship, and 0 indicating no relationship. Before we see the actual correlation value for these two variables, take a guess as to what value we are going to get!\nBefore we create our own function, we can use R’s cor function or numpy’s corrcoef function. You should get something around 0.6.\n\n# Results for R and Python will be slightly\n# different due to different random number generators\ncor(x, y)\nnp.corrcoef(x, y)\n\n\n\n[1] 0.557\n\n\nWhen you guessed the value, were you close? If so, congrats! If not, try fiddling with those knobs noted until things get a little clearer. But now that we already know the answer, let’s make sure that we can get the same answer by working through the formula via code. The following takes that initial formula approach and turns it into a function that we can use to compute the correlation between any two variables. We’ll then use that function to compute the correlation between our x and y variables.\n\nRPython\n\n\n\nmy_cor = function(x, y) {\n    # First, we need to compute the averages for x and y.\n    # The rest follows the formula.\n    x_bar = mean(x)\n    y_bar = mean(y)\n\n    numerator = sum((x - x_bar) * (y - y_bar))\n\n    denominator = sqrt(\n        sum((x - x_bar)^2) * sum((y - y_bar)^2)\n    )\n\n    numerator / denominator\n}\n\n# using the builtin functions\nmy_cor2 = function(x, y) {\n    cov(x, y) / (sd(x) * sd(y))\n}\n\nmy_cor(x, y)\n\n[1] 0.557\n\n\n\n\n\ndef my_cor(x, y):\n  # First, we need to compute the averages for x and y.\n  # The rest follows the formula.\n    x_bar = np.mean(x)\n    y_bar = np.mean(y)\n    \n    numerator = np.sum((x - x_bar) * (y - y_bar))\n    \n    denominator = np.sqrt(\n      np.sum((x - x_bar)**2) * np.sum((y - y_bar)**2)\n    )\n\n    # We will finish by dividing the numerator by the \n    # denominator.\n    # This will ensure that we have a value between -1 and 1.\n    return(numerator / denominator)\n\nmy_cor(x, y)\n\n0.5390318454354402\n\n\n\n\n\nIt doesn’t matter which language we use, the steps are largely the same when we break it down into the individual pieces!\n\n\n\n1.5.4 Visualization\nA long time ago, in a land far away, the authors of this book worked together to help clients traverse the forests of data to reach their modeling goals. While there were many great learning opportunities along the way, working with clients showed us the kinds of help that people really needed in adventuring with data and models. Even so, there were many requests that made us grimace, and one stood atop Mount Ridiculous: to produce a correlation matrix with 115 variables and export that matrix to a spreadsheet. We still don’t recommend such shenanigans, but there are ways to try and understand correlation matrices. Since we were in the business of helping people do their work better, one way we often did so was via a corrplot.\nWe’ll start with a something manageable. We create a data set with six variables of two sets: a, b , c, and x, y, z, and then we can take a quick look at the correlation matrix.\n\n\n\n\n\n\nCorrelation matrix\n\n\nfeature\na\nb\nc\nx\ny\nz\n\n\n\n\na\n1.00\n0.46\n0.45\n−0.16\n−0.21\n−0.19\n\n\nb\n0.46\n1.00\n0.49\n−0.07\n−0.18\n−0.14\n\n\nc\n0.45\n0.49\n1.00\n−0.16\n−0.23\n−0.20\n\n\nx\n−0.16\n−0.07\n−0.16\n1.00\n0.49\n0.48\n\n\ny\n−0.21\n−0.18\n−0.23\n0.49\n1.00\n0.52\n\n\nz\n−0.19\n−0.14\n−0.20\n0.48\n0.52\n1.00\n\n\n\n\n\n\n\nNow we have the pairwise correlations between all six of our variables, with 1’s on the diagonals (naturally, a variable has a perfect correlation with itself). You can check out the lower diagonal or the upper diagonal, because they contain the exact same information. Quickly, though, find the interesting pattern in that matrix!\nProducing the correlations between just 6 variables gives us 15 correlation coefficients to examine! You can see that you’ll need to spend more than a few seconds on finding the interesting patterns within the data (or if there are any patterns at all). Our brains are oriented towards vision, so we can use preattentive processing elements, like hue, saturation, and size, to make finding interesting patterns easier.\nSince we already have a correlation matrix, we can use various means to find those patterns, which include visualizing the matrix itself, network graphs and others. Here is one way to do it.\n\n\n\n\n\n\nSome recommended R packages for visualizing a correlation matrix include corrr, ggcor, and corrplot. For python, one has options for seaborn, pandas, biokit, and others.\n\n\n\n\n\n\n\n\nCorrelation matrix visualization\n\n\n\n\nLet’s break down what we’re seeing just a little bit. The lower triangle has the correlation values. It adds information, though, by changing the hue by correlation value – red for negative values and blue for positive values – and increasing the saturation as the correlation value becomes stronger. The upper triangle contains the same information, but the size of the circle is tied directly to the strength of the correlation. You’ll also notice that the weaker correlations are more hidden in the visualization, allowing us to focus only on those interesting relationships.\nWhat do you think? Was it easier to spot the points of interest? It looks like there is an a-b-c group and x-y-z group that are similarly correlated within their respective groups. We can also see that the a-b-c group is negatively correlated with the x-y-z group. Visualizing the correlation matrix can usually make it easier to find those interesting patterns.\n\n\n1.5.5 Commentary\nThe correlation is a starting point for understanding linear models, which serve as the foundation for modeling in general. It is very limited by only assessing linear relationships between variables, as well as only pairwise relationships. Other metrics can overcome these, but they have their own limitations. The basic Pearson-Product-Moment correlation coefficient is still the most widely used and a typical starting point in many data adventures.\nThings to explore further next:\n\nRank correlation (e.g., Spearman’s rho, Kendall’s tau)\nDistance metrics (e.g., euclidean, manhattan, cosine)\nNon-linear relationships and interactions (e.g., distance correlation, polynomial, splines)\nMultivariate relationships (e.g., partial correlations, r-squared)"
  },
  {
    "objectID": "introduction.html#moving-towards-an-excellent-adventure",
    "href": "introduction.html#moving-towards-an-excellent-adventure",
    "title": "1  Introduction",
    "section": "1.6 Moving Towards An Excellent Adventure",
    "text": "1.6 Moving Towards An Excellent Adventure\nRemember the point we made about “choosing your own adventure”? Statistical modeling and programming is an adventure, even if you never leave your desk! Every situation calls for choices to be made and every choice you make will lead you down a different path. You will run into errors, dead ends, and you might even find that you’ve spent considerable time to conclude that nothing interesting is happening in your data. This, no doubt, is part of the fun and all of those struggles make success that much sweeter. Like every adventure, things might not be immediately clear and you might find yourself in perilous situations! If you find that something isn’t making sense upon your first read, that is okay. Both authors have spent considerable time mulling over models and foggy ideas during our assorted (mis)adventures; nobody should expect to master complex concepts on a single read through! In any arena where you strive to develop skills, distributed practice and repetition are essential. When concepts get tough, step away from the book, and come back with a fresh mind.\nThanks for coming on this adventure with us and welcome to our Book of Models.\nThis is a test reference (hastie_elements_2009?)."
  },
  {
    "objectID": "linear_models.html#introducing-the-greatest-of-all-time",
    "href": "linear_models.html#introducing-the-greatest-of-all-time",
    "title": "3  The Foundation",
    "section": "3.1 Introducing the Greatest Of All Time",
    "text": "3.1 Introducing the Greatest Of All Time\nNow that you have some idea of what you’re getting into, it’s time to dive in! We’ll start things off by covering the building block of all modeling, and a solid understanding here will provide you the basis for just about anything that comes after, no matter how complex it gets. The linear model is our starting point. At first glance, it may seem like a very simple model, but it’s actually quite powerful and flexible, able to take in different types of inputs, handle nonlinear relationships, temporal and spatial relations, clustering, and more. Linear models have a long history, with even the formal and scientific idea behind correlation and linear regression being well over a century old1! And in that time, the linear model is far and away the most used model out there. But before we start talking about the linear model, we need to talk about what a model is in general.\n\n3.1.1 What is a Model?\nAt its core, a model is just an idea. It’s a way of thinking about the world, about how things work, how things change over time, how things are different from each other, and how they are similar. The underlying thread is that a model expresses relationships about things in the world around us. One can also think of a model as a tool, one that allows us to take information, in the form of data, and act on it in some way. Just like other ideas (and tools), models have consequences in the real world, and they can be used wisely or foolishly.\nOn a practical level, a model is expressed through a particular language, math, but don’t let that worry you if you’re not so inclined. As it’s still just an idea at its core, the idea is the most important thing to understand about a model. The math is just a formal way of expressing the idea in a manner that can be communicated and understood by others in a standard way, and math can help make the idea precise. But in everyday terms, we’re trying to understand things like how the amount of sleep relates to cognitive functioning, how the weather affects the number of people who visit a park, how much money to spend on advertising to increase sales, how to detect fraud, and so on. Any of these could form the basis of a model, as they stem from scientifically testable ideas, and they all express relationships between things we are interested in, possibly even with an implication of causal relations.\nIf you wanted to run a linear model to understand the relationship between sleep and cognitive functioning, you might express it in code as:\n\nRPython\n\n\n\nlm(cognitive_functioning ~ sleep)\n\n\n\n\nfrom statsmodels.formula.api import ols\n\nmodel = ols('cognitive_functioning ~ sleep', data=df).fit()\n\n\n\n\nVery easy! But that’s all it takes to express a straightforward idea. In this case, we’re saying that cognitive functioning is a linear (function) of sleep. By the end of this chapter you’ll also know why R’s function is lm (linear model) and the statsmodels function is ols, but both are doing the same thing."
  },
  {
    "objectID": "linear_models.html#key-ideas",
    "href": "linear_models.html#key-ideas",
    "title": "3  The Foundation",
    "section": "3.2 Key ideas",
    "text": "3.2 Key ideas\nWe can pose a few concepts key to understanding models. This is not an exhaustive list, but it’s a good start. We’ll cover each of these as we go along.\n\nWhat a model is: The model as an idea\nFeatures, targets, and input-output mappings: how do we get from input to output?\nPrediction: how do we use a model?\nInterpretation: what does a model tell us?\n\nPrediction underlies all interpretation\nWe can interpret a model at the feature level and as a whole\n\n\nAs we go along and cover these concepts, be sure that you feel you have the ‘gist’ of what we’re talking about. Almost everything of what comes after linear models builds on these ideas, so it’s important to have a firm grasp before climbing to new heights.\nChapter goals:\n\nUnderstand what a model is conceptually\nUnderstand what a linear model is and how features are mapped to the target\nBe able to get predictions from our model\nBe able to understand the results of a model at a basic level\nGet a sense of complexity and other issues"
  },
  {
    "objectID": "linear_models.html#what-goes-into-a-model",
    "href": "linear_models.html#what-goes-into-a-model",
    "title": "3  The Foundation",
    "section": "3.3 What goes into a model?",
    "text": "3.3 What goes into a model?\n\n3.3.1 Features and Targets\nIn the context of a model, how we specify the nature of the relationship depends on the context. In the interest of generality, we’ll refer to the target as what we want to explain, and features as those aspects of the data we will use to explain it. Because people come at data from a variety of contexts, they often use different terminology to mean the same thing. The table below shows some of the common terms used to refer to features and targets.\n\n\n\n\n\nTable 3.1:  Common Terms for Features and Targets \n  \n    \n    \n      Feature\n      Target\n    \n  \n  \n    independent variable\ndependent variable\n    predictor variable\nresponse\n    explanatory variable\noutcome\n    covariate\nlabel\n    x\ny\n    input\noutput\n    right-hand side\nleft-hand side\n  \n  \n  \n\n\n\n\n\nSome of these actually suggest a particular type of relationship (e.g., a causal relationship, an experimental setting), but we’ll typically avoid those terms if we can. In the end, we may use many of these words to describe things so that you are comfortable with the terminology, but typically we’ll stick with features and targets for the most part. In our opinion, this terminology has the least hidden assumptions/implications.\n\n\n3.3.2 Expressing Relationships\nAs noted, a model is a way of expressing a relationship between a set of features and a target, and one way of thinking about this is in terms of inputs and outputs. But how can we go from input to output? Well to begin, we assume that the features and target are correlated, i.e. that there is some relationship between the x and y. If so, then we can ultimately use the features to predict the target. In the simplest setting a correlation implies a relationship where x and y typically move up and down together (left plot) or they move in opposite directions where x goes up and y goes down (right plot).\n\n\n\n\n\nFigure 3.1: Correlation\n\n\n\n\nIn addition, the typical correlation suggests a linear relationship. There are many types of correlation metrics, but the most common one, the Pearson correlation, is explicitly a measure of the linear relationship between two variables. It’s expressed as a number between -1 and 1, where 0 means there is no linear relationship. As we move closer to a 1.0 correlation value, we would see a tighter scatterplot like the one on the left, until it became a straight line. The same happens for the negative relationship as we get closer to a value of -1. If we have only one feature and target, the Pearson correlation reflects the exact result of the linear model we’d conduct. But even with multiple features, we often use a version of the Pearson R to help us understand how the features account for the target’s variability."
  },
  {
    "objectID": "linear_models.html#the-linear-model",
    "href": "linear_models.html#the-linear-model",
    "title": "3  The Foundation",
    "section": "3.4 THE Linear Model",
    "text": "3.4 THE Linear Model\nThe linear model is perhaps the simplest functional model we can use to express a relationship between features and targets. And because of that, it’s possibly still the most common model used in practice, and it is the basis for many types of other models. Why don’t we run one now?\nThe following dataset has individual movie reviews and contains the rating (1-5 stars scale), along with features pertaining to the review (e.g., word count, etc.), those that regard the reviewer (e.g., age) and features about the movie (e.g., genre, release year). We’ll use the linear model to predict the rating from the length of the review in terms of word count.\nFor our first linear model, we’ll keep things simple. Let’s predict the rating from the length of the review in terms of word count. We’ll use the lm() function in R and the ols() function in Python2 to fit the model. Both functions take a formula as the first argument, which is a way of expressing the relationship between the features and target. The formula is expressed as y ~ x1 + x2 + ..., where y is the target name and x are the feature names. We also need to specify what the data object is, typically a data frame.\n\nRPython\n\n\n\ndf_reviews = read_csv(\"data/movie_reviews.csv\")\n\nmodel_reviews = lm(rating ~ word_count, data = df_reviews)\n\nsummary(model_reviews)\n\n\nCall:\nlm(formula = rating ~ word_count, data = df_reviews)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.0648 -0.3502  0.0206  0.3352  1.8498 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.49164    0.04236    82.4   &lt;2e-16 ***\nword_count  -0.04268    0.00369   -11.6   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.591 on 998 degrees of freedom\nMultiple R-squared:  0.118, Adjusted R-squared:  0.118 \nF-statistic:  134 on 1 and 998 DF,  p-value: &lt;2e-16\n\n\n\n\n\ndf_reviews = pd.read_csv('data/movie_reviews.csv')\n\nmodel_reviews = smf.ols('rating ~ word_count', data = df_reviews).fit()\n\nmodel_reviews.summary(slim = True)\n\n\nOLS Regression Results\n\n\nDep. Variable:\nrating\nR-squared:\n0.118\n\n\nModel:\nOLS\nAdj. R-squared:\n0.118\n\n\nNo. Observations:\n1000\nF-statistic:\n134.1\n\n\nCovariance Type:\nnonrobust\nProb (F-statistic):\n3.47e-29\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n3.4916\n0.042\n82.431\n0.000\n3.409\n3.575\n\n\nword_count\n-0.0427\n0.004\n-11.580\n0.000\n-0.050\n-0.035\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\nFor such a simple model, we certainly have a lot to unpack here! Don’t worry, you’ll eventually come to know what it all means. But it’s nice to know how easy it is to get the results!\nWe’ll start with the fact that the linear model posits a linear combination of the features. A linear combination is just a sum of the features, each of which has been multiplied by some specific value. That value is often called a coefficient, or possibly weight, depending on the context. The linear model is expressed as (math incoming!):\n\\[\ny = b_0 + b_1x_1 + b_2x_2 + ... + b_nx_n\n\\tag{3.1}\\]\n\n\\(y\\) is the target.\n\\(x_1, x_2, ... x_n\\) are the features.\n\\(b_0\\) is the intercept, which is kind of like a baseline value or offset. If we had no features at all it would just be the mean of the target.\n\\(b_1, b_2, ... b_n\\) are the coefficients or weights for each feature.\n\nBut lets start with something simpler, let’s say you want to take a sum of several features. In math you would write it as:\n\\[\nx_1 + x_2 + ... + x_n\n\\]\nIn the previous equation, x is the feature and n is the number identifier for the features, so \\(x_1\\) is the first feature, \\(x_2\\) the second, and so on. \\(x\\) is an arbitrary designation, you could use any letter, symbol you want, or even better, would be the actual feature name. Now look at the linear model.\n\\[\ny = x_1 + x_2 + ... + x_n\n\\]\nIn this case, the function is just a sum, something so simple we do it all the time. In the linear model sense though, we’re actually saying a bit more. Another way to understand that equation is that y is a function of x. We don’t show any coefficients, i.e. the bs in our initial depiction, but technically it’s as if each coefficient was a value of 1. In other words, for this simple linear model, we’re saying that each feature contributes in an identical fashion to the target.\nIn practice, features will not contribute in the same ways, because they correlate with the target differently or are on different scales. So if we want to relate some feature, x1, and some other feature, x2, to target y, we probably would not assume that they both contribute in the same way from the beginning. We might give relatively more weight to x1 than x2. In the linear model, we express this by multiplying each feature by a different coefficient. So the linear model is really just a sum of the features multiplied by their coefficients, i.e. a weighted sum. In fact, we’re saying that each feature contributes to the target in proportion to the coefficient. So if we have a feature x1 and a coefficient b1, then the contribution of x1 to the target is b1*x1. If we have a feature x2 and a coefficient b2, then the contribution of x2 to the target is b2*x2. And so on. So the linear model is really just a sum of the features multiplied by their coefficients.\nFor our model, here is the mathematical representation:\n\\[\n\\textrm{rating} = b_0 + b_1 \\cdot \\textrm{word\\_count}\n\\]\nAnd with the actual results of our model:\n\\[\n\\textrm{rating} = 3.49 + -0.04 \\cdot \\textrm{word\\_count}\n\\]\nNot too complicated we hope! But let’s make sure we see what’s going on here just a little bit more.\n\nOur idea is that the length of the review is in some way related to the eventual rating given to the movie.\nOur target is rating, and the feature is the word count\nWe map the feature to the target via the linear model, which provides an initial understanding of how the feature is related to the target. In this case, we start with a baseline of 3.49. This value makes sense only in the case of a rating with no review, but we have reviews for every observation, so it’s not very meaningful as is. We’ll talk about ways to get a more meaningful intercept later, but for now, that is our starting point. Moving on, if we add a single word to the review, we expect the rating to go down by -0.04 stars. So if we had a review that was 10 words long, i.e., the mean word count, we would predict a rating of 3.49 + 10*-0.04 = 3.1 stars.\n\n\n3.4.1 Models as a Graph\nWe can also express the linear model as a graph, which can be a very useful way to think about models in a visual fashion, and as we see others, can help us literally see how different models relate to one another. In the following, we have three features predicting a single target, so we have three nodes for the features, and a single node for the target. The feature nodes are connected to the target node by an edge, which is labeled with the coefficient. The graph below shows a basic linear model.\n\n\n\n\n\n\n\nDAG\n\n\nx1\n\nx\n1\n\n\n\ny\n\ny\n\n\n\nx1-&gt;y\n\n\nb\n1\n\n\n\nx2\n\nx\n2\n\n\n\nx2-&gt;y\n\n\nb\n2\n\n\n\nx3\n\nx\n3\n\n\n\nx3-&gt;y\n\n\nb\n3\n\n\n\nb0\n\nb\n0\n\n\n\nb0-&gt;y\n\n\n\n\n\n\nFigure 3.2: Linear Model as a Graph\n\n\n\n\n\n\n\n\n\n\n\n\n\nSo at this point you have the basics of what a linear model is and how it works. But there is a lot more to it than that. Just getting the model is easy enough, but we need to be able to use it and understand the details better, so we’ll get into that now!"
  },
  {
    "objectID": "linear_models.html#what-do-we-do-with-a-model",
    "href": "linear_models.html#what-do-we-do-with-a-model",
    "title": "3  The Foundation",
    "section": "3.5 What do we do with a model?",
    "text": "3.5 What do we do with a model?\nOnce we have a working model, there are two primary ways we can use it. One way to use a model is to help us understand the relationships between the features and our outcome of interest. In this way the focus can be said to be on explanation, or interpreting the model results. The other way to use a model is to use it to make estimates about the outcome for specific observations, often ones we haven’t seen in our data. In this way the focus is on prediction. In practice, we often do both, but the focus is usually on one or the other. We’ll cover both in detail here, starting with prediction.\n\n3.5.1 Prediction\nIt may not seem like much at first, but a model is of no use if it can’t be used to make predictions about what we can expect in the world around us. Once our model has been fit to the data, we can obtain our predictions by plugging in values for the features that we are interested in, and, using the corresponding weights and other parameters that have been estimated, come to a guess about a specific observation. Let’s go back to our results, starting with a simpler depiction.\n\n\n\n\n\n  \n    \n    \n      feature\n      estimate\n      std_error\n      statistic\n      p_value\n      conf_low\n      conf_high\n    \n  \n  \n    intercept\n3.49\n0.04\n82.43\n0.00\n3.41\n3.57\n    word_count\n−0.04\n0.00\n−11.58\n0.00\n−0.05\n−0.04\n  \n  \n  \n\n\n\n\nThe table shows the coefficient for each feature including the intercept, which is our starting point. In this case, the coefficient for word count is -0.04, which means that for every additional word in the review, the rating goes down by -0.04 stars. So if we had a review that was 10 words long, we would predict a rating of 3.49 + 10*-0.04 = 3.1 stars.\nWhen we’re talking about predictions for a linear model, we usually will see this as the following mathematically:\n\\[\n\\hat{y} = b_0 + b_1x_1 + b_2x_2 + ... + b_nx_n\n\\]\nWhat is \\(\\hat{y}\\)? The hat over the \\(y\\) just means that it’s a predicted value of the model, rather than the one we actually observe. In fact, we were missing something in our previous depictions of the linear model. We need to add what is usually referred to as an error term, \\(\\epsilon\\), to account for the fact that our predictions will not be perfect3. So the full linear model is:\n\\[\ny = b_0 + b_1x_1 + b_2x_2 + ... + b_nx_n + \\epsilon\n\\]\nThe error term is a random variable that represents the difference between the actual value and the predicted value. We can’t know what the error term is, but we can estimate it. We’ll talk more about that in the section on [estimation][#estimation].\n\n\n3.5.2 What kinds of predictions can we get?\nWhat predictions we get depends on the type of model we are using. For the linear model, we can get predictions for the target, which is a continuous variable. Very commonly, we also can get predictions for a categorical target, such as whether the rating is ‘good’ or ‘bad’. This simple breakdown pretty much covers everything, as we typically would be predicting a continuous variable or a categorical variable, or more of them, like multiple continuous variables, or a target with multiple categories, or sequences of categories (e.g. words). In our case, we can get predictions for the rating, which is a number between 1 and 5. Had our target been a binary good vs. bad rating, our predictions would still be numeric, and usually expressed as a probability between 0 and 1. We then would convert that probability to a class of good or bad depending on a chosen probability cutoff. We’ll talk about how to get predictions for categorical targets later.\nWe saw a prediction for a single observation, but we can also get predictions for multiple observations at once. In fact, we can get predictions for all observations in our dataset. Besides that, we can also get predictions for observations that we don’t have data for. The following shows how we can get predictions for all data, and for a single observation with a word count of 5.\n\nRPython\n\n\n\nall_predictions = predict(model_reviews)\nsingle_prediction = predict(model_reviews, newdata = data.frame(word_count = 5))\n\n\n\n\nall_predictions   = model_reviews.predict()\nsingle_prediction = model_reviews.predict(pd.DataFrame({'word_count': [5]}))\n\n\n\n\nHere is a plot of our predictions versus the actual ratings4. The reference line is where the points would fall if we had perfect prediction. We can see that the predictions are definitely not perfect, but they are not completely off base either. We’ll talk about how to assess the quality of our predictions later, but we can at least get a sense that we have a correspondence relationship between our predictions and target, which is definitely better than not having a relationship at all!\n\n\n![Predictions vs. Actual Ratings](linear_models_files/figure-html/my-first-model-predictions-plot-1.png){width=672}\n\n\nNow let’s look at what our prediction looks like for a single observation, and we’ll add in a few more- one for 10 words, and one for a 50 word review, which is beyond the length of any review in this dataset, and one for 12.3 words, which isn’t even possible for this data.\n\n\n\n\n\nTable 3.2:  Predictions for Specific Observations \n  \n    \n    \n      Word Count\n      Predicted Rating\n    \n  \n  \n    5.0\n3.3\n    10.0\n3.1\n    12.3\n3.0\n    50.0\n1.4\n  \n  \n  \n\n\n\n\n\nThe values reflect the negative coefficient from our model, reflecting a decreasing relationship. Further more, we see the power of the model’s ability to make predictions for what we can’t see. Maybe we limited our data review size, but we know there are 50 word reviews out there, and we can still make a guess as to what the rating would be for such a review. Maybe in another case, we know a group of people who have on average 12.3 word reviews, and we can make a guess as to what the average rating would be for that group. Our model doesn’t know anything about the context of the data, but we can use our knowledge to make predictions about the world around us. This is a very powerful capability, and it’s one of the main reasons we use models in the first place.\n\n\n3.5.3 Prediction Error\nAs we have seen, predictions are not perfect, and an essential part of the modeling endeavor is to better understand these errors and why they occur. In addition, error assessment is the fundamental way in which we assess a model’s performance, and, by extension, compare that performance to other models. In general, prediction error is the difference between the actual value and the predicted value or some function of it, and in statistical models, is also often called the residual. We can look at these individually, or we can look at them in aggregate with a single metric.\nLet’s start with looking at the residuals visually. Often the modeling package you use will have this as a default plotting method when doing a standard linear regression, so it’s wise to take advantage of it. We plot both the distribution of raw error scores and the cumulative distribution of absolute prediction error. Here we see a couple things. First, the distribution is roughly normal, which is a good thing, since statistical linear regression assumes our prediction error is normally distributed. Second, we see that the mean of the errors is zero, which is a consequence of linear regression, and the reason we look at the mean squared error rather than the mean error when assessing model performance. We can also see that most of our predictions are within 1 star rating.\n\n\n\n\n\nDistribution of Prediction Errors\n\n\n\n\nOf more practical concern however, is that we don’t see extreme values or clustering that might indicate a failure on the part of the model to pick up certain segments of the data. It still is good to look at the extremes just in case we can pick up on some aspect of the data that we could potentially incorporate into the model.\nLooking at our worst prediction in absolute terms, we see the observation has a typical word count, and so our simple model will just predict a fairly typical rating. But the actual rating is 1, which is 2.1 away from our prediction, a very noticeable difference.\n\n\n\n\n\nTable 3.3:  Worst Prediction \n  \n    \n    \n      rating\n      prediction\n      word_count\n    \n  \n  \n    1.0\n3.1\n10\n  \n  \n  \n\n\n\n\n\nWe can also get an overall assessment of the prediction error. In the case of the linear model we’ve been looking at, we can express this in a single metric as the sum or mean of our (squared) errors, the latter of which is a very commonly used modeling metric- MSE or mean squared error, or also, its square root - RMSE or root mean squared error.\nIf we look back at our results, we can see this expressed as the part of the output or as an attribute of the model5. The RMSE is more interpretable, and it gives us a sense that we can expect an average prediction error for rating of 0.59. Given that the rating is on a 1-5 scale, this maybe isn’t bad, but we could definitely hope to do better than get within roughly half a point on this scale. We’ll talk about ways to improve this later.\n\nRPython\n\n\n\nsummary(model_reviews) # 'Residual standard error' is approx RMSE\n\n\nCall:\nlm(formula = rating ~ word_count, data = df_reviews)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.0648 -0.3502  0.0206  0.3352  1.8498 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.49164    0.04236    82.4   &lt;2e-16 ***\nword_count  -0.04268    0.00369   -11.6   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.591 on 998 degrees of freedom\nMultiple R-squared:  0.118, Adjusted R-squared:  0.118 \nF-statistic:  134 on 1 and 998 DF,  p-value: &lt;2e-16\n\n\n\n\n\nnp.sqrt(model_reviews.scale)   # RMSE\n\n0.590728780660127\n\n\n\n\n\nAt this point you have the gist of prediction and prediction error, but there is a lot more to it. More detail can be found in the estimation chapter, since we often estimate the parameters of our model by picking those that will reduce the prediction error the most. For now, let’s move on to the other main use of models, which is to help us understand the relationships between the features and the target, or explanation."
  },
  {
    "objectID": "linear_models.html#how-do-we-interpret-the-model",
    "href": "linear_models.html#how-do-we-interpret-the-model",
    "title": "3  The Foundation",
    "section": "3.6 How do we interpret the model?",
    "text": "3.6 How do we interpret the model?\nWhen it comes to interpreting the results of our model, there are a lot of tools at our disposal, though many of the tools we can ultimately use depend on the specifics of the model we have employed. In general though, we can group our understanding approach to that of the feature level and the model level. A feature level understanding regards the relationship between a single feature and the target. Beyond that, we also attempt comparisons of feature contributions to prediction, i.e. relative importance. Model level interpretation is focused on assessments of how well the model ‘fits’ the data, or more generally, predictive performance. We’ll start with the feature level, and then move on to the model level.\n\n3.6.1 Feature Level\nAs mentioned, at the feature level, we are primarily concerned with the relationship between a single feature and the target. More specifically, we are interested in the direction and magnitude of the relationship, but in general, it all boils down to how a feature induces change in the target. For numeric features, we are curious about the change in the target given some amount of change in the feature. For categorical features it’s the same, but often we like to express the change in terms of group mean differences or something similar, since the order of categories is not usually meaningful. Key to the feature level interpretation is the specific predictions made at key feature values.\n\n3.6.1.1 Basics\nLet’s start with the basics by looking again at our coefficient table from the model output.\n\n\n\n\n\n  \n    \n    \n      feature\n      estimate\n      std_error\n      statistic\n      p_value\n      conf_low\n      conf_high\n    \n  \n  \n    intercept\n3.49\n0.04\n82.43\n0.00\n3.41\n3.57\n    word_count\n−0.04\n0.00\n−11.58\n0.00\n−0.05\n−0.04\n  \n  \n  \n\n\n\n\nHere, the main thing to look at are the actual feature coefficients and the direction of their relationship, positive or negative. We saw before that the coefficient for word count is -0.04, and this means that for every additional word in the review, the rating goes down by -0.04. So if we had a review that was 10 words long, we would predict a rating of 3.49 + 10*-0.04 = 3.1 stars.\nThis interpretation gives us directional information, but how can we interpret the magnitude of the coefficient? Let’s try and use some context to help us. The value for the coefficient is -0.04, and the standard deviation of the target, i.e. how much it moves around naturally on its own, is 0.63. So the coefficient is about 6% of the standard deviation of the target. In other words, the addition of a single word to a review results in an expected decrease of 6% of what the review would normally bounce around in value. We probably wouldn’t consider this negligible, but also, a single word change isn’t much. What would be a significant change in word count? Let’s consider the standard deviation of the feature. In this case it’s 5.07 for word count. So if we increase the word count by one standard deviation, we expect the rating to decrease by -0.04 * 5.07 = -0.2, and this translates in to a change of -0.2/0.63 = -0.32 standard deviation units of the target. Without additional context, many would think that’s a significant change [CITATION], or at the very least, that the coefficient is not negligible, and that the feature is indeed related to the target. But we can also see that the coefficient is not so large that it’s not believable.\n\n\n\n\n\n\nStandardized Coefficients\n\n\n\nThe calculation we just did results in what’s often called a ‘standardized’ or ‘normalized’ coefficient. In the case of the simplest model with only one feature like this, it is identical to the Pearson r correlation metric, which we invite you to check and confirm on your own, which should roughly equal our calculation using rounded values. In the case of multiple features, it represents a (partial) correlation between the target and the feature, after adjusting for the other features. But before you start thinking of it as a measure of importance, it is not. It provides some measure of the feature-target linear relationship, but that doesn’t not entail practical importance, nor is it useful in the presence of nonlinear relationships, interactions, and a host of other interesting things that are typical to data and models.\n\n\nAfter assessing the coefficients, next up in our table is the standard error. The standard error is a measure of how much the coefficient varies from sample to sample. If we collected the data multiple times, even under identical circumstances, we wouldn’t get the same value each time- it would bounce around a bit, and the standard error is an estimate of how much it would bounce around. In other words, it’s a measure of uncertainty, and along with the coefficients, it’s used to calculate everything else in the table. The statistic, here a t-statistic from the student t distribution6, is the ratio of the coefficient to the standard error. This gives us a sense of the effect relative to its variability, but the statistic’s primary use is to calculate the p-value related to its distribution7, which is the probability of seeing a coefficient as large as the one we have, if we assume from the outset that the true value of the coefficient is zero. In this case, the p-value is 3.47e-29, which is very small. We can conclude that the coefficient is statistically different from zero, and that the feature is related to the target, at least statistically speaking.\nAside from the coefficients, the most important output is the confidence interval (CI). The CI is a range of values that encapsulates the uncertainty we have in our guess about the coefficients. While our best guess for the effect of word count on rating is -0.04, we know it’s not exactly that, and the CI gives us a range of reasonable values we might expect the effect to be based on the data at hand and the model we’ve employed. In this case, the default is a 95% confidence interval, and we can think of the confidence interval like throwing horseshoes. If we kept collecting data and running models, 95% of our CIs would capture the true value, and this is one of them. That’s the technical definition, which is a bit abstract8, but we can also think of it more simply as a range of values that are good guesses for the true value. In this case, the CI is -0.05 to -0.035, and we can be 95% confident that a good range for the coefficient is between those values. We can also see that the CI is relatively narrow, which is good, as it implies that we have a good idea of what the coefficient is. If it was very wide, we would have a lot of uncertainty about the coefficient, and we would not likely not want to base important decisions regarding it.\n\nKeep in mind that your model has a lot to say about what you’ll be able to say at the feature level. As an example, as we get into machine learning models, you won’t have as easy a time with coefficients and their confidence intervals. For now we’ll stop here, but there is a lot more to the story when it comes to feature level interpretation, and we’ll continue to return to the topic. But first, let’s take a look at interpreting things in another way.\n\n\n\n3.6.2 Is it a Good Model?\nThus far, we’ve focused on interpretation at the feature level. But knowing the interpretation of a feature doesn’t do you much good if the model itself is poor! In that case, we also need to assess the model as a whole, and as with the feature level, we can do this in a few ways. Before getting too carried away with asking whether your model is any good or not, you always need to ask your self relative to what? Many model claim top performance, but are statistically indistinguishable from many other models. So we need to be careful about how we assess our model, and what we compare it to.\nFirst, we can start with the predictions of our model. As noted previously, how well the predictions and target line up is a measure of how well the model fits the data. Most model-level interpretation involves assessing and comparing model fit and variations on this theme. One of the better ways to assess model fit is visually, so let’s look at our predictions vs. the target.\n\nRPython\n\n\n\npredictions = predict(model_reviews)\ny = df_reviews$rating\n\n\n\n\npredictions = model_reviews.predict()\ny = df_reviews.rating\n\n\n\n\n\n\n\n\n\nFigure 3.3: Predictions vs. Observed Ratings\n\n\n\n\nThe one on the left is using the raw target and predictions- very stripey! The reason is that our ratings are only at the single decimal place precision, and our word count is at the integer level precision, so we have a lot of ties. The right side jitters the data randomly a bit so we can see a better pattern, but is otherwise the same. In general, the closer to a line this plot becomes the better, so we can tell already there is still a lot of noise left to explain beyond our model.\n\n3.6.2.1 Model Metrics\nWe’ve already discussed mean-squared error9, but there are other metrics we can use to assess model fit. As we noted, (R)MSE is a very popular measure for continuous targets, telling us the standard deviation of errors, or how much they bounce around on average. In our case, the value was 0.59. Another metric we can use in this particular situation is the mean absolute error, which is similar to the mean squared error, but instead of squaring the errors, we just take the absolute value. Conceptually it attempts to get at the same idea, how much our predictions miss on average, and here the value is 0.46, which we actually showed in our initial residual plot @fig-residuals-plot. With either metric, the closer to zero the better, since as we get closer, we are reducing error.\nWe can also look at the R-squared (R2)value of the model. R2 is possibly the most popular measure of model performance with linear regression and linear models in general. Before squaring, it’s just the correlation of the values that we saw in the previous plot (Figure 3.3). When we square it, we can interpret it as a measure of how much of the variance in the target is explained by the model. In this case, our model shows the R2 is 0.12, which is pretty good for a single feature model. We interpret it that 12% of the target is explained by our model. In addition, we can also interpret R2 as 1 - the prorportion of error varaince in the target, which we can calculate as \\(1 - \\frac{\\textrm{MSE}}{var(y)}\\). In other words the complement of R2 is the proportion of the variance in the target that is not explained by the model. Either way, our result suggests there is plenty of work left to do!\nNote also, that with R2 we get a sense of the variance shared between all features in the model and the target, however complex the model gets. As long as we use it descriptively as a simple correspondence assessment of our predictions and target, it’s a fine metric. For various reasons, it’s not a great metric for comparing models to each other, but again, as long as you don’t get carried away, it’s fine.\n\n\n\n3.6.3 Prediction vs. Explanation\nIn your humble authors’ views, one can’t stress enough the importance of a model’s ability to predict the target. It can be a poor model, maybe because the data is not great, or we’re exploring a new area, but we’ll always be interested in how well a model fits the observed data, and predicts new data.\n\n\nEven to this day, statistical significance is focused on a great deal, even to the point that a much hullabaloo is made about models that have no predictive power at all. As strange as it may sound, you can read whole journal articles, news articles, and business reports in many fields with hardly any mention of prediction. The focus is almost entirely on the explanation of the model, and usually the statistical significance of the features. In those settings, statistical significance is often used as a proxy for importance, which it never should be. Unfortunately, it is affected by other things besides the size of the coefficient, and without an understanding of the context of the features (like how long typical reviews are, what their range is, what variability of ratings is, etc.), the information it provides is extremely limited, and many would argue, not even useful at all. If we are very interested in the coefficient, it is better to focus on the range of possible values, which is provided by the confidence interval. While a confidence interval is also a loaded description of a feature’s relationship to the target, we can use it in a very practical way as a range of possible values for that weight, and more importantly, think of possibilities rather than certainties.\nSuffice it to say at this point that how much one focuses on prediction vs. explanation depends on the context and goals of the data endeavor. There are cases where predictive capability is of utmost importance, and we care less about about explanatory details, but not to the point of ignoring it. For example, even with deep learning models for image classification, where the inputs are just RGB values, we’d still like to know what the (notably complex) model is picking up on, otherwise we may be classifying images based on background nonsense. In some business settings, we are very or even mostly interested in the coefficients/weights, which might indicate how to allocate resources in some fashion, but if they come from a model with no predictive power, this may be a fruitless endeavor.\nIn the end we’ll need to balance our efforts to suit the task at hand. Prediction and explanation are both fundamental to the modeling endeavor."
  },
  {
    "objectID": "linear_models.html#adding-complexity",
    "href": "linear_models.html#adding-complexity",
    "title": "3  The Foundation",
    "section": "3.7 Adding Complexity",
    "text": "3.7 Adding Complexity\nWe’ve seen how to fit a model with a single feature and interpret the results, and that helps us to get oriented to the process. However, we’ll always have more than one feature for a model except under some very specific circumstances, such as exploratory data analysis. So let’s see how we can do that with a model that makes more sense.\n\n3.7.1 Multiple Features\nWe can add more features to our model very simply. Using the standard functions we’ve already demonstrated, we just add them to the formula (both R and statsmodels) as follows.\n\n'y ~ feature_1 + feature_2 + feature_3'\n\nIn other cases, additional features will just be the additional input columns. We might have a lot of features, and even for linear models this could be dozens in some scenarios. A compact depiction of our model uses the matrix representation, which we’ll show in the callout below, and you can find more detail in the matrix section overview. For our purposes, all you really need to know is that this:\n\\[\ny = X\\beta\\qquad  \\textrm{or}\\qquad y = \\alpha + X\\beta\n\\tag{3.2}\\]\nis the same as this:\n\\[\ny = \\alpha + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 \\dots\n\\]\nwhere \\(X\\) is a matrix of features10, and \\(\\beta\\) is a vector of coefficients. Matrix multiplication allows us an efficient way to get our expected value/prediction.\n\n\n\n\n\n\nMatrix Representation of a Linear Model\n\n\n\n\n\nHere we’ll show the matrix representation form of the linear model, for the typical case where we have more than one feature in the model. In the following, y is a vector of all target observations, and likewise each x is a (row) vector of all observations for that feature. The b vector is the vector of coefficients. The 1 serves as a means to incorporate the intercept. It’s just a feature that always has a value of 1. The matrix multiplication is just a compact way of expressing the sum of the features multiplied by their coefficients. We can even do it more\nHere is y as a vector of observations, n x 1.\n\\[\n\\textbf{y} = \\begin{bmatrix}\ny_1 \\\\\ny_2 \\\\\n\\vdots \\\\\ny_n\n\\end{bmatrix}\n\\tag{3.3}\\]\nHere is the vector for x, including the intercept:\n\\[\n\\textbf{X} = \\begin{bmatrix}\n1 & x_{11} & x_{12} & \\dots & x_{1p} \\\\\n1 & x_{21} & x_{22} & \\dots & x_{2p} \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n1 & x_{n1} & x_{n2} & \\dots & x_{np}\n\\end{bmatrix}\n\\tag{3.4}\\]\nAnd finally, here is the vector of coefficients:\n\\[\n\\textbf{b} = \\begin{bmatrix}\nb_0 \\\\\nb_1 \\\\\n\\vdots \\\\\nb_p\n\\end{bmatrix}\n\\tag{3.5}\\]\nPutting it all together, we get the linear model in matrix form:\n\\[\n\\textbf{y = Xb }\n\\tag{3.6}\\]\n\n\n\nWith that in mind, let’s get to our model! In what follows, we keep the word count, but now we add some aspects of the reviewer, such as age and the number of children in the household, features related to the movie- the release year, the length of the movie in minutes, and the total reviews received. We’ll also add another review level feature- the year the review was written. We’ll use the same approach as before, and literally just add them as we depicted in our linear model formula (Equation 3.1).\n\nRPython\n\n\n\nmodel_reviews_extra = lm(\n    rating ~\n        word_count\n        + age\n        + review_year\n        + release_year\n        + length_minutes\n        + children_in_home\n        + total_reviews,\n    data = df_reviews\n)\n\nsummary(model_reviews_extra)\n\n\nCall:\nlm(formula = rating ~ word_count + age + review_year + release_year + \n    length_minutes + children_in_home + total_reviews, data = df_reviews)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.8231 -0.3399  0.0107  0.3566  1.5144 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      -4.56e+01   7.46e+00   -6.11  1.5e-09 ***\nword_count       -3.03e-02   3.33e-03   -9.10  &lt; 2e-16 ***\nage              -1.69e-03   9.24e-04   -1.83   0.0683 .  \nreview_year       9.88e-03   3.23e-03    3.05   0.0023 ** \nrelease_year      1.33e-02   1.79e-03    7.43  2.3e-13 ***\nlength_minutes    1.67e-02   1.53e-03   10.90  &lt; 2e-16 ***\nchildren_in_home  1.03e-01   2.54e-02    4.05  5.5e-05 ***\ntotal_reviews     7.62e-05   6.16e-06   12.36  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.52 on 992 degrees of freedom\nMultiple R-squared:  0.321, Adjusted R-squared:  0.316 \nF-statistic:   67 on 7 and 992 DF,  p-value: &lt;2e-16\n\n\n\n\n\nmodel_reviews_extra = smf.ols(\n    formula = 'rating ~ word_count + age + review_year + release_year + length_minutes + children_in_home + total_reviews',\n    data = df_reviews\n).fit()\n\nmodel_reviews_extra.summary(slim = True)\n\n\nOLS Regression Results\n\n\nDep. Variable:\nrating\nR-squared:\n0.321\n\n\nModel:\nOLS\nAdj. R-squared:\n0.316\n\n\nNo. Observations:\n1000\nF-statistic:\n67.02\n\n\nCovariance Type:\nnonrobust\nProb (F-statistic):\n3.73e-79\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n-45.5688\n7.463\n-6.106\n0.000\n-60.215\n-30.923\n\n\nword_count\n-0.0303\n0.003\n-9.102\n0.000\n-0.037\n-0.024\n\n\nage\n-0.0017\n0.001\n-1.825\n0.068\n-0.004\n0.000\n\n\nreview_year\n0.0099\n0.003\n3.055\n0.002\n0.004\n0.016\n\n\nrelease_year\n0.0133\n0.002\n7.434\n0.000\n0.010\n0.017\n\n\nlength_minutes\n0.0167\n0.002\n10.897\n0.000\n0.014\n0.020\n\n\nchildren_in_home\n0.1028\n0.025\n4.051\n0.000\n0.053\n0.153\n\n\ntotal_reviews\n7.616e-05\n6.16e-06\n12.362\n0.000\n6.41e-05\n8.83e-05\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 2.82e+06. This might indicate that there arestrong multicollinearity or other numerical problems.\n\n\n\n\n\nThere is definitely more to unpack here but it’s important to note that it’s just more stuff, not different stuff. The model-level components are the same in that we still see R2 etc., although they are all ‘better’ (higher R2, lower error) because we have a more predictive model. Our coefficents look the same also, and the fact is we’d interpret the in the same way. Starting with word count, we see that it’s still statistically significant, but it has been reduced just slightly from our previous model where it was the only feature (-0.04 vs. -0.03). Why? This suggests that word count has some non-zero correlation, sometimes called collinearity, with other features that are also explaining the target to some extent. Our linear model shows the effect of each feature controlling for other features, or, holding other features constant11. Conceptually this means that the effect of word count is the effect of word count after we’ve accounted for the other features in the model. In this case, an increase of a single word results in a -0.03 drop. Looking at another feature, the addition of a child to the home is associated with 0.1 bump in rating, accounting for the other features.\nThinking about prediction, how would we get a prediction for a movie rating with a review that is 12 words long, written in 2020, by a 30 year old with one child, for a movie that is 100 minutes long, released in 2015, with 10000 total reviews? Conceptually we just plug in the values for each feature into the model equation exactly as we did for a single feature. But since that’d be tedious, we’ll just use the predict function, which is a bit easier and what we’d normally do.\n\nRPython\n\n\n\npredict_observation = tibble(\n    word_count = 12,\n    age = 30,\n    children_in_home = 1,\n    review_year = 2020,\n    release_year = 2015,\n    length_minutes = 100,\n    total_reviews = 10000\n)\n\npredict(\n    model_reviews_extra,\n    newdata = predict_observation\n)\n\n   1 \n3.26 \n\n\n\n\n\npredict_observation = pd.DataFrame(\n    {\n        'word_count': 12,\n        'age': 30,\n        'children_in_home': 1,\n        'review_year': 2020,\n        'release_year': 2015,\n        'length_minutes': 100,\n        'total_reviews': 10000\n    },\n    index = ['new_observation']\n)\n\nmodel_reviews_extra.predict(predict_observation)\n\nnew_observation    3.2595\ndtype: float64\n\n\n\n\n\nIn our example we’re just getting a single prediction, but don’t let that hold you back! You can predict an entire data set if you want, and use any values for the features you want. We’ll do this explicitly in the machine learning chapter, but for now, try getting a prediction for a different set of values.\n\n\n3.7.2 More Interpretation Tools\n\n3.7.2.1 SHAP Values\nSome models are more complicated than can be explained by a simple coefficient, e.g. nonlinear effects in generalized additive models, or may not even have feature-specific coefficients, like gradient boosting models, or may have many parameters associated with a feature, as in deep learning. Such models typically won’t come with statistical output like standard errors and confidence intervals either. But we’ll still have some tricks up our sleeve to help us figure things out!\nA very common interpretation tool is called a SHAP value. SHAP stands for SHapley Additive exPlanations, and it provides a means to understand how much each feature contributes to a specific prediction. It’s based on a concept from game theory called the Shapley value, which is a way to understand how much each player contributes to the outcome of a game. The reason we bring it up here is that it is has a nice intuition in the linear model case, and demonstrating now is a good way to get a sense of how it works. While the actual computations can be tedious, the basic idea is relatively straightforward- for a given prediction at a specific observation with set feature values, we can calculate the difference between the prediction at that observation and the average prediction. This is the local effect of the feature. However, we must also consider doing this for all possible values of other features that might be in a model, as well as considering whether other features are present for the prediction or not. The initial Shapley approach is to average the local effects over all possible combinations of features, which is computationally intractable for all but the simplest models. The SHAP approach offers more computationally feasible methods for estimation which, while still computationally intensive, is doable for many models. The SHAP approach also has the benefit of being able to be applied to any model, and it’s the approach we’ll use here and return to with some of our other models.\nLet’s look at the SHAP values for our model. We’ll start with a single feature value/observation, using our multifeature model. Here we’ll use the first observation where there are 12 words for word count, age of reviewer is 30, a movie length of 100 minutes etc. To aid our understanding, we calculate the shap value related to word count at that observation by hand, and using a package.\n\nRPython\n\n\n\n# first we need to get the average prediction\navg_pred = mean(predict(model_reviews_extra))\n\n\n# then we need to get the prediction for the feature value of interest\n# for all observations, and average them\npred_observation = predict(\n    model_reviews_extra,\n    newdata = df_reviews |&gt; mutate(word_count = 12)\n)\n\n# then we can calculate the shap value\nshap_value = mean(pred_observation) - avg_pred\n\n# we can also use the DALEX package to do this for us\nexplainer = DALEX::explain(model_reviews_extra, verbose = FALSE)\n\n# observation of interest we want shap values for\nobs_of_interest = tibble(\n    word_count = 12,\n    age = 30,\n    children_in_home = 1,\n    length_minutes = 100,\n    total_reviews = 10000,\n    release_year = 2015,\n    review_year = 2020,\n)\n\nshap_value_package = DALEX::predict_parts(\n    explainer,\n    obs_of_interest,\n    type = \"shap\"\n)\n\n\n\n\n# first we need to get the average prediction\navg_pred = model_reviews_extra.predict(df_reviews).mean()\n\n# then we need to get the prediction for the feature value of interest\npred_observation = model_reviews_extra.predict(df_reviews.assign(word_count = 12))\n\n# then we can calculate the shap value\nshap_value = pred_observation.mean() - avg_pred\n\n\n# now use the shap package for this; it does not work with statsmodels though,\n# and single feature models are a bit cumbersome, but we still get there in the end!\nimport shap\nfrom sklearn.linear_model import LinearRegression\n\n# set data up for shap and sklearn\nfnames = ['word_count', 'age', 'review_year', 'release_year', 'length_minutes', 'children_in_home', 'total_reviews']\n\nX = df_reviews[fnames]\ny = df_reviews['rating']\n\n# use a linear model that works with shap\nmodel_reviews = LinearRegression().fit(X, y)\n\n# 1000 instances for use as the 'background distribution'\nX_sample = shap.maskers.Independent(data = X, max_samples = 1000)  \n\n# # compute the SHAP values for the linear model\nexplainer_linear = shap.Explainer(\n    model_reviews.predict, \n    X_sample   \n)\n\n# find an index where word_count is 12\nobs_of_interest = pd.DataFrame({\n    'word_count': 12,\n    'age': 30,\n    'children_in_home': 1,\n    'review_year': 2020,\n    'release_year': 2015,\n    'length_minutes': 100,\n    'total_reviews': 10000\n}, index = ['new_observation'])\n\nshap_values_linear = explainer_linear(obs_of_interest)\n\nshap_value = shap_values_linear.values[0, 0]\n\n\n\n\n\n\n\n\n\n  \n    \n    \n      ours\n      dalex\n    \n  \n  \n    −0.051\n−0.051\n  \n  \n  \n\n\n\n?(caption)\n\nSo we see the contribution to a prediction for a single feature, but the shap-related packages provide them for all features, and thus get a sense of each feature’s contribution to the predicion. The following shows this as a visualization, either a force plot and waterfall plot. Smaller contributions are aggregated to one effect to simplify the plot. The dotted line represents the average prediction from our model (E[f(x)]) and the prediction we have for the observation (f(x)). We see that total reviews and length in minutes contribute most to the prediction at this observation, followed by release year. We can also see that the effect of movie length is negative.\n\n\n\n\n\nSHAP Visualizations\n\n\n\n\nPretty neat huh? So for any observation we want to, and more importantly, for any model we might use, we can get a sense of how features contribute to that prediction. We also can get a sense of how much each feature contributes to the model as a whole by aggregating these values across all observations in our data, and this provides a measure of feature importance, but we’ll come back to that in a bit.\nIf we are concerned with a single feature’s relationship with the target, we can also look at the partial dependence plot (PDP). The PDP shows the relationship between a feature and the target, but averaged over all other features. In other words, it shows the effect of a feature on the target, but averaged over all other features. For the linear case, it has a direct correspondence to the shap value. The SHAP value is the value the difference between the average prediction and the point on the PDP for a feature at a specific feature value.\nWe can also look at the individual conditional expectation (ICE) plot, which is a PDP for a single observation. The PDP is a very common plot for understanding the relationship between a feature and the target, and we’ll see it again with other models. The ICE plot is less common, but it’s a nice way to see how a feature affects the target for a single observation.\nIn addition, there are other plots that are similar to the PDP and ICE, such as the accumulated local effect (ALE) plot, which is a bit more robust to correlated features than the PDP plo. Where the PDP and ICE plots show the average effect of a feature on the target, the ALE plot focuses on average differences in predictions for the feature at a specific value versus predictions at feature values nearby, and centers the result so that the average difference is zero. We’ll show all three here.\nhttps://christophm.github.io/interpretable-ml-book/ (good reference for all plots)\n\n\n\n\n\nPartial Dependence and Individual Conditional Expectation Plots\n\n\n\n\nKinda cool but maybe not so interesting in that they all kind of tell us the same thing about our negative relationship between word count and rating, which we already knew from our coefficient value. The real power will come in later when we use interactions, nonlinear effects, and other models. But it’s good to note now that the PDP, ICE, and ALE plots are a nice way to get a sense of the relationship between a feature and the target, and we’ll see them again with other models.\n\n\n\n3.7.3 Feature Importance\nHow important is a feature? It’s a common question, and one that is often asked of models, but the answer ranges from ‘it depends’ and ‘it doesn’t matter’. Let’s start with some hard facts:\n\nThere is no single definition of importance.\nThere is no single metric for any model that will definitively tell you how important a feature is relative to others in all data/model contexts.\nThere are many metrics for a given model that are equally valid, but may come to different conclusions.\nAny non-zero feature contribution is potentially ‘imortant’, however small.\nMany metrics of importance fail to adequately capture interactions and deal with correlated features.\nAll measures of importance are measured with uncertainty, and the uncertainty can be large.\nRelative to… what? A poor model will still have relatively ‘important’ features, but they may not be useful.\nIt rarely makes sense to drop features based on importance alone, and will typically drop performance to do so.\nIn the end, what will you do with the information?\n\nTo show just how difficult measuring feature importance is, we only have to stick with our simple linear regression. Think again about R2: it tells us the proportion of the target explained by our features. An ideal measure of importance would be able to tell us how much each feature contributes to that proportion, or in other words, decomposes R2 into the relative contributions of each feature. One of the most common measures of importance in linear models is the standardized coefficient we demonstrated earlier. You know what it doesn’t do? Decompose R2. The easiest situation we could hope for with regard to feature importance is the basic linear model we’ve been using. Everything is linear, with no interactions, or other things going on. And yet there are many logical ways to determine feature importance, and some even break down R2, but they won’t necessarily agree with each other in ranking or relative differences. If you can get a measure of statistical difference between whatever metric you choose, it’s often the case that ‘top’ features will not be statistically different from other features. So what do we do? We’ll show a few methods here, but the main point is that there is no single answer, and it’s important to understand what you’re trying to do with the information.\nLet’s start things off by returning to our SHAP value. If we take the average absolute shap for each feature, we get a sense of the typical contribution size for the features. We can then rank order them as accordingly. Here we see that the most important features here are the number of reviews and the length of the movie. Note that we can’t speak to direction here, only magnitude. We can also see that word count is relatively less important.\n\n\n\n\n\nSHAP Importance\n\n\n\n\nNow here are some additional methods, some more reasonable than others, some which decompose R2 and those that do not. Aside from SHAP, the value represents the proportion of the R2 value that is attributable to the feature. The ones that truly decompose R2 are in agreement for the most part and seem to think highly of word count. The latter seem to be more varied, and only SHAP devalues word count, but possibly for good reason. Which is best? Which is correct? None. But we can get a sense at least that total reviews and length in minutes are likely useful features to our model.\n\n\n\n\n\nFigure 3.4: Feature Importance by Various Methods\n\n\n\n\n\n\n3.7.4 Model Level Interpretation\nAs before we can move beyond feature level interpretation, but we are still going to be concerned with the same sorts of questions - how well does the model fit? Have we impoved the fit significantly? What do our predictions look like, and so on.\nAs an example, we see that our R2 has gone up to 0.32, but this comes with an important caveat - adding any feature would increase our R2, even one that was pure noise! So while it is informative, we want to look at MSE or MAE to determine whether the model has truly improved. In both cases they’ve been reduced. For example, RMSE is now 0.52 a reduction of 12%, so we can feel confident that our model has improved. While there is more a lot more to unpack, we will save doing so for the Model Criticism chapter. At least at this point, we have an idea of how to assess our this added complexity at the model level.\n\n\n\n\n\n\nAdjusted R2\n\n\n\nPart of the output contains an ‘adjusted’ R2. This is a version of R2 that penalizes the addition of features to the model as a way to account for the fact that adding features will always increase R2, even if they are not useful. This is why we can’t use R2 alone to determine whether a model has improved, and why we suggest only considering it as a descriptive statistic. But the adjusted version is kind of a hack, and can even be negative for very poor models. If you want to compare models, use MSE, MAE, and similar metrics.\n\n\n\n\n3.7.5 Other Complexity\n\n3.7.5.1 Categorical Features\nCategorical features can be added to a model just like any other feature. The main issue is that they have to be represented numerically, because models only work on numerically coded features and targets. The simplest and most common encoding is called a one-hot encoding scheme, which creates a new feature for each category, and assigns a 1 if the observation is in that category, and a 0 otherwise. This is also called a dummy coding when used for statistical models. Here is an example of what the coding looks like for the season feature. This is really all there is to it.\n\n\n\n\n\n  \n    \n    \n      rating\n      season\n      Fall\n      Summer\n      Winter\n      Spring\n    \n  \n  \n    2.70\nFall\n1\n0\n0\n0\n    4.20\nFall\n1\n0\n0\n0\n    3.70\nFall\n1\n0\n0\n0\n    2.70\nFall\n1\n0\n0\n0\n    2.40\nSummer\n0\n1\n0\n0\n    4.00\nSummer\n0\n1\n0\n0\n    1.80\nFall\n1\n0\n0\n0\n    2.40\nSummer\n0\n1\n0\n0\n    2.50\nWinter\n0\n0\n1\n0\n    4.30\nSummer\n0\n1\n0\n0\n  \n  \n  \n\n\n\n\nWhen using statistical models we don’t have to do this ourselves. Even other tools for machine learning models will typically have a way to identify and appropriately handle categorical features, even in very complex ways when it comes to deep learning models. What is important is to be aware that they require special handling, but often this is done behind the scenes. Now let’s do a quick example using a categorical feature with our data, and we’ll keep a numeric feature as well just for consistency.\n\nRPython\n\n\n\nmodel_cat = lm(\n    rating ~ word_count + season,\n    data = df_reviews\n)\n\nsummary(model_cat)\n\n\nCall:\nlm(formula = rating ~ word_count + season, data = df_reviews)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9184 -0.3622  0.0133  0.3589  1.8372 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    3.3429     0.0530   63.11  &lt; 2e-16 ***\nword_count    -0.0394     0.0036  -10.96  &lt; 2e-16 ***\nseasonSpring  -0.0301     0.0622   -0.48     0.63    \nseasonSummer   0.2743     0.0445    6.17  9.8e-10 ***\nseasonWinter  -0.0700     0.0595   -1.18     0.24    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.572 on 995 degrees of freedom\nMultiple R-squared:  0.176, Adjusted R-squared:  0.173 \nF-statistic: 53.1 on 4 and 995 DF,  p-value: &lt;2e-16\n\n\n\n\n\nmodel_cat = smf.ols(\n    formula = \"rating ~ word_count + season\",\n    data = df_reviews\n).fit()\n\nmodel_cat.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nrating\nR-squared:\n0.176\n\n\nModel:\nOLS\nAdj. R-squared:\n0.173\n\n\nMethod:\nLeast Squares\nF-statistic:\n53.09\n\n\nDate:\nSun, 10 Dec 2023\nProb (F-statistic):\n1.41e-40\n\n\nTime:\n16:46:51\nLog-Likelihood:\n-857.85\n\n\nNo. Observations:\n1000\nAIC:\n1726.\n\n\nDf Residuals:\n995\nBIC:\n1750.\n\n\nDf Model:\n4\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n3.3429\n0.053\n63.109\n0.000\n3.239\n3.447\n\n\nseason[T.Spring]\n-0.0301\n0.062\n-0.483\n0.629\n-0.152\n0.092\n\n\nseason[T.Summer]\n0.2743\n0.044\n6.171\n0.000\n0.187\n0.362\n\n\nseason[T.Winter]\n-0.0700\n0.059\n-1.177\n0.239\n-0.187\n0.047\n\n\nword_count\n-0.0394\n0.004\n-10.963\n0.000\n-0.047\n-0.032\n\n\n\n\n\n\nOmnibus:\n2.882\nDurbin-Watson:\n2.079\n\n\nProb(Omnibus):\n0.237\nJarque-Bera (JB):\n3.059\n\n\nSkew:\n-0.040\nProb(JB):\n0.217\n\n\nKurtosis:\n3.259\nCond. No.\n53.5\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\nWe now see the usual output. There is word count again, with its slightly negative assocation with rating. And we have an effect for each season as well… except, wait a second, where is the fall effect? The coefficients are interepreted the same way - as we move one unit on x, we see a corresponding change in y. But moving from one category to another requires starting at some category in the first place! So one is chosen arbitrarily, but you would have control over this. In our model, fall is chosen because its first alphabetically. So if we look at say, the effect of summer, we see an increase in the rating of 0.27 relative to fall.\nA better approach to understanding categorical features for standard linear models is through what are called marginal effects, which can provide a kind of average prediction for each category while accounting for the other features in the model. Better still is to visualize these, and we can use something like our PDP approach from before to do so12. It’s actually tricky to define ‘average’ when there are multiple features and interactions involved, so be careful, but we’d interpret the result similarly. In this case, we expect higher ratings for summer releases.\n\n\n\n\n\nMarginal Effects of Season on Rating\n\n\n\n\n\n\n3.7.5.2 Interactions (a preview)\nWe’ll demonstrate this more in the extensions chapter, but a common way to add complexity in linear models is through interactions. This is where we allow the effect of a feature to vary depending on the values of another feature, or even itself! As a conceptual example, we might expect that the effect of the number of children in the home on rating is different for movies from different genres (much higher for kids movies, maybe lower for horror movies), or that genre and season work together in some way to affect rating (e.g. action movies get higher ratings in summer). We might also consider that the length of a movie might plateau or even have a negative effect on rating after a certain point, i.e. having a curvilinear effect. All of these are types of interactions we can explore. Interactions allow us to incorporate nonlinear relationships into the model, and so greatly extend the linear model’s capabilities - we basically get to use a linear model in a nonlinear way!"
  },
  {
    "objectID": "linear_models.html#assumptions-and-more",
    "href": "linear_models.html#assumptions-and-more",
    "title": "3  The Foundation",
    "section": "3.8 Assumptions and More",
    "text": "3.8 Assumptions and More\nMOVE BULK OF THIS TO MODEL CRITICISM??\nEvery model you use has underlying assumptions which, if not met, could potentially result in incorrect inferences. The standard linear regression model we’ve shown is no different, and it has a number of assumptions that must be met for it to be statistically valid. Briefly they are:\n\nThat your model is not grossly misspecified (e.g., you’ve included the right features and not left out important ones)\nThe data your modeling reflects the population you want to make generalizations about\nThe model is linear in the parameters (i.e. no \\(\\beta_1*e^\\beta_2\\) type stuff)\nThe features are not correlated with the error (prediction errors)\nYour data observations are independent of each other\nThe prediction errors are homoscedastic (don’t have large errors with certain predictions vs low with others)\nNormality of the errors (i.e. your prediction errors). Another way to put it is that your target variable is normally distributed conditional on the features.\n\nSome of these are more important than others depending on what you’re trying to do with the model, so\nThings it does not assume:\n\nThat the features are normally distributed\n\nFor example, using categorical features is fine\n\nThat the relationship between the features and target is linear\n\nInteractions, polynomial terms, etc. are all fine\n\nThat the features are not correlated with each other\n\nThey usually are\n\nThat all linear models have these assumptions\n\nIf you don’t meet these assumptions, it doesn’t mean:\n\nThat your model will have poor predictions\nThat your conclusions will necessarily be incorrect\n\nIf you do meet those assumptions, your coefficient estimates are unbiased13, and in general, your statistical inferences are correct ones. If you don’t meet them, there are alternative versions of the linear model you could use that would get around the problem. For example, data that runs over a sequence of time (time series data) violates the independence assumption, but we would use a time series or similar model instead. If normality is difficult to meet, you could assume a different data generating distribution. We’ll discuss some of these in the extensions chapter, but it’s also important to note that not meeting the assumptions may only mean you’ll prefer a different type of linear or other model to use for the data. Our opinion is that not meeting the assumptions often is the result of a poor model, e.g. using poor features in an underspecified way (e.g. not including interactions).\nOn top of not meeting the assumptions, we may in fact introduce bias to get better prediction! For example, we might use a penalized regression model to reduce the variance in our predictions, at the cost of introducing bias in the coefficients. We’ll talk more of this in the machine learning chapter, but suffice it to say for now, if you are more interested in prediction, you may be less interested in the statistical assumptions of the basic linear model.\n\n3.8.1 More Complex Models\nLet’s say your running some XGBoost or Deep Linear Model X and getting outsanding predictions. Assumption smumptions you say! And you might even be right! But if you want to talk confidently about feature contributions, or know something about the uncertainty in the predictions (which you’re assessing right?) well, maybe you might want to know if you’re meeting your assumptions. Some of them are:\n\nYou have enough data to make the model generalizable\nYour data isn’t biased (e.g., you don’t have 90% of your data from one region when you want to talk about a whole area)\nYou adequately sampled the hyperparameter space (e.g. you didn’t just use the defaults or a small grid search)\nYour observations are independent or at least exchangeable and don’t have data leakage, or you are explicitly modeling spatio-temporal dependence\nThat all the parameter settings you set are correct or at least viable (e.g. you let the model run for a long enough set of iterations, your batch size was adequate, you had enough hidden layers, etc.)\n\nAnd if you want to talk about specific feature contributions:\n\nThe features are largely uncorrelated\nThe features largely do not interact (but then why are you doing a complex model that is inherently interactive), or that your understanding of feature contribution deals with the interactions\n\nYeah… so, sorry to say, using non-statistical models doesn’t mean you don’t have to worry about assumptions, you still have some of the old stuff and some new ones to boot.\nModel criticism and diagnostics, but briefly note something here."
  },
  {
    "objectID": "linear_models.html#classification",
    "href": "linear_models.html#classification",
    "title": "3  The Foundation",
    "section": "3.9 Classification",
    "text": "3.9 Classification\nWe’ve been using a continuous target, but what about a categorical target? For example, what if we just had a binary target of whether a movie was good or bad? We will dive much more into classification models in our upcoming chapters, but it turns out that we can still formulate it as a linear model problem. The main difference is that we use a transformation of our linear combination, sometimes called a link function, and we’ll need to use a different objective function rather than least squares, such as the binomial likelihood, to deal with the binary target. This also means we’ll move away from R2 as a measure of model fit, and look at something something else, like accuracy.\nGraphically we can see it in the following way, which when compared with our linear model, doesn’t look much different. In what follows, we create our linear combination \\(\\mu\\) and put it through the sigmoid function \\(\\sigma\\), which is a common link function for binary targets14. The result is a probability, which we can then use to classify the observation as good or bad based on a chosen threshold. We’ll see this in more detail in the classification chapter.\n\n\n\n\ngraph LR\n    n1((X&lt;sub&gt;1&lt;/sub&gt;)) --&gt; |b&lt;sub&gt;1&lt;/sub&gt;| n4((µ))\n    n2((X&lt;sub&gt;2&lt;/sub&gt;)) --&gt; |b&lt;sub&gt;2&lt;/sub&gt;| n4((µ))\n    n3((X&lt;sub&gt;3&lt;/sub&gt;)) --&gt; |b&lt;sub&gt;3&lt;/sub&gt;| n4((µ))\n\n    n4 --&gt; n5((\"σ(µ)\"))\n    n5 --&gt; n6((Y))\n\n    style n1 fill:#f9f,stroke:#333,stroke-width:4px\n    %% style e3 font-size:50%\n\n\n\nLinear Model with Transformation\n\n\n\nAs soon as we move away from the standard linear model and use transformations of our linear predictor, simple coefficient interpretation becomes difficult, sometimes exceedingly so. What will help"
  },
  {
    "objectID": "linear_models.html#more-linear-models",
    "href": "linear_models.html#more-linear-models",
    "title": "3  The Foundation",
    "section": "3.10 More linear models",
    "text": "3.10 More linear models\nBefore we leave our humble linear model, let’s look at some others\nGeneralize Linear Models and related\n\nTrue GLM e.g. logistic, poisson\nOther distributions: beta regression, tweedie, t (so-called robust), truncated\nPenalized regression: ridge, lasso, elastic net\nCensored outcomes: Survival models, tobit\n\nMultivariate/multiclass/multipart\n\nMultivariate regression (multiple targets)\nMultinomial/Categorical/Ordinal regression (&gt;2 classes)\nZero (or some number) -inflated/hurdle/altered\nMixture models and Cluster analysis\n\nRandom Effects\n\nMixed effects models (random intercepts/coefficients)\nGeneralized additive models (GAMMs)\nSpatial models (CAR)\nTime series models (ARIMA)\nFactor analysis\n\nLatent Linear Models\n\nPCA, Factor Analysis\nMixture models\nStructural Equation Modeling, Graphical models generally\n\nAll of these are explicitly linear models or can be framed as such, and most are either identical in description to what you’ve already seen or require only a tweak or two - e.g. a different distribution, a different link function, penalizing the coefficients, etc. In other cases, we can bounce from one to the another. For example we can reshape our multivariate outcome to be amenable to a mixed model approach, and get the exact same results. We can potentially add a random effect to any model, and that random effect can be based on time, spatial or other considerations. The important thing to know is that the linear model is a very flexible tool, and allows you to model most of the types of outcomes were interested in. As such, it’s a very powerful tool.\nWhat about nonlinear models?"
  },
  {
    "objectID": "linear_models.html#commentary",
    "href": "linear_models.html#commentary",
    "title": "3  The Foundation",
    "section": "3.11 Commentary",
    "text": "3.11 Commentary\nLinear models are a very popular tool for data analysis, and for good reason. They are relatively easy to understand, and they are very flexible. They can be used for prediction, explanation, and inference, and they can be used for a wide variety of data types. They are also very easy to interpret, and there are many tools at our disposal to help us understand them. But they are not without their limitations, and we’ll discuss some of those here.\n\nOpinions\nLimitations/Failure points\nSummary\n\n\n3.11.1 Choose your own adventure\nNow that you’ve got the basics, where do you want to go?\n\nIf you want a deeper dive into how we get the results from our model: head Estimation\nIf you want to know more about how to understand the model: head to Model Criticism\nIf you want to do some more modeling: go to Extensions or Machine Learning\nGot more data questions? Go to the Data chapter"
  },
  {
    "objectID": "linear_models.html#quick-exercise",
    "href": "linear_models.html#quick-exercise",
    "title": "3  The Foundation",
    "section": "3.12 Quick Exercise",
    "text": "3.12 Quick Exercise\n\nImport X Data. Stick with the current data if you want, just try out other features, or maybe try the world happiness data 2018 data . You can find details about it in the appendix ADD LINK.\nFit a linear model, try to keep it to no more than three features.\nGet all the predictions for the data, and try at least\nInterpret the coefficients\nAssess the model fit\n\n# refs to add\nStatistical modeling rigorous: Econometrics books like Wooldrigde, Greene, etc.\nBasic DS?\nAssumptions: https://statmodeling.stat.columbia.edu/2013/08/04/19470/"
  },
  {
    "objectID": "model_criticism.html",
    "href": "model_criticism.html",
    "title": "3  Model criticism",
    "section": "",
    "text": "PLACEHOLDER"
  },
  {
    "objectID": "steps.html",
    "href": "steps.html",
    "title": "4  Steps in Modeling",
    "section": "",
    "text": "Never forget the crossover!"
  },
  {
    "objectID": "estimation.html#introduction-to-model-estimation",
    "href": "estimation.html#introduction-to-model-estimation",
    "title": "5  How do we obtain a model?",
    "section": "5.1 Introduction to Model Estimation",
    "text": "5.1 Introduction to Model Estimation\nModel estimation is the process of finding the parameters associated with a model that allow us to reach a particular modeling goal. Different types of models will have different parameters to estimate, and there are different ways to estimate them. In general though, the goal is the same, find the set of parameters that will lead to the best predictions under the current data modeling context.\nWith model estimation, we can break things down into the following steps:\n\nStart with an initial guess for the parameters\nCalculate the prediction error, or some function of it, or some other value that represents our model’s objective\nUpdate the guess\nRepeat steps 2 & 3 until we find a ‘best’ guess\n\nPretty straightforward, right? Well, it’s not always so simple, but this is the general idea in most applications. In this chapter, we’ll show how to do this ourselves to take away the mystery a bit from when you run standard model functions in typical contexts. Hopefully then you’ll gain more confidence when you do use them!\n\n\n\n\n\n\nEstimation vs. Optimization\n\n\n\nWe can use estimation as general term for finding parameters, while optimization can be seen as a term for finding parameters that maximize or minimize some objective function, or even a combination of objectives. In some cases we can estimate parameters without optimization, because there is a known way of solving the problem, but in most modeling situations we are going to use some optimization approach to find a ‘best’ set of parameters."
  },
  {
    "objectID": "estimation.html#key-ideas",
    "href": "estimation.html#key-ideas",
    "title": "5  How do we obtain a model?",
    "section": "5.2 Key ideas",
    "text": "5.2 Key ideas\n\nParameters are the values associated with a model\nEstimation is the process of finding the parameters associated with a model\nThe objective function produces a value that we want to, for example, maximize or minimize\nPrediction error is the difference between the actual value of the target and the predicted value of the target, and is often used to calculate the objective function\nOptimization is the process of finding the parameters that maximize or minimize some objective function\nModel Selection is the process of choosing the best model from a set of models"
  },
  {
    "objectID": "estimation.html#data-setup",
    "href": "estimation.html#data-setup",
    "title": "5  How do we obtain a model?",
    "section": "5.3 Data Setup",
    "text": "5.3 Data Setup\nFor the examples here, we’ll use the world happiness dataset for the year 2018. We’ll use the happiness score as our target, and we’ll use the GDP per capita as our primary feature, though we may throw in some others. Let’s take a look at the data here, but for more information see the appendix.\nNOTE GTEXTRAS PLOT CAUSES PDF ERROR AND WILL HAVE TO BE IMPORTED AS FILE\n\n\n\n\n\n  \n    \n    \n      term\n      happiness_score\n      healthy_life_expectancy_at_birth\n      log_gdp_per_capita\n      perceptions_of_corruption\n    \n  \n  \n    happiness_score\nNA\n0.78\n0.82\n−0.47\n    healthy_life_expectancy_at_birth\n0.78\nNA\n0.86\n−0.34\n    log_gdp_per_capita\n0.82\n0.86\nNA\n−0.34\n    perceptions_of_corruption\n−0.47\n−0.34\n−0.34\nNA\n  \n  \n  \n\n\n\n\nOur happiness score has values from around 3-7, life expectancy and gdp appear to have some notable variability, and corruption perception is skewed toward lower values. We can also see that the features and target are correlated with each other, which is not surprising.\nWe’ll do some minor cleaning and renaming of columns, and we’ll drop any rows with missing values. We’ll also scale the features so that they are on the same scale, which as noted in the data chapter, can help make estimation easier.\n\nRPython\n\n\n\ndf_happiness &lt;- read_csv(\"data/world_happiness_2018.csv\") |&gt;\n    drop_na() |&gt;\n    select(\n        country,\n        happiness_score,\n        healthy_life_expectancy_at_birth,\n        log_gdp_per_capita,\n        perceptions_of_corruption\n    ) |&gt;\n    rename(\n        happiness  = happiness_score,\n        life_exp   = healthy_life_expectancy_at_birth,\n        log_gdp_pc = log_gdp_per_capita,\n        corrupt    = perceptions_of_corruption\n    ) |&gt;\n    mutate(\n        gdp_pc = exp(log_gdp_pc), # put back on original scale before scaling\n        across(life_exp:gdp_pc, \\(x) (x - mean(x)) / sd(x))\n    ) |&gt;\n    select(-log_gdp_pc) # drop the log version\n\n\n\n\ndf_happiness = (\n    pd.read_csv('data/world_happiness_2018.csv')\n    .dropna()\n    .rename(\n        columns = {\n            'happiness_score': 'happiness',\n            'healthy_life_expectancy_at_birth': 'life_exp',\n            'log_gdp_per_capita': 'log_gdp_pc',\n            'perceptions_of_corruption': 'corrupt'\n        }\n    )\n    .assign(\n        gdp_pc = lambda x: np.exp(x['log_gdp_pc']),\n    )    \n    [['country', 'happiness','life_exp', 'gdp_pc', 'corrupt']]\n)\n\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\ndf_happiness[['life_exp', 'gdp_pc', 'corrupt']] = scaler.fit_transform(\n    df_happiness[['life_exp', 'gdp_pc', 'corrupt']]\n)"
  },
  {
    "objectID": "estimation.html#starting-out-by-guessing",
    "href": "estimation.html#starting-out-by-guessing",
    "title": "5  How do we obtain a model?",
    "section": "5.4 Starting Out by Guessing",
    "text": "5.4 Starting Out by Guessing\nSo we’ll start with a model in which we predict a country’s level of happiness by their life expectancy, where if you can expect to live longer, maybe you’re probably in a country with better health care, higher incomes, and other important stuff. We’ll stick with our simple linear model as well.\nAs a starting point we can just guess what the parameter should be, but how would we know what to guess? How would we know which guesses are better than others? Let’s try a few guesses and see how they do. Let’s say that we don’t think life expectancy matters, and that most countries are at a happiness value of 4. We can plug this into the model and see what we get:\n\\[\n\\textrm{prediction} = 4 + 0\\cdot\\textrm{life\\_exp}\n\\]\nAlternatively we could use the data to inform our guess. We start with a mean of happiness score, but moving up a standard deviation of life expectancy (roughly ~1 years) would move a whole point of happiness \\[\n\\textrm{prediction} = \\overline{\\textrm{happiness}} + 1\\cdot\\textrm{life\\_exp}\n\\]\nIn this case, our offset (or intercept) is the mean of the target, and our coefficient for the scaled life expectancy is 1. This is probably a better guess, since it is at least data driven, but it’s still not great. But how do we know it’s better?"
  },
  {
    "objectID": "estimation.html#prediction-error",
    "href": "estimation.html#prediction-error",
    "title": "5  How do we obtain a model?",
    "section": "5.5 Prediction Error",
    "text": "5.5 Prediction Error\nWe can compare the predictions from each guess to the actual values of the target. We can do this by calculating the prediction error, or in the context of a linear model, they are also called residuals. The prediction error is the difference between the actual value of the target and the predicted value of the target. We can express this as:\n\\[\n\\epsilon = y - \\hat{y}\n\\] \\[\n\\textrm{error} = \\textrm{target} - \\textrm{(model based) guess}\n\\]\nNot only does this tell us how far off our model prediction is, it gives us a way to compare models. With a measure of prediction error, we can get a metric for total error for all observations/predictions, or similarly the average error. If one model or parameter set has less total or average error, we can say it’s a better model than one that has more. Ideally we’d like to choose a model with the least error, but we’ll see that this is not always possible1. For now, let’s calculate the error for our two guesses. One thing though, if we miss the mark above or below our target, we still want it to count the same in terms of prediction error. In other words, if the true happiness score is 5 and our model predicts 5.5 or 4.5, we want those to count the same when we total up our error2. One way we can do this is to use the squared error value, or maybe the absolute value. We’ll use squared error here, and we’ll calculate the mean of the squared errors for all our predictions. We’ll do this for our two models above.\n\nRPython\n\n\n\ny &lt;- df_happiness$happiness\n\n# Calculate the error for the guess of 4\nprediction &lt;- 4\nmse_four &lt;- mean((y - prediction)^2)\n\n# Calculate the error for our other guess\nprediction &lt;- mean(y) + 1 * df_happiness$life_exp\nmse_other &lt;- mean((y - prediction)^2)\n\n\n\n\ny = df_happiness['happiness']\n\n# Calculate the error for the guess of four\nprediction = 4\nmse_four   = np.mean((y - prediction)**2)\n\n# Calculate the error for our other guess\nprediction = y.mean() + 1 * df_happiness['life_exp']\nmse_other  = np.mean((y - prediction)**2)\n\n\n\n\nNow let’s look at our Mean Squared Error (MSE), and we’ll also inspect the square root of it, or the Root Mean Squared Error, as that puts things back on the original target scale. We also add the Mean Absolute Error as another metric. Inspecting the metrics, we can see that we are off on average by over a point for our ‘#4’ model, but notably less when guessing the mean.\n\n\n\n\n\n  \n    \n    \n      Model\n      MSE\n      RMSE\n      MAE\n      RMSE % drop\n      MAE % drop\n    \n  \n  \n    #4\n3.36\n1.83\n1.52\n\n\n    Other\n0.50\n0.71\n0.58\n61%\n62%\n  \n  \n  \n\n\n\n\nWe can see that the other model is not only better, but results in a 61% drop in RMSE, and similar for MAE. We’d definitely prefer the other model over the ‘#4’ model. Furthermore, we can see how we can compare models in a general fashion.\nWell, this is useful, and at least we can say one model is better than another. But you’re probably hoping there is an easier way to do get a good guess for our model parameters, especially when we have possibly dozens of features and/or parameters to keep track of, and there is!\nMOVE THIS TO LATER\n\n\n\n\n\n\nA Note on Terminology\n\n\n\nThe objective function is often called the loss function, and sometimes the cost function. However, these both imply that we are trying to minimize the function, which is not always the case3, and it’s arbitrary whether you want to minimize or maximize the function. In fact, some people will minimize the negative likelihood when using maximum likelihood! As such we’ll try to stick to the more neutral term objective function, but you may see the other terms used interchangeably in this text. In addition, some packages will use the term metric to refer a value that you might want to examine as well, or even use to compare models. For example, the MSE is a metric, but it may also be the objective function we are trying to minimize. Other metrics we could calculate without being the objective might be Adjusted R-squared and median absolute error. We could also use MSE as the objective, but use percentage drop in error from baseline when selecting among several models that minimized MSE. This can be very confusing when starting out! We’ll try to stick to the term metric for additional values that we might want to examine separate from the objective function value."
  },
  {
    "objectID": "estimation.html#ordinary-least-squares",
    "href": "estimation.html#ordinary-least-squares",
    "title": "5  How do we obtain a model?",
    "section": "5.6 Ordinary Least Squares",
    "text": "5.6 Ordinary Least Squares\nFor a simple linear model, we can estimate the parameters in several ways, but the most common is to use the Ordinary Least Squares (OLS) method. OLS is a method of estimating the coefficients that minimizes the sum of the squared errors, which we’ve just been doing in the previous section4. In other words, it finds the coefficients that minimize the sum of the squared differences between the predicted values and the actual values. We can express this as:\n\\[\n\\textrm{Value} = \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2\n\\]\nWhere \\(y_i\\) is the actual value of the target for observation \\(i\\), and \\(\\hat{y_i}\\) is the predicted value from the model. The sum of the squared errors is also called the residual sum of squares (RSS), as opposed to the total sums of squares (i.e. the variance of the target), and the part explained by the model (model or explained sums of squares). The OLS method finds the coefficients that minimize the sum of the squared differences between the predicted values and the actual values. It’s called ordinary least squares because there are other least squares methods - generalized least squares, weighted least squares, and others, but we don’t need to worry about that for now. What matters is that we have a way to estimate the coefficients that minimizes the sum of the squared errors.\nThe resulting value - the sum or mean of the squared errors, as we noted can be referred to as our objective value, while the objective function is just the process of taking the predictions and observed target values and totaling up their squared differences. We can use this value to find the best parameters for a specific model, as well as compare models with different parameters, such as a model with additional features versus one with fewer. We can also use this value to compare different types of models that are using the same objective function, such as a linear model and a decision tree model.\nLet’s calculate the OLS estimate for our model. From our steps above, we need guesses and a way to update them. For now, we can just provide a bunch of guesses, and just move along from one set to the next, and ultimately just choose whichever has the lowest value.\n\nRPython\n\n\n\nols &lt;- function(X, y, par, sum_sq = FALSE) {\n    X &lt;- cbind(1, X)\n    # Calculate the predicted values\n    y_hat &lt;- X %*% par # %*% is matrix multiplication\n\n    # Calculate the error\n    error &lt;- y - y_hat\n\n    # Calculate the value as sum or mean squared error\n    value &lt;- crossprod(error) # crossprod is matrix multiplication\n\n    if (!sum_sq) {\n        value &lt;- value / nrow(X)\n    }\n\n    # Return the value\n    return(value)\n}\n\n# create a grid of guesses\nguesses &lt;- crossing(\n    b0 = seq(1, 7, 0.1),\n    b1 = seq(-1, 1, 0.1)\n)\n\n# Example for one guess\nols(\n    X = df_happiness$life_exp,\n    y = df_happiness$happiness,\n    par = unlist(guesses[1, ])\n)\n\n       [,1]\n[1,] 23.777\n\n\n\n\n\ndef ols(par, X, y, sum = False):\n    # add a column of 1s for the intercept\n    X = np.c_[np.ones(X.shape[0]), X]\n\n    # Calculate the predicted values\n    y_hat = X @ par\n    \n    # Calculate the error\n    value = np.sum((y - y_hat)**2)\n    \n    # Calculate the value as sum or average\n    if not sum:\n        value = value / X.shape[0]\n    \n    # Return the value\n    return(value)\n\n# create a grid of guesses\nfrom itertools import product\n\nguesses = pd.DataFrame(\n    product(\n        np.arange(1, 7, 0.1),\n        np.arange(-1, 1, 0.1)\n    ),\n    columns = ['b0', 'b1']\n)\n\n# Example for one guess\nols(\n    par = guesses.iloc[0,:],\n    X = df_happiness['life_exp'],\n    y = df_happiness['happiness']\n)\n\n23.793842044979073\n\n\n\n\n\nNow we want to calculate the loss for each guess and find which one gives us the minimum function value. Note that above, we could get the total or mean squared error by setting the sum parameter to TRUE or FALSE. Either is fine, but it’s more common to use the mean, which is a little more understandable - how far do our guess deviate from the true value on average? In the following darker suggests a better mean squared error result from our approach.\n\n\n\n\n\nIf we inspect our results from the built-in functions, we had estimates of 5.44 and 0.89 for our coefficients. These are very similar but not exactly the same, but in the end we can see that we get pretty dang close to what our basic lm or statsmodels functions would get us. Pretty neat!\n\n\n\n\n\n\nEstimation as ‘Learning’\n\n\n\nEstimation can be seen as the process of a model learning which parameters will best allow the predictions to match the observed data, and hopefully, predict as-yet-unseen future data. This is a very common way to think about estimation in machine learning, and it is a useful way to think about our simple linear model also.\nOne thing to keep in mind is that it is not a magical process. It takes good data, a good idea (model), and an appropriate estimation method to get good results."
  },
  {
    "objectID": "estimation.html#optimization",
    "href": "estimation.html#optimization",
    "title": "5  How do we obtain a model?",
    "section": "5.7 Optimization",
    "text": "5.7 Optimization\nBefore we get into other objective functions, let’s think about a better way to find the best parameters for our model. Rather than just guessing, we can use a more systematic approach, and thankfully, there are tools out there to help us. We just use a function like our OLS function, give it a starting point, and let the algorithms do the rest! Thanks to some nifty approaches to making better guesses, these tools eventually arrive at a pretty good set of parameters. Well, they usually do, but not always- nothing’s perfect! But they are pretty good, and they are a lot better than guessing. Let’s see how we can use one of these tools to find the best parameters for our model.\nPreviously we created a set of guesses to search over to see which set of parameters resulted in prediction that matched the data best. What we did is called a grid search, and it is a bit of a brute force approach to finding the best fitting model. You can imagine that a couple of unfortunate or problematic scenarios, such as having a very large number of parameters, or that our specified range doesn’t allow us to get to the right sets of parameters, or we specify a very large range, but the best fitting model is within a very narrow part of that range, such that we waste a lot of time.\nIn general, we can think of optimization as a way to find the best parameters for our model. We start with an initial guess, see how well it does in terms of our objective function, and then try to improve it with a new guess. We continue to do so until a stopping point is reached. Here is an example.\n\nStart with an initial guess for the parameters\nCalculate the objective function given the parameters\nUpdate the parameters to a new guess (that hopefully improves the objective function)\nCalculate the objective function given the new parameters\nRepeat until the improvement is small enough or we reach a maximum number of iterations we want to attempt\n\nThis is what we described before with estimation in general. The key idea now is how we update the old parameters with a new guess at each iteration. Different optimization algorithms use different approaches to find the updated parameters. At some point, either the improvement is small enough, or we reach a maximum number of iterations we want to attempt, and either of these is something we can set ourselves. If we meet the terms of our objective, we say that we have reached convergence. Sometimes, the number of iterations is not enough for us to reach convergence, and we have to try again with a different set of parameters, a different algorithm, maybe use some data transformations, or something else.\nSo let’s try it out! Both R and Python offer a function where we can specify the objective function, and it will try to find the best parameters for us. We’ll use the optim function in R and the minimize function in Python. It needs several inputs:\n\nthe objective function\nthe initial guess for the parameters to get things going\ninputs to the objective function\noptions for the optimization process, e.g. algorithm, maximum number of iterations, etc.\n\nWith these inputs, we’ll let the optimization functions do the rest of the work. We’ll also compare our results to the built-in functions to make sure we’re on the right track.\n\nRPython\n\n\n\nour_result &lt;- optim(\n    par    = c(1, 0),\n    fn     = ols,\n    X      = df_happiness$life_exp,\n    y      = df_happiness$happiness,\n    method = \"BFGS\" # optimization algorithm\n)\n\n# our_result\n\n\n\n\nfrom scipy.optimize import minimize\n\nour_result = minimize(\n    fun    = ols,\n    x0     = np.array([1., 0.]),\n    args   = (np.array(df_happiness['life_exp']), np.array(df_happiness['happiness'])),\n    method = 'BFGS' # optimization algorithm\n)\n\n# our_result\n\n\n\n\n\n\n\n\n\nTable 5.1:  Comparison of our results to built-in function \n  \n    \n    \n      Parameter\n      Built-in\n      Our Result\n    \n  \n  \n    Intercept\n5.4450\n5.4450\n    Life Exp. Coef.\n0.8880\n0.8880\n    Objective/MSE\n0.4890\n0.4890\n  \n  \n  \n\n\n\n\n\nSo our little function and the right tool allows us to come up with the same thing as base R and statsmodels! I hope you’re feeling pretty good at this point because you should! You just proved you could do what seems like magic, but really all it took is just a little knowledge about some key concepts. Let’s try some more!"
  },
  {
    "objectID": "estimation.html#maximum-likelihood",
    "href": "estimation.html#maximum-likelihood",
    "title": "5  How do we obtain a model?",
    "section": "5.8 Maximum Likelihood",
    "text": "5.8 Maximum Likelihood\nIn our example thus far, we have been minimizing the specific objective (or loss) function, which basically takes our parameter estimates, produces a prediction, and returns the sum or mean of the squared errors. But this is just one approach we could take. Now we’d like you to think about the data generating process. We have a model that says happiness is a function of life expectancy, but more specifically, let’s think about how the observed value of the happiness score is generated in a statistical sense. In particular, what kind of probability distribution might be involved? Ignoring the model, we might think that each happiness value is generated by some random process, and that the process is the same for each observation. Let’s assume that random process is a normal distribution. So something like this would describe it mathematically:\n\\[\n\\textrm{happiness} \\sim N(\\mu, \\sigma)\n\\]\nwhere \\(\\mu\\) is the mean of the happiness and \\(\\sigma\\) is the standard deviation, or in other words, we can think of happiness as a random variable that is drawn from a normal distribution with \\(\\mu\\) and \\(\\sigma\\) as the parameters of that distribution.\nLet’s apply this idea to our linear model setting. In this case, the mean is a function of life expectancy, and we’re not sure what the standard deviation is, but we can go ahead and write our model as follows.\n\\[\n\\mu = \\beta_0 + \\beta_1 * \\textrm{life\\_exp}\n\\] \\[\n\\textrm{happiness} \\sim N(\\mu, \\sigma)\n\\]\nNow, we can think of the model as a way to estimate the parameters of the normal distribution, but we have an additional parameter to estimate. We still have our previous coefficients, but now we need to estimate \\(\\sigma\\), which is basically our RMSE, as well. But we still have to think of things a little differently. When we compare our prediction to the observed value, we don’t look at the simple difference, but we are still interested in the discrepancy between the two. So now we think about the likelihood of observing the happiness score given our prediction, which is based on the estimated parameters, i.e. given the \\(\\mu\\) and \\(\\sigma\\), and \\(\\mu\\) is a function of the coefficients and life expectancy. We can write this as:\n\\[\n\\textrm{Pr}(\\textrm{happiness} \\mid \\textrm{life\\_exp}, \\beta_0, \\beta_1, \\sigma)\n\\]\n\\[\n\\textrm{Pr}(\\textrm{happiness} \\mid \\mu, \\sigma)\n\\]\nEven more generally, the likelihood gives us a sense of the probability given the parameter estimates \\(\\theta\\). \\[\n\\textrm{Pr}(\\textrm{Data} \\mid \\theta)\n\\]\nHere is a simple code demo to get a likelihood in the context of our model. The values you see are referred to statistically as probability density values, and they are technically not probabilities, but rather the probability density, or relative likelihood, at that observation5. For your conceptual understanding, if it makes it easier, you can think of them in the same was as you do probabilities, but just know that technically they are not.\n\nRPython\n\n\nUPDATE VALUES WHEN DEMO IS SETTLED\n\n# two example life expectancy scores, mean and 1 sd above\nlife_expectancy &lt;- c(0, 1)\n\n# observed happiness scores\nhappiness &lt;- c(4, 5.2)\n\n# predicted happiness with rounded coefs\nmu &lt;- 5 + 1 * life_expectancy\n\n# just a guess for sigma\nsigma &lt;- .5\n\n# likelihood for each observation\nL &lt;- dnorm(happiness, mean = mu, sd = sigma)\nL\n\n[1] 0.1079819 0.2218417\n\n\n\n\n\nfrom scipy.stats import norm\n\n# two example life expectancy scores, mean and 1 sd above\nlife_expectancy = np.array([0, 1])\n\n# observed happiness scores\nhappiness = np.array([4, 5.2])\n\n# predicted happiness with rounded coefs\nmu = 5 + 1 * life_expectancy\n\n# just a guess for sigma\nsigma = .5\n\n# likelihood for each observation\nL = norm.pdf(happiness, loc = mu, scale = sigma)\nL\n\narray([0.10798193, 0.22184167])\n\n\n\n\n\nGiven a guess at the parameters, and an assumption about the distribution of the data, we can calculate the likelihood of observing each data point, and total those sum those up, just like we did with our squared errors. In theory, we’d deal with the product of each likelihood, but in practice we sum the log of the likelihood, otherwise values would get too small for our computers to handle. Here is a corresponding function we can use to calculate the likelihood of the data given our parameters. Note that the actual likelihood value returned isn’t really interpretable, we just use it to compare models with different sets of parameter guesses. Even if our total likelihoods under comparison are negative, we prefer the model with the relatively higher likelihood. As we just demonstrated, we’ll use optim to help us get good guesses6.\n\nRPython\n\n\n\nlikelihood &lt;- function(par, X, y) {\n    X &lt;- cbind(1, X)\n    # setup\n    beta &lt;- par[-1] # coefficients\n    sigma &lt;- exp(par[1]) # error sd, exp keeps positive\n\n    N &lt;- nrow(X)\n\n    LP &lt;- X %*% beta # linear predictor\n    mu &lt;- LP # identity link in the glm sense\n\n    # calculate (log) likelihood\n    ll &lt;- dnorm(y, mean = mu, sd = sigma, log = TRUE)\n    -sum(ll) # for minimization\n}\n\n\nour_result &lt;- optim(\n    par = c(1, 0, 0),\n    fn  = likelihood,\n    X   = df_happiness$life_exp,\n    y   = df_happiness$happiness\n)\n\n# our_result\n\n\n\n\ndef likelihood(par, X, y):\n    # add a column of 1s for the intercept\n    X = np.c_[np.ones(X.shape[0]), X]\n\n    # setup\n    beta   = par[1:]         # coefficients\n    sigma  = np.exp(par[0])  # error sd, exp keeps positive\n\n    N = X.shape[0]\n\n    LP = X @ beta          # linear predictor\n    mu = LP                # identity link in the glm sense\n\n    # calculate (log) likelihood\n    ll = norm.logpdf(y, loc = mu, scale = sigma) \n    return(-np.sum(ll))\n\nour_result = minimize(\n    fun  = likelihood,\n    x0   = np.array([1, 0, 0]),\n    args = (np.array(df_happiness['life_exp']), np.array(df_happiness['happiness']))\n)\n\n\n\n\nHow would we switch to a maximum likelihood approach using readily available functions? In both R and Python you can switch to using glm and GLM respectively would be the place to start. We can use different likelihoods distributions corresponding to the binomial, poisson and others. Still other packages would allow even more distributions for consideration. In general, we choose a distribution that we feel best reflects the data generating process. For binary targets for example, we typically would feel a bernoulli or binomial distribution is appropriate. For count data, we might choose a poisson or negative binomial distribution. For targets that fall between 0 and 1, we might go for a beta distribution. There are many distributions, and even when some might feel more appropriate, we might choose another for convenience. Some distributions tend toward a normal (a.k.a. gaussian) distribution depending on various factors, while others are special cases of more general distributions. For example, the exponential distribution is a special case of the gamma distribution, and a cauchy is equivalent to a t distribution with 1 degree of freedom, and the t tends toward a normal with increasing degrees of freedom. Here is a visualization of the relationships among some of the more common distributions.\n\n\n\n\n\nFigure 5.1: Relationships among some of univariate probability distributions. Image from Wikipedia\n\n\n\n\nHere are examples of standard GLM functions in R and Python\n\nRPython\n\n\n\nglm(happiness ~ life_exp, data = df_happiness, family = gaussian)\nglm(binary_target ~ x1 + x2, data = some_data, family = binomial)\nglm(count ~ x1 + x2, data = some_data, family = poisson)\n\n\n\n\nimport statsmodels.formula.api as smf\n\nsmf.glm('happiness ~ life_exp', data = df_happiness, family = sm.families.Gaussian())\nsmf.glm('binary_target ~ x1 + x2', data = some_data, family = sm.families.Binomial())\nsmf.glm('count ~ x1 + x2', data = some_data, family = sm.families.Poisson())\n\n\n\n\nWith that in mind, we can compare our result to a built-in function that has capabilities beyond OLS. As before, we’re duplicating the basic glm result. We show more decimal places on the log likelihood estimate to prove we aren’t getting exactly the same result\n\n\n\n\n\n  \n    \n    \n      Parameter\n      Built-in\n      Our Result\n    \n  \n  \n    Intercept\n5.44\n5.44\n    Life Exp. Coef.\n0.89\n0.89\n    Sigma\n0.71\n0.701\n    LogLik (neg)\n118.80\n118.80\n  \n  \n  \n    \n      1 Parameter estimate is exponentiated\n    \n  \n\n\n\n?(caption)\n\nLet’s think more about what’s going on here. It turns out that our objective function defines a space or surface. We can think of it as a landscape, and we are trying to find the lowest point on that landscape. We can then think of our guesses as points on that landscape, and we are trying to find the lowest point. Let’s start get a sense of this with the following visualization, based on a single parameter. The data is drawn from Poisson distributed variable with true mean \\(\\theta=5\\). We note the calculated likelihood increases as we estimate values for \\(\\theta\\) closer to \\(5\\), or more precisely, whatever the mean observed value is for the data. However, with more and more data, the final ML estimate will converge on the true value. Model estimation finds that maximum on the curve, and optimization algorithms are the means to find it.\n\n\n\n\n\nFigure 5.2: Likelihood function one parameter\n\n\n\n\nNow let’s add a parameter. If we have more than one parameter, we now have a surfaace to deal with. Given some starting point, an optimization procedure then travels along the surface looking for a minimum/maximum point. For simpler settings such as this, we can visualize the likelihood surface and its minimum point. However, even our simple demo model has three parameters plus the likelihood, so would be difficult to visualize without additional complexity. To get around this, we show the results for an alternate model where happiness is standardized also, which means the intercept is zero7, and we don’t have to show that.\nCANT DO INTERACTIVE WITH PDF/LATEX. NEED WORKAROUND.\n\n\n\n\n\nFigure 5.3: Likelihood surface with two parameters\n\n\n\nWe can also see the path our estimates take, starting at a rather poor point, but quickly updating to better values. We also see little exploratory jumps creating a star like pattern, before things ultimately settle to the best values. In general, these updates and paths are dependent on the optimization algorithm one uses.\n\n\n\n\n\nFigure 5.4: Optimization path two parameters\n\n\n\n\n\nIt turns out that in the case of a normal distribution, the maximum likelihood estimate of the standard deviation is the estimate as the standard deviation of the residuals. Furthermore, the maximum likelihood estimates and OLS estimates converge to the same estimates as the sample size increases. For any data of significance, these estimates are indistinguishable, and the OLS estimate is the maximum likelihood estimate for linear regression.\n\n\n5.8.0.1 Additional Thoughts on Maximum Likelihood\nNEEDS WORK\nOne of the key things to note is that maximum likelihood is an estimation technique that relies on specifying the probability distribution that serves as the data generating process. Maximum likelihood allows us to be explicit about why we think those target values are the way they are. The likelihood also serves as a fundamental part of Bayesian analysis, which we’ll discuss more later. In general, maximum likelihood is a powerful technique that can be used in many contexts, and likelihoods can be used as the objective for many machine learning algorithms as well."
  },
  {
    "objectID": "estimation.html#estimation-quick-review",
    "href": "estimation.html#estimation-quick-review",
    "title": "5  How do we obtain a model?",
    "section": "5.9 Estimation: Quick Review",
    "text": "5.9 Estimation: Quick Review\nMOVE WHERE? NEEDED?\nAt this point we understand a few things:\n\nParameters are the values associated with a model\nObjective functions specify a modeling goal with which to estimate the parameters.\nEstimation is a way of finding the best model, i.e. parameters that help us achieve a goal.\nOptimization is the process of finding the parameters that maximize or minimize some objective function\nThe likelihood is an alternate way to assess the match of data and model, and allows us to compare the relative fits of models"
  },
  {
    "objectID": "estimation.html#penalized-objectives",
    "href": "estimation.html#penalized-objectives",
    "title": "5  How do we obtain a model?",
    "section": "5.10 Penalized Objectives",
    "text": "5.10 Penalized Objectives\nMOVE TO AFTER CLASSIFICATION?\nOne thing we may want to take into account of with our models is their complexity, especially in the context of overfitting. We talk about this in the machine learning chapter also, but the basic idea is that we can get too close to the data we have, such that when we try to predict on new data, our performance suffers or even gets worse than a simpler model. In other words, we are not generalizing well. One way to deal with this is to penalize the objective function value for complexity, or at least favor simpler models that might do as well. In some contexts this is called regularization, and in other contexts shrinkage, since the values are typically shrunk toward zero.\nAs a starting point, in our basic linear model we can add a penalty that is applied to the size of coefficients, and we can control the strength of the penalty. This is called ridge regression or L2 regularization. The penalty is just the sum of the squared coefficients multiplied by a constant, which we call \\(\\lambda\\). We can write this formally as:\n\\[\n\\textrm{Value} = \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2 + \\lambda \\sum_{j=1}^{p} \\beta_j^2\n\\]\nWhere \\(y_i\\) is the actual value of the target for observation \\(i\\), and \\(\\hat{y_i}\\) is the predicted value from the model. The first part is the same as before, but the second part is the penalty for \\(p\\) features. The penalty is the sum of the squared coefficients multiplied by a constant, which we call \\(\\lambda\\). This is an additional parameter to the model that we will typically want to estimate in some fashion, e.g. through cross-validation, often called a hyperparameter, mostly just to distinguish it from those that may be of actual interest. For example, we could probably care less what the actual value for \\(\\lambda\\) is, but we would still be interested in the coefficients.\nInterestingly, as you’ll notice that this is just OLS+, you might be wondering how our results or interpretation might change. Well for starters, L2 regularization is not limited to linear regression, so just keep that in mind. But also, if we know that OLS produces unbiased estimates if assumptions of linear regression are met, that means these estimates have to be biased since they won’t be the same, right? Your are correct! As we note in the ML chapter, the bias-variance tradeoff is a key concept in machine learning, and this is a good example of that. We are introducing some bias in order to reduce the variance. In other words, we are willing to accept some bias in order to get a model that generalizes better.\nAnother common penalty that is the sum of the absolute value of the coefficients, which is called lasso regression or L1 regularization. An interersting property of the lasso is that in typical implementations, it will potentially zero out coefficients, which is the same as dropping the feature from the model altogether. This is a form of feature selection or variable selection. The true values are never zero, but if we want to use a ‘best subset’ of features, this is one way we could do so. We can write the lasso objective as:\n\\[\n\\textrm{Value} = \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2 + \\lambda \\sum_{j=1}^{p} |\\beta_j|\n\\]\nBut let’s get to a code example to make sure we understand this better! Here is an example of a function that calculates the ridge objective. To make things interesting, let’s add the other features we talked about regarding GDP per capita and perceptions of corruption.\n\nRPython\n\n\n\nridge &lt;- function(par, X, y, lambda = 0) {\n    # add a column of 1s for the intercept\n    X &lt;- cbind(1, X)\n\n    # Calculate the predicted values\n    mu &lt;- X %*% par # %*% is matrix multiplication\n\n    # Calculate the value as sum squared error\n    error &lt;- crossprod(y - mu)\n\n    # Add the penalty\n    value &lt;- error + lambda * crossprod(par)\n\n    return(value)\n}\n\nour_result &lt;- optim(\n    par = c(0, 0, 0, 0),\n    fn = ridge,\n    X = df_happiness |&gt; select(-happiness, -country) |&gt; as.matrix(),\n    y = df_happiness$happiness,\n    lambda = 0.1,\n    method = \"BFGS\"\n)\n\n\n\n\n# we use lambda_ because lambda is a reserved word in python\ndef ridge(par, X, y, lambda_ = 0):\n    # add a column of 1s for the intercept\n    X = np.c_[np.ones(X.shape[0]), X]\n\n    # Calculate the predicted values\n    mu = X @ par\n    \n    # Calculate the error\n    value = np.sum((y - mu)**2)\n    \n    # Add the penalty\n    value = value + lambda_ * np.sum(par**2)\n    \n    return(value)\n\nour_result = minimize(\n    fun  = ridge,\n    x0   = np.array([0, 0, 0, 0]),\n    args = (\n        np.array(df_happiness.drop(columns=['happiness', 'country'])),\n        np.array(df_happiness['happiness']), \n        0.1\n    )\n)\n\n\n\n\nWe can compare this to built-in functions as we have before, and can see that the results are very similar, but not exactly the same. We would not worry about such differences in practice, but the main point is again, we can use simple functions that do just about as well as any what we’d get from package output.\n\n\n\n\n\n  \n    \n    \n      Parameter\n      Built-in1\n      Our Result\n    \n  \n  \n    Intercept\n5.44\n5.44\n    Life Exp. Coef.\n0.49\n0.52\n    Corrupt\n−0.12\n−0.11\n    GDP_PC\n0.42\n0.44\n  \n  \n  \n    \n      1 Showing results from R glmnet package with alpha = 0, lambda = .1\n    \n  \n\n\n\n?(caption)\n\nCANT USE LINK WITH HASHTAG IN FIGURE CAP FOR LATEX/PDF, WILL HAVE TO USE WORKAROUND INSTEAD OF DIRECT SECTION LINK\n\n\n\n\n\n\nIt turns out that, given a a set λ penalty, ridge regression estimates need not be estimated, as there is an analytical result. See a demo."
  },
  {
    "objectID": "estimation.html#classification",
    "href": "estimation.html#classification",
    "title": "5  How do we obtain a model?",
    "section": "5.11 Classification",
    "text": "5.11 Classification\nSo far we’ve been assuming a continuous target, but what if we have a categorical target? When we want to model categorical targets, conceptually nothing changes- we can still have an objective function that maximizes or minimizes some goal. However, we need to think about how we can do this in a way that makes sense for the target.\n\n5.11.1 Misclassification\nA straightforward correspondence to MSE is a function that minimizes classification error (or maximizes accuracy). In other words, we can think of the objective function as the proportion of incorrect classifications. This is called the misclassification rate. We can write this as:\n\\[\n\\textrm{Loss} = \\frac{1}{n} \\sum_{i=1}^{n} \\mathbb{1}(y_i \\neq \\hat{y_i})\n\\]\nWhere \\(y_i\\) is the actual value of the target for observation \\(i\\), arbitrarily coded as 1 or 0, and \\(\\hat{y_i}\\) is the predicted class from the model. The \\(\\mathbb{1}\\) is an indicator function that returns 1 if the condition is true, and 0 otherwise. In other words, we are counting the number of times the predicted value is not equal to the actual value, and dividing by the number of observations.\n\nRPython\n\n\n\n# misclassification rate\nmisclassification &lt;- function(par, X, y, class_threshold = .5) {\n    X &lt;- cbind(1, X)\n    # Calculate the predicted values\n    mu &lt;- X %*% par # %*% is matrix multiplication\n\n    # Convert to a probability ('sigmoid' function)\n    p &lt;- 1 / (1 + exp(-mu))\n\n    # Convert to a class\n    predicted_class &lt;- as.integer(\n        ifelse(p &gt; class_threshold, \"good\", \"bad\")\n    )\n\n    # Calculate the error\n    error &lt;- y - predicted_class\n\n    return(mean(error))\n}\n\n\n\n\ndef misclassification_rate(par, X, y, class_threshold = .5):\n    # add a column of 1s for the intercept\n    X = np.c_[np.ones(X.shape[0]), X]\n\n    # Calculate the predicted values\n    mu = X @ par\n    \n    # Convert to a probability ('sigmoid' function)\n    p = 1 / (1 + np.exp(-mu))\n    \n    # Convert to a class\n    predicted_class = np.where(p &gt; class_threshold, 1, 0)\n    \n    # Calculate the error\n    error = y - predicted_class \n    \n    return(np.mean(error))\n\n\n\n\nWe’ll leave it as an exercise to the reader to play around with this, as the next objective function is more commonly used.\n\n\n5.11.2 Log loss\nAnother approach is to use the log loss, sometimes called logistic loss or cross-entropy. If we have just the binary case it is:\n\\[\n\\textrm{Loss} = -\\sum_{i=1}^{n} y_i \\log(\\hat{y_i}) + (1 - y_i) \\log(1 - \\hat{y_i})\n\\]\nWhere \\(y_i\\) is the actual value of the target for observation \\(i\\), and \\(\\hat{y_i}\\) is the predicted value from the model (essentially a probability). It turns out that this is the same as log-likelihood used in a maximum likelihood approach for logistic regression. We typically prefer this objective function to classification error because it is smooth like in the visualization we showed before for maximum likelihood (Figure 5.3), which means it is differentiable. This is important because it allows us to use optimization algorithms that rely on derivatives in updating the parameter estimates.\n\nRPython\n\n\n\nobjective &lt;- function(par, X, y) {\n    X &lt;- cbind(1, X)\n\n    # Calculate the predicted values on the raw scale\n    y_hat &lt;- X %*% par\n\n    # Convert to a probability ('sigmoid' function)\n    y_hat &lt;- 1 / (1 + exp(-y_hat))\n\n    # likelihood (or dbinom(y, size = 1, prob = y_hat, log = TRUE))\n    ll &lt;- y * log(y_hat) + (1 - y) * log(1 - y_hat)\n\n    return(sum(-ll))\n}\n\n\n\n\ndef objective(par, X, y):\n    # add a column of 1s for the intercept\n    X = np.c_[np.ones(X.shape[0]), X]\n\n    # Calculate the predicted values\n    y_hat = X @ par\n    \n    # Convert to a probability ('sigmoid' function)\n    y_hat = 1 / (1 + np.exp(-y_hat))\n    \n    # likelihood\n    ll = y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat)\n    \n    return(-np.sum(ll))\n\n\n\n\nLet’s go ahead and demonstrate this. Let’s go back to our movie review data, just make our current target a rating of good if the rating is 3 or greater, and bad otherwise. Our features will be the review year (starting at zero) We have a binary rating in the processed version of our data. Let’s use optim to get the best parameters for a model. We’ll compare our results to the built-in glm function to get the same results. We can see that the results are very similar, but not exactly the same. We would not worry about such differences in practice\n\nRPython\n\n\n\ndf_reviews_pr = read_csv(\"data/movie_reviews_processed.csv\")\n\nmod_logloss &lt;- optim(\n    par = c(0, 0, 0, 0),\n    fn = objective,\n    X = df_reviews_pr |&gt;\n        select(review_year_0, age_sc, word_count_sc) |&gt;\n        as.matrix(),\n    y = df_reviews_pr$rating_good\n)\n\nmod_glm &lt;- glm(\n    rating_good ~ review_year_0 + age_sc + word_count_sc,\n    data   = df_reviews_pr,\n    family = binomial\n)\n\n\n\n\nfrom scipy.optimize import minimize\n\nmod_logloss = minimize(\n    objective,\n    x0 = np.array([0, 0, 0, 0]),\n    args = (\n        df_reviews_pr[['review_year_0', 'age_sc', 'word_count_sc']], \n        df_reviews_pr['rating_good']\n    )\n)\n\nmod_glm = smf.glm(\n    'rating_good ~ review_year_0 + age_sc + word_count_sc',\n    data   = df_reviews_pr,\n    family = sm.families.Binomial()\n).fit(method = 'lbfgs')\n\n\n\n\nWe actually have to go out several decimal places before we start seeing differences between our result and the built-in function. So when it comes to classification, you should feel confident in what’s going on under the hood.\n\n\n\n\n\nTable 5.2:  Comparison of log loss results \n  \n    \n    \n      name\n      Ours\n      GLM\n    \n  \n  \n    LogLike\n622.5935\n622.5935\n    int\n−0.0819\n−0.0818\n    review_year_0\n0.0213\n0.0213\n    age_sc\n−0.2213\n−0.2213\n    word_count_sc\n−0.7344\n−0.7343"
  },
  {
    "objectID": "estimation.html#optimization-algorithms",
    "href": "estimation.html#optimization-algorithms",
    "title": "5  How do we obtain a model?",
    "section": "5.12 Optimization Algorithms",
    "text": "5.12 Optimization Algorithms\n\n5.12.1 Gradient Descent\nOne of the most common approaches in optimization is called gradient descent. The idea behind it is that we can use the gradient of the objective function to guide us to the best fitting parameters. Conceptually, this works in the exact same way as described with other estimation approaches like maximum likelihood - gradient descent is just a way to find that path along the objective surface. More formally, the gradient is the vector of partial derivatives of the objective function with respect to each parameter. That may not mean much to you, but the basic idea is that the gradient is a vector that points in the direction of steepest ascent in terms of the objective function. So if we want to maximize the objective function, we can take a step in the direction of the gradient, and if we want to minimize it, we can take a step in the opposite direction of the gradient. The size of the step is called the learning rate, and, like our penalty parameter we saw with penalized regression, it is a hyperparameter that we can tune. If the learning rate is too small, it will take a longer time to converge. If the learning rate is too large, we might overshoot the objective and never converge. There are a number of variations on gradient descent that have been developed over time. Here is a function to illustrate the process. Let’s see this in action with the happiness data model we used previously.\n\nRPython\n\n\n\ngradient_descent &lt;- function(\n    par,\n    X,\n    y,\n    tolerance = 1e-3,\n    maxit = 1000,\n    learning_rate = 1e-3,\n    adapt = FALSE,\n    verbose = TRUE,\n    plotLoss = TRUE) {\n    # add a column of 1s for the intercept\n    X &lt;- cbind(1, X)\n    N &lt;- nrow(X)\n\n    # initialize\n    beta &lt;- par\n    names(beta) &lt;- colnames(X)\n    mse &lt;- crossprod(X %*% beta - y) / N\n    tol &lt;- 1\n    iter &lt;- 1\n\n    while (tol &gt; tolerance && iter &lt; maxit) {\n        LP &lt;- X %*% beta\n        grad &lt;- t(X) %*% (LP - y)\n        betaCurrent &lt;- beta - learning_rate * grad\n        tol &lt;- max(abs(betaCurrent - beta))\n        beta &lt;- betaCurrent\n        mse &lt;- append(mse, crossprod(LP - y) / N)\n        iter &lt;- iter + 1\n\n        if (adapt) {\n            stepsize &lt;- ifelse(\n                mse[iter] &lt; mse[iter - 1],\n                stepsize * 1.2,\n                stepsize * .8\n            )\n        }\n\n        if (verbose && iter %% 10 == 0) {\n            message(paste(\"Iteration:\", iter))\n        }\n    }\n\n    if (plotLoss) {\n        p &lt;- tibble(mse) |&gt;\n            mutate(iter = 1:n()) |&gt;\n            ggplot(aes(iter, mse)) +\n            geom_hline(yintercept = 0) +\n            geom_line() +\n            scale_x_continuous(breaks = seq(0, 50, 10)) +\n            scale_y_continuous(breaks = seq(0, round_any(max(mse), 10), 5)) +\n            labs(x = \"Iteration\", y = \"MSE\")\n        print(p)\n    }\n\n    list(\n        par    = beta,\n        loss   = mse,\n        MSE    = crossprod(LP - y) / nrow(X),\n        iter   = iter,\n        fitted = LP\n    )\n}\n\nour_result &lt;- gradient_descent(\n    par = c(0, 0, 0, 0),\n    X = df_happiness |&gt; select(life_exp, gdp_pc, corrupt) |&gt; as.matrix(),\n    y = df_happiness$happiness,\n    learning_rate = 1e-3,\n    verbose = FALSE,\n    plot = FALSE # shown later\n)\n\n\n\n\ndef gradient_descent(\n    par, \n    X, \n    y, \n    tolerance = 1e-3, \n    maxit = 1000, \n    learning_rate = 1e-3, \n    adapt = False, \n    verbose = True, \n    plotLoss = True\n):\n    # add a column of 1s for the intercept\n    X = np.c_[np.ones(X.shape[0]), X]\n    \n    # initialize\n    beta = par\n    loss = np.sum((X @ beta - y)**2)\n    tol = 1\n    iter = 1\n\n    while (tol &gt; tolerance and iter &lt; maxit):\n        LP = X @ beta\n        grad = X.T @ (LP - y)\n        betaCurrent = beta - learning_rate * grad\n        tol = np.max(np.abs(betaCurrent - beta))\n        beta = betaCurrent\n        loss = np.append(loss, np.sum((LP - y)**2))\n        iter = iter + 1\n\n        if (adapt):\n            stepsize = np.where(loss[iter] &lt; loss[iter - 1], stepsize * 1.2, stepsize * .8)\n\n        if (verbose and iter % 10 == 0):\n            print(\"Iteration:\", iter)\n\n    if (plotLoss):\n        plt.plot(loss)\n        plt.show()\n\n    return({\n        \"par\": beta,\n        \"loss\": loss,\n        \"RSE\": np.sqrt(np.sum((LP - y)**2) / (X.shape[0] - X.shape[1])),\n        \"iter\": iter,\n        \"fitted\": LP\n    })\n\nour_result = gradient_descent(\n    par = np.array([0, 0, 0, 0]),\n    X = df_happiness[['life_exp', 'gdp_pc', 'corrupt']].to_numpy(),\n    y = df_happiness['happiness'].to_numpy(),\n    learning_rate = 1e-3,\n    verbose  = False,\n    plotLoss = False # will show below\n)\n\n\n\n\nComparing our results, we have the following table. Again, we see that the results are very similar.\n\n\n\n\n\nTable 5.3:  Comparison of gradient descent results \n  \n    \n    \n      Value\n      Built-in\n      Our Result\n    \n  \n  \n    Intercept\n5.445\n5.437\n    Life Exp. Coef.\n0.525\n0.521\n    GDP_PC\n0.438\n0.439\n    Corrupt\n−0.105\n−0.107\n    MSE\n0.367\n0.367\n  \n  \n  \n\n\n\n\n\nIn addition, when we visualize the loss function across iterations, we see smooth decline in the MSE value as we go along.\n\n\n\n\n\nFigure 5.5: Gradient descent path\n\n\n\n\n\n\n5.12.2 Stochastic Gradient Descent\nStochastic gradient descent is a variation on gradient descent that uses a random sample of the data to estimate the gradient, while the true gradient is the gradient of the objective function with respect to all of the data. As such, the stochastic gradient descent is less accurate than gradient descent. The advantage of stochastic gradient descent is that it is faster than gradient descent. In practice, stochastic gradient descent is often used in machine learning applications where the data is large, and the tradeoff between accuracy and speed is worth it.\nLet’s see this in action with the happiness data model we used previously. The following is a conceptual version of the AdaGrad approach8, which is a variation of stochastic gradient descent that adjusts the learning rate for each parameter. We will also add a variation that averages the parameter estimates across iterations, which is a common approach to improve the performance of stochastic gradient descent, but by default it is not used, just something you can play with. We are going to use a ‘batch size’ of one, which is similar to a ‘streaming’ or ‘online’ version where we update the model with each observation. Since our data are alphabetically ordered, we’ll shuffle the data first. We’ll also use a stepsize_tau parameter, which is a way to adjust the learning rate at early iterations. We’ll set it to zero for now, but you can play with it to see how it affects the results. The values for the learning rate and stepsize_tau are arbitrary, selected after some initial playing around, but you can play with them to see how they affect the results.\nNOTE: SHOULD MAYBE CLEAN UP/ALTER TO LESS VERBOSE VERSION\n\nRPython\n\n\n\nstochastic_gradient_descent &lt;- function(\n    par, # parameter estimates\n    X, # model matrix\n    y, # target variable\n    learning_rate = 1, # the learning rate\n    stepsize_tau = 0, # if &gt; 0, a check on the LR at early iterations\n    average = FALSE # a variation of the approach\n    ) {\n    # initialize\n    X &lt;- cbind(1, X)\n    beta &lt;- par\n\n    # Collect all estimates\n    betamat &lt;- matrix(0, nrow(X), ncol = length(beta))\n\n    # Collect fitted values at each point))\n    fits &lt;- NA\n\n    # Collect loss at each point\n    loss &lt;- NA\n\n    # adagrad per parameter learning rate adjustment\n    s &lt;- 0\n\n    # a smoothing term to avoid division by zero\n    eps &lt;- 1e-8\n\n    for (i in 1:nrow(X)) {\n        Xi &lt;- X[i, , drop = FALSE]\n        yi &lt;- y[i]\n\n        # matrix operations not necessary here,\n        # but makes consistent with standard gd func\n        LP &lt;- Xi %*% beta\n        grad &lt;- t(Xi) %*% (LP - yi)\n        s &lt;- s + grad^2 # adagrad approach\n\n        # update\n        beta &lt;- beta - learning_rate / (stepsize_tau + sqrt(s + eps)) * grad\n\n        # a variation\n        if (average & i &gt; 1) {\n            beta &lt;- beta - 1 / i * (betamat[i - 1, ] - beta)\n        }\n\n        betamat[i, ] &lt;- beta\n        fits[i] &lt;- LP\n        loss[i] &lt;- crossprod(LP - yi)\n    }\n\n    LP &lt;- X %*% beta\n    lastloss &lt;- crossprod(LP - y)\n\n    list(\n        par = beta, # final estimates\n        par_chain = betamat, # estimates at each iteration\n        MSE = sum(lastloss) / nrow(X),\n        fitted = LP\n    )\n}\n\n# setting a seed ensures replicability\nset.seed(123)\n\n# generate random sample indices (could also have done within the function)\nidx &lt;- sample(1:nrow(df_happiness), nrow(df_happiness))\n\nX_train = df_happiness |&gt;\n    select(life_exp, gdp_pc, corrupt) |&gt;\n    dplyr::slice(idx) |&gt;\n    as.matrix()\n\ny_train &lt;- df_happiness$happiness[idx]\n\nour_result &lt;- stochastic_gradient_descent(\n    par = c(mean(df_happiness$happiness), 0, 0, 0),\n    X = X_train,\n    y = y_train,\n    learning_rate = .15,\n    stepsize_tau = .1\n)\n\n\n\n\ndef stochastic_gradient_descent(\n    par, # parameter estimates\n    X, # model matrix\n    y, # target variable\n    learning_rate = 1, # the learning rate\n    stepsize_tau = 0, # if &gt; 0, a check on the LR at early iterations\n    average = False # a variation of the approach\n):\n    # initialize\n    X = np.c_[np.ones(X.shape[0]), X]\n    beta = par\n\n    # Collect all estimates\n    betamat = np.zeros((X.shape[0], beta.shape[0]))\n\n    # Collect fitted values at each point))\n    fits = np.zeros(X.shape[0])\n\n    # Collect loss at each point\n    loss = np.zeros(X.shape[0])\n\n    # adagrad per parameter learning rate adjustment\n    s = 0\n\n    # a smoothing term to avoid division by zero\n    eps = 1e-8\n\n    for i in range(X.shape[0]):\n        Xi = X[None, i, :]\n        yi = y[i]\n\n        # matrix operations not necessary here,\n        # but makes consistent with standard gd func\n        LP = Xi @ beta\n        grad = Xi.T @ (LP - yi)\n        s = s + grad**2 # adagrad approach\n\n        # update\n        beta = beta - learning_rate / (stepsize_tau + np.sqrt(s + eps)) * grad\n\n        # a variation\n        if (average & i &gt; 1):\n            beta = beta - 1 / i * (betamat[i - 1, :] - beta)\n\n        betamat[i, :] = beta\n        fits[i] = LP\n        loss[i] = np.sum((LP - yi)**2)\n\n    LP = X @ beta\n    lastloss = np.sum((LP - y)**2)\n\n    return({\n        \"par\": beta, # final estimates\n        \"par_chain\": betamat, # estimates at each iteration\n        \"MSE\": lastloss / X.shape[0],\n        \"fitted\": LP\n    })\n\n# setting a seed ensures replicability\nnp.random.seed(1234)\n\n# generate random sample indices (could also have done within the function)\nidx = np.random.choice(df_happiness.shape[0], df_happiness.shape[0], replace = False)\n\nX_train = df_happiness[['life_exp', 'gdp_pc', 'corrupt']].to_numpy()[idx, :]\ny_train = df_happiness['happiness'].to_numpy()[idx]\n\nour_result = stochastic_gradient_descent(\n    par = np.array([np.mean(df_happiness['happiness']), 0, 0, 0]),\n    X = X_train,\n    y = y_train,\n    learning_rate = .15,\n    stepsize_tau = .1\n)\n\n\n\n\nNext we’ll compare it to OLS estimates. Very similar even though SGD normally would not be used for such a small dataset.\n\n\n\n\n\nTable 5.4:  Comparison of stochastic gradient descent results \n  \n    \n    \n      Value\n      Built-in\n      Our Result\n    \n  \n  \n    Intercept\n5.44\n5.47\n    Life Exp. Coef.\n0.52\n0.51\n    GDP_PC\n0.44\n0.39\n    Corrupt\n−0.11\n−0.11\n    MSE\n0.37\n0.37\n  \n  \n  \n\n\n\n\n\nAnd here’s a plot of the estimates as they moved along the data. For this plot we don’t include the intercept as it’s on a notably different scale. We can see that the estimates are moving around a bit, but they appear to be converging to a solution.\n\n\n\n\n\nFigure 5.6: Stochastic gradient descent path\n\n\n\n\n\n\n\n5.12.3 Other Optimization Algorithms\nThere are lots of other approaches to optimization. For example, here are some of the options available in R’s optim or scipy’s minimize function:\n\nNelder-Mead\nBFGS\nL-BFGS-B (provides constraints)\nConjugate gradient\nSimulated annealing\nNewton’s method\nGenetic algorithms\n\nThe main reason to choose one method over another usually is some sort of computational gain, e.g. memory or speed, or it may just work better for some types of models in practice. For statistical problems, many GLM-type functions appear to use Newton’s as a default, but more complicated models may implement other means, and even then might best be served by a different approach. In general, we can always try a few different methods to see which works best, and often there would be little differences in the results. For example, here are the results from the happiness data using the BFGS method, which is a quasi-Newton method. Here is our resuls from using OLS, compared to different algorithms, some we used some we didn’t. We can see that the results are very similar, and for simpler modeling endeavors they should converge on the same result.\n\n\n\n\n\nTable 5.5:  Comparison of optimization results \n  \n    \n    \n      parameter\n      NM1\n      BFGS2\n      CG3\n      GD4\n      Built-in\n    \n  \n  \n    Intercept\n5.445\n5.445\n5.445\n5.437\n5.445\n    Life Exp. Coef.\n0.525\n0.525\n0.525\n0.521\n0.525\n    GDP_PC\n0.437\n0.438\n0.438\n0.439\n0.438\n    Corrupt\n−0.105\n−0.105\n−0.105\n−0.107\n−0.105\n    MSE\n0.367\n0.367\n0.367\n0.367\n0.367\n  \n  \n  \n    \n      1 NM = Nelder-Mead\n    \n    \n      2 BFGS = Broyden–Fletcher–Goldfarb–Shanno\n    \n    \n      3 CG = Conjugate gradient\n    \n    \n      4 GD = Gradient descent"
  },
  {
    "objectID": "estimation.html#other-estimation-approaches",
    "href": "estimation.html#other-estimation-approaches",
    "title": "5  How do we obtain a model?",
    "section": "5.13 Other Estimation Approaches",
    "text": "5.13 Other Estimation Approaches\nBefore leaving our estimation discussion, we should mention that there are many other approaches to estimation that are out there. These include variaions on least squares, method of moments, generalized estimating equations, robust estimation, and more. The above that we’ve focused on will generally be sufficient for most applications, but it’s good to be aware of others. But there are two we want to discuss in a little bit detail before we leave model estimation formally given their widespread usage, and that is the bootstrap and Bayesian estimation.\n\n5.13.1 Bootstrap\nThe bootstrap is a resampling approach to estimation. We sample with replacement from the data, generating an entirely new data set of the same size, and then estimate the model. We repeat this process many times, collecting parameter estimates, predictions, or any thing we want to calculate along the way. Ultimately we end up with a distribution of possible parameter estimates.\nThis distribution is useful for inference, as we can use the distribution to calculate confidence intervals, prediction intervals or intervals for anything we happen to calculate. The average estimate will typically be the same as whatever the underlying model used would produced, but the bootstrap provides a way to get at a measure of uncertainty with fewer assumptions about how that distribution should take sahape. The bootstrap is very flexible, and it can be used with any estimation approach. Here is a function to illustrate the process. Let’s see this in action with the happiness data model we used previously.\n\nRPython\n\n\n\nbootstrap &lt;- function(X, y, nboot = 100, seed = 123) {\n    # add a column of 1s for the intercept\n    N &lt;- nrow(X)\n\n    # initialize\n    beta &lt;- matrix(NA, (1+ncol(X))*nboot, nrow = nboot, ncol = 1+ncol(X))\n    colnames(beta) &lt;- c('Intercept', colnames(X))\n    mse &lt;- rep(NA, nboot)\n\n    # set seed\n    set.seed(seed)\n\n    for (i in 1:nboot) {\n        # sample with replacement\n        idx &lt;- sample(1:N, N, replace = TRUE)\n        Xi &lt;- X |&gt; slice(idx)\n        yi &lt;- y[idx]\n\n        # estimate model\n        mod &lt;- lm(yi ~., data = Xi)\n\n        # save results\n        beta[i, ] &lt;- coef(mod)\n        mse[i] &lt;- sum((mod$fitted - yi)^2) / N\n    }\n\n    # given mean estimates, calculate MSE\n    y_hat = cbind(1, as.matrix(X)) %*% colMeans(beta)\n    final_mse &lt;- sum((y - y_hat)^2) / N\n\n    list(\n        beta = as_tibble(beta),\n        MSE = mse,\n        final_mse = final_mse\n    )\n}\n\nour_result &lt;- bootstrap(\n    X = df_happiness |&gt; select(life_exp, gdp_pc, corrupt),\n    y = df_happiness$happiness,\n    nboot = 250\n)\n\n\n\n\ndef bootstrap(X, y, nboot=100, seed=123):\n    cn = X.columns\n    # add a column of 1s for the intercept\n    X = np.c_[np.ones(X.shape[0]), X]\n    N = X.shape[0]\n\n    # initialize\n    beta = np.empty((nboot, X.shape[1]))\n    \n    # beta = pd.DataFrame(beta, columns=['Intercept'] + list(cn))\n    mse = np.empty(nboot)    \n\n    # set seed\n    np.random.seed(seed)\n\n    for i in range(nboot):\n        # sample with replacement\n        idx = np.random.randint(0, N, N)\n        Xi = X[idx, :]\n        yi = y[idx]\n\n        # estimate model\n        model = LinearRegression(fit_intercept=False)\n        mod = model.fit(Xi, yi)\n\n        # save results\n        beta[i, :] = mod.coef_\n        mse[i] = np.sum((mod.predict(Xi) - yi)**2) / N\n\n    # given mean estimates, calculate MSE\n    y_hat = X @ beta.mean(axis=0)\n    final_mse = np.sum((y - y_hat)**2) / N\n\n    return dict(beta = beta, mse = mse, final_mse = final_mse)\n\nour_result = bootstrap(\n    X = df_happiness[['life_exp', 'gdp_pc', 'corrupt']],\n    y = df_happiness['happiness'],\n    nboot = 250\n)\n\n\n\n\nHere are the results of the interval estimates for the coefficients. For each parameter, we have the mean estimate, the lower and upper bounds of the 95% confidence interval, and the width of the interval. We can see that the bootstrap intervals are wider than the OLS intervals, possibly better capturing the uncertainty in this model based on not too many observations.\n\n\n\n\n\nTable 5.6:  Bootstrap parameter estimates \n  \n    \n    \n      Parameter\n      mean\n      lower\n      upper\n      Lower OLS\n      Upper OLS\n      Diff Width1\n    \n  \n  \n    Intercept\n5.44\n5.33\n5.55\n5.33\n5.56\n0.23\n    life_exp\n0.51\n0.30\n0.70\n0.35\n0.70\n0.35\n    gdp_pc\n0.46\n0.18\n0.76\n0.24\n0.64\n0.40\n    corrupt\n−0.10\n−0.29\n0.09\n−0.25\n0.04\n0.28\n  \n  \n  \n    \n      1 Width of bootstrap estimate minus width of OLS estimate\n    \n  \n\n\n\n\n\nLet’s look at the bootstrap distributions for each coefficient. With standard statistical estimates, we are assuming a distribution like the normal, which is a very specific shape. With the bootstrap, we can be more flexible, thought often it may tend toward the distribution that would otherwise be assumed. These aren’t perfectly symmetrical, but they suit our needs in that we can extract the lower and upper quantiles to create an interval estimate.\n\n\n\n\n\nFigure 5.7: Bootstrap distributions of parameter estimates\n\n\n\n\nThe bootstrap is a commonly used for predictions and other metrics, but it is computationally inefficient, and can become prohibitive with large data sizes. Also, the simple bootstrap will likely not estimate the appropriate uncertainty for some types of statistics (e.g. extreme values) or in some data contexts (e.g. correlated observations). Overcoming the limitations may typically require an even more computationally intensive approach, further limiting its utility. But it is a useful tool to have in your toolbox, and it can be used in conjunction with other approaches to get at uncertainty in a model.\n\n\n5.13.2 Bayesian Estimation\nThe bayesian approach to modeling is a philosophical view point, an entirely different way to think about probability, a different way to measure uncertainty, and on a practical level, just another way to get model parameter estimates. It can be as frustrating as it is fun to use, and one of the really nice things about using bayesian estimation is that it can handle model complexities that other approaches don’t do well.\nThe basis of bayesian estimation is the likelihood, same as with maximum likelihood, and everything we did there follows to here. However, here we can incorporate domain knowledge, in the form of prior distributions about the parameters, which we specify in addition to the likelihood. For example, we may say that the coefficients for a linear model come from a normal distribution centered on zero with some variance. The combination prior distributions with the likelihood ultimately results in the posterior distribution. And this is the key difference when comparing bayesian estimation to the others we’ve talked about, and something it shares in common with the bootstrap- the end result is not a point estimate of the parameters, but rather a distribution of possible parameter values.\n\nDealing with distributions instead of single estimates is a different way to think about modeling, but it can be very useful. For example, as we did with the bootstrap, the bayesian posterior distribution is useful for inference. With these distributions, we can look at any range in between for our credible interval, which is the bayesian equivalent of a confidence interval9. Here is an example of the posterior distribution for the parameters of our happiness model, along with 95% intervals.\n\n\n\n\n\nFigure 5.8: Posterior distribution of parameters\n\n\n\n\nWith bayesian modeling, we use the bayesian algorithm of our choosing, give it starting values and proceed much in the same way as other optimization procedures. In the bayesian approach, we always specify a number of iterations as the stopping rule, i.e. when the model should terminate. These iterations are single draws from the posterior distribution for each parameter. So if we specified 1000 iterations, we would have 1000 draws from the posterior distribution for each parameter. Typically we don’t use the first few hundred draws, as these are considered burn-in or warmup draws, and we use the remaining draws for inference. The number of burn-in draws is a bit of an art, but it’s not too important as long as it’s not too small. The more iterations we set, the longer it will take to run. We also specify multiple chains, which are each doing the exact same thing, but do to the random nature of the bayesian approach, would take different estimation paths. We can then compare the chains to see if they are converging to the same result, which is a check on the model. If they are not converging, we may need to run the model longer, or we may need to change something else. Here is an example of the chains for our happiness model for the life expectancy coefficient. We can see that they are converging to the same result, so we are good to go. Nowadays we have statistics that allow us to check whether the chains are converging, making it easier to assess many parameters quickly.\n\n\n\n\n\nFigure 5.9: Bayesian chains for life expectancy coefficient\n\n\n\n\nWhen we are interested in making predictions, we can use the results to generate a distribution of possible predictions for each observation, which can be very useful when we want to quantify uncertainty in for complex models. This is referred to as posterior predictive distribution. Here is a plot of several draws of predicted values against the true happiness scores.\n\n\n\n\n\nFigure 5.10: Posterior predictive distribution of happiness values\n\n\n\n\nNote that any metric we can calculate from a model will also have a distribution. For example, you have a classification model and you want to know the accuracy or true positive rate of the model. Instead of a single number, you now have access to a distribution of values for that metric. Why? For every sample of the distribution of parameters, you generate a prediction, convert it a class and compare it to the true class. So now you have a posterior predictive distribution for the predicted probabilities and class, and you can then calculate the accuracy, area under a receiver operating curve, true positive rate, etc., for each sample, and you have a distribution of possible values. As an example, we did this for our happiness model and show the interval estimate for R2. Pretty neat!\n\n\n\n\n\nTable 5.7:  Bayesian R2 \n  \n    \n    \n      Bayes R2\n      Lower\n      Upper\n    \n  \n  \n    0.71\n0.65\n0.75\n  \n  \n  \n    \n       95% Credible interval for R-squared\n    \n  \n\n\n\n\n\n\n\n\n\n\n\nThere is nothing keeping you from doing posterior predictive checks with other estimation approaches, and it’s a good idea to do so. For example, in a GLM you have the beta estimates and the covariance matrix for them, and can simulate from a normal distribution with those estimates. But it’s a bit more straightforward with the bayesian approach, and some pacakges will allow you to do this automatically even.\n\n\n\n\n5.13.2.1 Additional Thoughts\nIt turns out that any standard (frequentist) statistical model can be seen as a bayesian one from a particular point of view. Here are a couple:\n\nGLM and related estimated via maximum likelihood: Bayesian estimation with a flat/uniform prior on the parameters.\nRidge Regression: Bayesian estimation with a normal prior on the coefficients, penalty parameter is related to the variance of the prior\nLasso Regression: Bayesian estimation with a Laplace prior on the coefficients, penalty parameter is related to the variance of the prior\n\nSo in many modeling contexts, you’re probably doing a restrictive form of bayesian estimation already. Hopefully this helps to demystify the bayesian approach a bit, and you feel more comfortable switching to it. R has excellent tools here, like brms and tidybayes, and Python has pymc3 and arviz, which are also useful.\nWe can see that the bayesian approach is very flexible, and can be used for many different types of models, and can be used to get at uncertainty in a model in ways that other approaches can’t. It’s not a panacea, and it’s not always the best approach, but it’s a good one to have in your toolbox.\nWHERE TO PUT THIS PRIOR STUFF?\nThe tough part about the bayesian approach is specifying priors, but even when you don’t have a great idea, many have offered solutions, and there are ways to check whether what you’ve chosen makes sense for your data before trying the model itself.\n\n\n\n\n\n\nSpecification of priors can be done in different ways, and nowadays, there is a lot of information on how to do so, and with some tools, it’s also pretty straightforward to check whether the priors are sensible without even running a model. When you do have actual prior knowledge, either domain knowledge (e.g. a prior study found the beta values to be positive), statistical knowledge, (e.g. only the largest standard coefficients go near or beyond 1), data from time periods, there’s typically at least something to help you specify your priors with sensible values. This takes away most of the luster of the primary argument against the bayesian approach, which is the subjective nature of priors. But there is likewise so much subjective decision making in other approaches, that it’s not really a useful argument to begin with. The bayesian approach just makes it more explicit. And if you don’t have any prior knowledge, you can use non- or weakly- informative priors, which will likely have little influence and let the data do the talking, producing a result that is not that different from maximum likelihood estimation.\n\n\n\nMOVE MH EXAMPLE TO APPENDIX\nMetropolis-Hastings demo"
  },
  {
    "objectID": "estimation.html#commentary",
    "href": "estimation.html#commentary",
    "title": "5  How do we obtain a model?",
    "section": "5.14 Commentary",
    "text": "5.14 Commentary"
  },
  {
    "objectID": "estimation.html#refs",
    "href": "estimation.html#refs",
    "title": "5  How do we obtain a model?",
    "section": "5.15 Refs",
    "text": "5.15 Refs\nMaximum Likelihood\nSGD\nGradient Descent, Step-by-Step Stochastic Gradient Descent, Clearly Explained!!! https://www.databricks.com/glossary/adagrad https://machinelearningmastery.com/gradient-descent-with-adagrad-from-scratch/\nBayesian\n\nBDA\nStatistical Rethinking\nPrior Specification"
  },
  {
    "objectID": "machine_learning.html#introduction",
    "href": "machine_learning.html#introduction",
    "title": "6  Machine Learning",
    "section": "6.1 Introduction",
    "text": "6.1 Introduction\nMachine learning is used everywhere, and allows us to do things that would have been impossible just a couple decades ago. It is used in everything from self-driving cars, to medical diagnosis, to predicting the next word in your text message. The ubiquity of it is such that it, and related adventures like artificial intelligence, are used as buzzwords, and it is not always clear what it meant by the one speaking them. In this chapter we hope you’ll come away with a better understanding of what machine learning is, and how it can be used in your own work. Because however you define, it sure can be fun!\nMachine learning is a branch of data analysis with a primary focus on predictive performance. Honestly, that’s pretty much it from a practical standpoint. It is not a subset of particular types of models, it does not preclude using statistical models, it doesn’t mean that a program spontaneously learns without human involvement1, it doesn’t necessarily have anything to do with ‘machines’ outside of laptop, and it doesn’t even mean that the model is particularly complex. Machine learning, at its core, is a set of tools and a modeling approach that attempts to maximize and generalize performance, and compare models based on that performance2.\nThis is a different focus than statistical modeling approaches that put much more emphasis on interpreting coefficients and uncertainty. But it is not an exclusive one. Some implementations of machine learning include models that have their basis in traditional statistics, while others are often sufficiently complex that they are scarcely interpretable without a lot of effort, or are used in contexts where interpretation simply isn’t important. However, even after you conduct your modeling via machine learning, you may still fall back on statistical analysis for further exploration of the results. For example, you may want to know the uncertainty of your performance metric, or if the model is significantly better than another model. In any event, here we will also discuss some of the key ideas in machine learning, such as model assessment, loss functions, and cross-validation. Later we’ll demonstrate common models used, but if you want to dive in, you can head there now!\n\n\n\n\n\n\nML by any other name… AI, statistical learning, data mining, predictive analytics, data science, BI, there are a lot of names used alongside or even interchangeably with machine learning. It’s mostly worth noting that using ‘machine learning’ without context makes it very difficult to know what tools have actually been employed, so you may have to do a bit of digging to find out the details."
  },
  {
    "objectID": "machine_learning.html#key-ideas",
    "href": "machine_learning.html#key-ideas",
    "title": "6  Machine Learning",
    "section": "6.2 Key ideas",
    "text": "6.2 Key ideas\n\nMachine learning is not a set of modeling techniques, but rather a modeling focus on predictive performance, and a set of tools and methods to achieve that.\nModels used in machine learning are typically more complex and difficult to interpret than those used in standard statistical models, but any model, including classical statistical ones, can be used with ML.\nThere are many performance metrics used in machine learning, and care should be taken to choose the appropriate one for your situation.\nObjective functions likewise should be chosen for the situation, and are often different than the performance metric.\nMultiple performance metrics are able to be used for any given model assessment scenario.\nRegularization is a general approach to penalize complexity in a model, and is typically used to prevent overfitting in order to improve generalization.\nCross-validation is a method that allows us to select parameters and hyperparameters for our models, and to compare models to one another by assessing a model’s performance on data that was not used to fit the model. \n\n\n6.2.1 Why this matters\nMachine learning applications help define the modern world and how we interact with it. There are few aspects of modern society that have not been touched by it in some way. By understanding the basic ideas behind machine learning, you will be able to understand the models and techniques that are used in these applications, and be able to apply them to your own work. You’ll also be able to understand the limitations of these models."
  },
  {
    "objectID": "machine_learning.html#general-approach",
    "href": "machine_learning.html#general-approach",
    "title": "6  Machine Learning",
    "section": "6.3 General Approach",
    "text": "6.3 General Approach\nLet’s start with a general approach to machine learning to help us get some bearings. Here is a (very) general outline of the process we could take.\n\nDefine the problem, including the target variable(s)\nSelect the model(s) to be used, including one baseline model\nDefine the performance objective and metric(s) used for model assessment\nDefine the search space (parameters, hyperparameters) for those models\nDefine the search method (optimization)\nImplement some sort of cross-validation technique and collect the corresponding performance metrics\nEvaluate the results on unseen data with the chosen model\n\nHere is a more concrete example:\n\nDefine the problem: predict the price of a house\nSelect the model(s) to be used: ridge regression, standard regression with no penalty as baseline\nDefine the objective and performance metric(s): (R)MSE, R-squared\nDefine the search space (parameters, hyperparameters) for those models: penalty parameter\nDefine the search method (optimization): grid search\nImplement some sort of cross-validation technique: 5-fold cross-validation\nEvaluate the results on unseen data: RMSE on test data\n\nAs we go along in this chapter, we’ll get a better sense of what these things mean, but for now, let’s just get a sense of the general process."
  },
  {
    "objectID": "machine_learning.html#concepts",
    "href": "machine_learning.html#concepts",
    "title": "6  Machine Learning",
    "section": "6.4 Concepts",
    "text": "6.4 Concepts\n\n6.4.1 Objective Functions\nWe’ve implmented a variety of objective functions in other chapters, such as mean squared error for numeric targets and log loss for binary targets. As we have also noted elsewhere, the objective function is not necessarily the same as the performance metric we ultimately use to select a model. For example, we may use log loss as the objective function, but then use accuracy as the performance metric. In that setting, the log loss provides a ‘smooth’ objective function to search the parameter space over, while accuracy is a straightforward and more interpretable metric for stakeholders. In this case, the objective function is used to optimize the model, while the performance metric is used to evaluate the model. In some cases, the objective function and performance metric are the same (e.g. (R)MSE), and even if not, they might have selected the same ‘best’ model, but this is not always the case.\n\n\n\n\n\nTable 6.1:  Commonly used objective functions in machine learning for standard\nregression and classification tasks. \n  \n    \n      \n    \n    \n    \n      Objective Function\n      Description\n    \n  \n  \n    \n      Regression\n    \n    Mean Squared Error (MSE)\nCalculates the average of the squared differences between the predicted and actual values.\n    Mean Absolute Error (MAE)\nCalculates the average of the absolute differences between the predicted and actual values.\n    Huber Loss\nLess sensitive to outliers than MSE.\n    Log Likelihood\nUsed for models where the response variable follows a known distribution, and we want to maximize the likelihood of observing the data given the model parameters.\n    \n      Classification\n    \n    Binary Cross-Entropy / Log Likelihood (Loss)\nUsed for binary classification problems. Calculates the negative log-likelihood of the class labels given the predicted probabilities.\n    Categorical Cross-Entropy\nUsed for multi-class classification problems. Calculates the negative log-likelihood of the class labels given the predicted probabilities.\n  \n  \n  \n\n\n\n\n\nFor specific types of tasks, such as predicting ranks, you might use something else, but the above will apply in some of the most common settings. Even when dealing with different types of outcomes, such as counts, proportions, etc., one can typically use a likelihood objective. Recall that ‘maximum likelihood’ functions can be turned into a minimization problem by taking the negative (log) likelihood.\n\n\n6.4.2 Performance Metrics\nThere are many performance metrics used in machine learning, and care should be taken to choose the appropriate one for your situation. Typically we have a standard set we might use for the type of predictive problem. For example for numeric targets, we typically are interested in (R)MSE and MAE, but given a particular scenario we might want to switch to something else. For example, if we are predicting a count type of target, we might use Poisson deviance, which is akin to a Poisson GLM in statistical modeling.\nMost classification-based metrics are based on the confusion matrix, which is a table of the predicted classes versus the observed classes. Here is an example.\n\n\n\n\nExample Confusion Matrix\n  \n    \n    \n      Predicted\n      Observed Negative\n      Observed Positive\n    \n  \n  \n    Negative\n62\n10\n    Positive\n10\n18\n  \n  \n  \n\n\n\n\nThe diagonal of the confusion matrix is the number of correct predictions, and the off-diagonal is the number of incorrect predictions. In this particular example we have an accuracy of 80%. However, there are many metrics we can calculate from this simple confusion matrix, and many of these can also be extended to the multiclass setting. For example, we can calculate the accuracy in general as well as for each class.\n\n\n\n\n\nTable 6.2:  Commonly used performance metrics in machine learning. \n  \n    \n      \n    \n    \n    \n      Metric\n      Description\n      Other Names/Notes\n    \n  \n  \n    \n      Regression\n    \n    RMSE\nRoot mean squared error\nMSE (before square root)\n    MAE\nMean absolute error\n\n    MAPE\nMean absolute percentage error\n\n    RMSLE\nRoot mean squared log error\n\n    R-squared\nAmount of variance shared by predictions and observed target\nCoefficient of determination\n    Deviance/AIC\nGeneralization of sum of squared error for non-continuous/gaussian settings\nAlso \"deviance explained\" for similar R-sq interpretation\n    Pinball Loss\nQuantile loss function\nMAE if estimating median\n    \n      Classification\n    \n    Accuracy\nPercent correct\nError rate is 1 - Accuracy\n    Precision\nPercent of positive predictions that are correct\nPositive Predictive Value\n    Recall\nPercent of positive samples that are predicted correctly\nSensitivity, True Positive Rate\n    Specificity\nPercent of negative samples that are predicted correctly\nTrue Negative Rate\n    Negative Predictive Value\nPercent of negative predictions that are correct\n\n    F1\nHarmonic mean of precision and recall\nF-Beta1\n    Lift\nRatio of percent positive predictions to percent positive samples\n\n    AUC\nArea under the ROC curve\n\n    Type I Error\nFalse Positive Rate\nalpha\n    Type II Error\nFalse Negative Rate\nbeta, Power is 1 - beta\n    False Discovery Rate\nPercent of positive predictions that are incorrect\n\n    False Omission Rate\nPercent of negative predictions that are incorrect\n\n    False Positive Rate\nPercent of negative samples that are predicted incorrectly\n\n    False Negative Rate\nPercent of positive samples that are predicted incorrectly\n\n    Phi\nCorrelation between predicted and actual\nMatthews Correlation\n    Log loss\nNegative log likelihood of the predicted probabilities\n\n  \n  \n  \n    \n      1 Beta = 1 for F1\n    \n  \n\n\n\n\n\n\n\n6.4.3 Generalization\nOne of the key differences separating ML from traditional statistical modeling approaches is the assessment on unseen or future data, a concept commonly referred to as generalization. The basic idea is that we want to build a model that will perform well on new data, and not just the data we used to fit the model, because ultimately data is ever evolving, and we don’t want to be beholden to a particular set of data we just happened to have at a particular time.\nBut how do we do this? For starters, we can simply split our data into two sets, a training set and a test set, with the latter being typically a smaller subset, say 25%, but this amount is arbitrary. We fit the model on the training set, and then use the model to make predictions on the test set. This is also known as the holdout method. Consider a simple linear regression. We can fit the linear regression model on the training set, which provides us coefficients, etc. We can then use that model result to predict on the test set, and then compare the predictions to the actual values in the test set. Here we demonstrate this with our simple linear model from before.\n\nPythonR\n\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nimport pandas as pd\n\ndf_movie_reviews = pd.read_csv(\"data/movie_reviews_processed.csv\")\n\nX = df_movie_reviews[\n    [\n        'word_count',\n        'age',\n        'review_year',\n        'release_year',\n        'length_minutes',\n        'children_in_home',\n        'total_reviews',\n    ]\n]\n\ny = df_movie_reviews['rating']\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.25, random_state=123\n)\n\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n# get predictions\ny_pred = model.predict(X_test)\n\n# get RMSE on test\nmean_squared_error(y_test, y_pred, squared=False)\n\n\n\nRMSE on test: 0.53\n\n\n\n\n\ndf_movie_reviews = read_csv(\"data/movie_reviews_processed.csv\")\n\n# create a train and test set\n\nlibrary(rsample)\n\nset.seed(123)\n\nsplit = initial_split(df_movie_reviews, prop = .75)\n\ndf_train = training(split)\ndf_test  = testing(split)\n\nmodel_reviews_extra = lm(\n    rating ~\n        word_count\n        + age\n        + review_year\n        + release_year\n        + length_minutes\n        + children_in_home\n        + total_reviews,\n    data = df_train\n)\n\n# get predictions\ntest_preds = predict(model_reviews_extra, newdata = df_test)\n\n# get RMSE on test\nyardstick::rmse_vec(df_test$rating, test_preds)\n\n\n\nRMSE on test: 0.54\n\n\n\n\n\nAs we’ll see later, there limitations to doing it this simply, but conceptually this is an important idea, and one we will continue to return in our discussion of machine learning.\n\n\n6.4.3.1 Using Metrics for Model Evaluation and Selection\nAs we’ve seen, there are many performance metrics to choose from to assess model performance, and the choice of metric depends on the type of problem. For example, for a problem for numeric targets, we might use RMSE, while for a classification problem, we might use accuracy. As discussed, it turns out that assessing the metric on the data we used to fit the model does not give us the best assessment of that metric. This is because the model will do better on the data it was trained on than on new data it wasn’t trained on, and we can generally always improve that metric in training by making the model more complex. However, in many modeling situations, this complexity comes at the expense of generalization, and the model will not perform as well on new data, something we’ll discuss in more detail shortly. So what we really want to ultimately say about our model will regard performance on the test set with our chosen metric, and not the data we used to fit the model. At that point, we can also compare multiple models to one another given their performance on the test set, and select the one that performs best.\n\n\n6.4.3.2 Understanding Test Error and Generalization\nIn the following discussion, you can think of a standard linear model scenario, e.g. with squared-error loss function, and a data set where we split some of the observations in a random fashion into a training set, for initial model fitting, and a test set, which will be kept separate and independent, and used to measure generalization performance. We note training error as the (average) loss over the training sets we could create in this process, and test error as the (average) prediction error obtained when a model fitted on the training data is used to make predictions on the test data. So, in addition to the previously noted goal of finding the ‘best’ model (model selection), we are interested further in estimating the prediction error with new data (model performance).\n\n6.4.3.2.1 Generalization in the Classical Regime\nSo consider a modeling situation where we have the usual situation of splitting data into training and test sets. We run the model on the training set, but we are more interested in generalization error, or how well it predicts on the test set. We can think of the test error as the average error over many such splits of the data into training and test sets. Given this scenario, let’s look at the following visualization inspired by (hastie_elements_2009?).\n\n\n\nFigure 6.1: Bias Variance Tradeoff\n\n\nPrediction error on the test set, shown in red, is a function of several components, and the terms bias and variance generally refer to two of those components. One thing to note is that even if we had the ‘true’ model given the features specified correctly, there would still be prediction error due to the random data generating process.\nThe main idea here is that as the model complexity increases, we potentially capture more of the data variability. The so-called bias, which is the difference in our average prediction and the true model prediction, decreases, but this only continues for training error (shown in blue), where eventually our model can fit the training data perfectly! For test error though, as the model complexity increases, the bias decreases, but the variance, which is the variability in prediction with changes in data, eventually increases. This is because we get too close to the training data and do poorly when we try to generalize beyond it. This is the known as the bias-variance tradeoff - we can reduce one source of error in the test set at the expense of the other, but not both at the same time indefinitely. In other words, we can reduce bias by increasing model complexity, but this will eventually increase variance in our test predictions. We can reduce variance by reducing model complexity, but this will increase bias. The goal is to find the sweet spot where we have a model that is complex enough to capture the underlying process, but not so complex that it overfits to the training data. Recall that we’re not as interested in training error except to get a sense of how well the model fits the data- ideally it at least does well on training!\n\n\n6.4.3.2.2 Generalization in Deep Learning\nIt turns out, that with lots of data and very complex models, or maybe just in most settings, our classical understanding doesn’t hold up like we’d think. In fact, we can get a model that fits the training data perfectly, and yet ultimately still generalizes well to new data! This phenomenon is encapsulated in the notion of double descent. The idea is that, with overly complex models such as those employed with deep learning, we get to the point of interpolating the data exactly, much like our overfitting plot above Figure 6.3. But as we continue to increase the complexity of the model, we actually start to generalize better again, and visually this displays as a double descent in terms of test error. We see an initial decrease in test error as the model gets better in general. After a while, it begins to rise as seen in the classical regime (Figure 6.1), to where we hit a peak at the point where we have as many parameters as data points. Beyond that however, as we go even more complex with our model, we can possibly see a decrease in test error again. Crazy!\nWe demonstrate this on the classic mtcars dataset, which has only 32 observations! We repeatedly train a model to predict miles per gallon on only 10 of those observations, and assess test error on the rest. The model we use is a form of ridge regression, but implemented such that we can use splines for car weight, horsepower, and displacement3. We fit increasingly complex models, and plot the test error and training error as a function of model complexity. We see that the test error dips, rises, hits a peak, and then starts to decrease again. This is the double descent phenomenon with one of the simplest datasets around. Cool!\n\n\n\n\n\nFigure 6.2: Double Descent on the classic mtcars dataset\n\n\n\n\n\n\n\n6.4.3.3 Generalization Summary\nThe take home point is this: our primary concern is generalization error. We can reduce this error by increasing model complexity, but this may eventually cause test error to increase. However, with enough data and model complexity, we can get to the point where we can fit the training data perfectly, and yet still generalize well to new data. Unless you are doing deep learning, you can maybe assume the classical regime holds, but when doing deep learning, you can worry less about the model’s complexity. In any event, we still want to employ tools regularization to help reduce generalization error.\n\n\n\n6.4.4 Regularization\nAs we’ve seen, a key aspect of the machine learning approach is to generalize to new data. One way to improve generalization is through the use of regularization, which is a general appraoch to penalize complexity in a model, and is typically used to prevent overfitting. Overfitting occurs when a model fits the training data very well, but does not generalize well to new data, and this is often due to the model being too complex, and thus fitting to noise in the training data that isn’t present in other data. Note that the converse can also happen, and is often the case with simpler models, where the model does not fit the training data well, and so does not generalize well to new data either, and this is known as underfitting4.\nWe demonstrate this in the following visualization. The first plot shows results from a model that is notably complex, and in doing so presents a very wiggly result. This is an example of overfitting, and is often seen in models that are too complex for the underlying data. The second plot shows a straight line fit as we’d get from linear regression, which is an example of underfitting. The third plot shows a model that is a better fit to the data, and is an example of a model that is complex enough to capture the nonlinear aspect of the data, but not so complex that it is trying to capitalizing on noise in the data.\n\n\n\n\n\nFigure 6.3: Overfitting and Underfitting\n\n\n\n\nWhen we examine generalization performance5, we see that the overfit model does best on training data, but relatively very poorly on test- nearly a 20% increase in the RMSE value. The underfit model doesn’t change as much in performance because it was poor to begin with on training. Our ‘better’ model wasn’t best on training, but was best on test.\n\n\n\n\n\nTable 6.3:  RMSE for each model on new data \n  \n    \n      \n    \n    \n    \n      Model\n      RMSE\n      % change\n    \n  \n  \n    \n      Train\n    \n    Better\n2.18\n\n    Over\n1.97\n\n    Under\n3.05\n\n    \n      Test\n    \n    Better\n2.19\n0.6\n    Over\n2.34\n19.1\n    Under\n3.24\n6.1\n  \n  \n  \n\n\n\n\n\nWe have already seen one example of regularization in the ridge regression model (ADD CHAPTER LINK), where we add a penalty term to the objective function. This penalty term is a function of the coefficients, and is based on sum of the squared values of the coefficients. It is also known as an L2 penalty, and is a very common type of regularization. Another common approach for linear modelsis the L1 penalty, which is the sum of the absolute values of the coefficients. This is used in the lasso model. There are other types of regularization as well, such as the elastic net, which is a combination of the L1 and L2 penalties. The relative size of the two penalties is controlled by a mixing parameter, and the optimal value of that parameter is determined by cross-validation.\nIt turns out that regularization is used in many modeling scenarios. Here is a quick rundown of some examples.\n\nGAMs also use penalized regression for estimation, where the coefficients used in the basis functions are penalized (typically with L2). This keeps the ‘wiggly’ part of the GAM from getting too wiggly (as in the overfit model above), tending toward a linear effect.\nSimilarly, the variance estimate of a random effect in mixed models, e.g. for the intercept or slope, is inversely related to an L2 penalty on the fixed effects estimates for that group effect. The more penalization applied, the less random effect variance, and the more the random effect is shrunk toward the overall mean6.\n\n\nStill another form of regularization occurs in the form of priors in Bayesian models. For example, the variance on the prior for regression coefficients could be very large, which amounts to a result where there is little influence of the prior on the posterior, or it could be very small, which amounts to a result where the prior has a lot of influence on the posterior, shrinking it toward the prior mean, which is typically zero. In fact, ridge regression is a frequentist form of standard Bayesian linear regression with a normal distribution prior for the coefficients, and the L2 penalty is related to the variance of that prior.\nAs a final example of regularization, dropout is a technique used in deep learning to prevent overfitting. It works by randomly dropping out some of the nodes in intervening/hidden layers in the network during training. This tends to force the network to learn more robust features, allowing for better generalization.\n\nIn short, regularization comes in many forms across the modeling landscape, and is a key aspect of machine learning and traditional statistical modeling alike. In general, we can add the same sort of penalty to any number of models, such as logistic regression, neural networks, recommender systems etc. The primary goal again is to hopefully increase our ability to generalize the selected model to new data.\n\n\n6.4.5 Cross-validation\nSo we’ve talked a lot about generalization to unseen data, so now let’s think about some ways to go about a general process of selecting parameters for a model and assessing performance and generalization.\nAs noted previously, the simplest approach is to split the data into training and test sets, fit the model on the training set, and then assess performance on the test set. This is all well and good, but the test error has uncertainty, and would be slightly different with any training-test split we came up with. We’d also like to get a better assessment when searching the parameter space, because there are oftentimes parameters for which we have no way of guessing the value beforehand. In this case, we need to figure out the best parameters before assessing a final model’s performance. One way to do this is to split the data into multiple test sets, which we now call validation sets, because we still want a test set to be held out that is in no way used during the training process. We fit the model on the training set, and then assess performance on the validation set(s). We then repeat this process for many different splits of the data into training and validation sets, and average the results. This is known as K-fold cross-validation.\nHere is a visualzation of 3-fold cross validation. We use split the data into 2/3 for training, 1/3 for test. We then do this for a total of 3 times, such that the test set is on a different part of the data each time, and all observations are used for both training and test at some point. We then average the results of any metric across the test sets. Note that in each case, there is no overlap of data between the training and test sets.\n\nThe idea is that we are trying to get a better estimate of the test error by averaging over many different test sets. The number of folds, or splits, is denoted by \\(K\\). The value of \\(K\\) can be any number, but typically is 10 or less. The larger the value of \\(K\\), the more accurate the estimate of the test error, but the more computationally expensive it is, and in application, you generally don’t need much to get a good estimate of the mean error. However, with smaller datasets, one can even employ a leave-one-out approach, where \\(K\\) is equal to the number of observations in the data.\nSo cross-validation provides a better measure of the test error. If we are interested when we look at models with different parameters, we can pit their respective average errors against one another, and select the model with the lowest average error, a process known generally as model selection. This works for choosing a model within a potential set of hyperparameter settings (e.g. different penalty parameters for regularized regression), or for choosing a model from a set of different model types (e.g. standard linear model approach vs. boosting).\nNow how might we go about this for modeling purposes? Very easily with modern packages. In the following we demonstrate this with a logistic regression model. We use the LogisticRegressionCV function in sklearn to perform k-fold cross-validation to select the best penalty parameter. We then apply the best model to the test set and calculate accuracy. We do the same thing in R with the mlr3 package. We use the resample function to perform k-fold cross-validation to select the best penalty parameter. In both settings we are interested in the average accuracy score.\n\nPythonR\n\n\n\n# import necessary libraries\nfrom pandas import read_csv\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.metrics import accuracy_score\n\ndf_movies = read_csv(\"data/movie_reviews_processed.csv\")\n\nX = df_movies.filter(regex=\"_sc$\")\ny = df_movies[\"rating_good\"]\n\n# Cs is the (inverse) penalty parameter;\nclf = LogisticRegressionCV(penalty='l2', Cs=[1], cv=5, max_iter=1000)\nclf.fit(X, y)\n\nLogisticRegressionCV(Cs=[1], cv=5, max_iter=1000)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionCVLogisticRegressionCV(Cs=[1], cv=5, max_iter=1000)\n\n# clf.scores_  # show the accuracy score for each fold\n\n# print the average accuracy score\nprint(clf.score(X, y))\n\n0.674\n\n\n\n\n\n# Load necessary libraries\nlibrary(mlr3)\nlibrary(mlr3learners)\n\ndf_movies = read_csv(\n  \"data/movie_reviews_processed.csv\", \n  col_select = matches('_sc|rating_good')\n)\n\ndf_movies = df_movies %&gt;% \n  mutate(rating_good = as.factor(rating_good))\n\n\n# Define task\ntask_lr_ridge = TaskClassif$new(\"movie_reviews\", df_movies, target = \"rating_good\")\n\n# Define learner (alpha = 0 is ridge regression)\nlearner_lr_ridge = lrn(\"classif.cv_glmnet\", alpha = 0, predict_type = \"response\")\n\nlearner_lr_ridge$param_set$values$alpha = 1 # set the penalty parameter to some value\n\n# Define resampling strategy\nresult_lr_ridge = resample(\n    task       = task_lr_ridge,\n    learner    = learner_lr_ridge,\n    resampling = rsmp(\"cv\", folds = 5)\n)\n\nINFO  [15:39:58.307] [mlr3] Applying learner 'classif.cv_glmnet' on task 'movie_reviews' (iter 1/5)\nINFO  [15:39:58.419] [mlr3] Applying learner 'classif.cv_glmnet' on task 'movie_reviews' (iter 2/5)\nINFO  [15:39:58.469] [mlr3] Applying learner 'classif.cv_glmnet' on task 'movie_reviews' (iter 3/5)\nINFO  [15:39:58.515] [mlr3] Applying learner 'classif.cv_glmnet' on task 'movie_reviews' (iter 4/5)\nINFO  [15:39:58.561] [mlr3] Applying learner 'classif.cv_glmnet' on task 'movie_reviews' (iter 5/5)\n\n# result_lr_ridge$score(msr('classif.acc')) # show the accuracy score for each fold\n\n# print the average accuracy score\nresult_lr_ridge$aggregate(msr('classif.acc'))\n\nclassif.acc \n      0.658 \n\n\n\n\n\nIn each case above, we end up with five separate accuracy values, one for each fold. Our final assessment of the model’s accuracy is the average of these five values. This is a better estimate of the model’s accuracy than if we had just used a single test set, and in the end it is based on the entire data.\n\n6.4.5.1 Methods of Cross-validation\nThere are different approaches we can take for cross-validation that we may need for different data scenarios. Here are some of the more common ones.\n\nShuffled: Shuffling prior to splitting can help avoid data ordering having undue effects.\nGrouped/stratified: In cases where we want to account for the grouping of the data, e.g. for data with a hierarchical structure. We may want groups to appear in training or test, but not both (grouped k-fold). Or we may want to ensure group proportions across training and test sets (stratified k-fold).\nTime-based: e.g. for time series data, where we only want to assess error on future values\nCombinations: e.g. grouped and time-based\n\nHere are images from the scikit-learn library documentation depicting some different cross-validation approaches.\n\n\n\n\n\n\nk-fold\n\n\n\n\n\n\n\nGrouped\n\n\n\n\n\n\n\n\n\nStratified\n\n\n\n\n\n\n\nTime series\n\n\n\n\n\nIn general, the form we employ will be based on our data needs.\n\n\n\n\n\n\nIt’s generally always useful to use a stratified approach to cross-validation, especially with classification problems, as it helps ensure a similar balance of the target classes across training and test sets. You can also employ this with numeric target to have similar distribution of the target across training and test sets.\n\n\n\n\n\n\n6.4.6 Tuning\nOne problem with the previous ridge logistic model we just used is that we set the penalty parameter to a fixed value. We can do better by searching over a range of values instead, and picking a ‘best’ one. This is generally known as hyperparameter tuning, or simply tuning. We can do this with cross-validation as well where we will use k-fold cross-validation to assess the error for each value of the penalty parameter values. We then select the value of the penalty parameter that gives the lowest average error. This is a form of as model selection.\nAnother potential point of concern is that we are using the same data to both select the model and assess its performance. This is a form of a more general phenomenon of data leakage, and may result in an overly optimistic assessment of performance. One solution is to do as we’ve discussed before, which is to split the data into three parts: training, validation, and test. We use the training set(s) to fit the model, the validation set(s) to select the model, and then finally use test set to assess the model’s performance. The validation approach is used to select the model, and the test set is used to assess the model’s performance. The following visualations from the scikit-learn documentation illustrates the process.\n\n\n\n\n\n\nTrain-Validation-Test Workflow\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs the performance on test is not without uncertainty, we can actually nest the entire process within a validation approach, where we have an inner loop of k-fold cross-validation and an outer loop to assess the model’s performance on multiple hold out sets. This is known as nested cross-validation. This is a more computationally expensive approach, and generally would require more data, but it would result in a more robust assessment of performance\n\n\n\n\n6.4.6.1 A Tuning Example\nWhile this may start to sound complicated, it doesn’t have to be, as tools are available to make our generalization journey a lot easier. In the following we demonstrate this with a ridge based logistic regression model. The approach we use is called a grid search, where we explictly step through potential values of the penalty parameter. While we only look at one parameter here, for a given modeling approach we could constuct a ‘grid’ of sets of parameter values7 to search over as well.\nWe use the LogisticRegression function in sklearn to perform k-fold cross-validation to select the best penalty parameter. We then apply the best model to the test set and calculate accuracy. We do the same thing in R with the mlr3tuning package. We use the AutoTuner function to perform k-fold cross-validation to select the best penalty parameter. In both settings we are interested in the average accuracy score across the folds, and ultimately the test set8.\n\nPythonR\n\n\n\n# import necessary libraries\nfrom sklearn.model_selection import GridSearchCV, train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\nX = df_movie_reviews.filter(regex=\"_sc$\")\ny = df_movie_reviews[\"rating_good\"]\n\n# split the dataset into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.25, random_state=42\n)\n\n# define the parameter values for GridSearchCV\nparam_grid = {\n    'C': [0.1, 1, 2, 5, 10, 20],\n}\n\n# perform k-fold cross-validation to select the best penalty parameter\n# Note that LogisticRegression by default is ridge regression for scikit-learn\ngrid_search = GridSearchCV(\n    LogisticRegression(), param_grid=param_grid, cv=5, scoring='accuracy'\n)\n\ngrid_search.fit(X_train, y_train)\n\nGridSearchCV(cv=5, estimator=LogisticRegression(),\n             param_grid={'C': [0.1, 1, 2, 5, 10, 20]}, scoring='accuracy')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=5, estimator=LogisticRegression(),\n             param_grid={'C': [0.1, 1, 2, 5, 10, 20]}, scoring='accuracy')estimator: LogisticRegressionLogisticRegression()LogisticRegressionLogisticRegression()\n\n\n\n\nBest C: 2\nAccuracy on train set: 0.661\nAccuracy on test set: 0.692\n\n\n\n\n\n# Load necessary libraries\nlibrary(mlr3verse)\nlibrary(paradox)\n\n# split the dataset into training and test sets\ntrain_idx = sample(1:nrow(df_movie_reviews), nrow(df_movie_reviews) * .75)\n\ndf_movie_reviews_ = df_movie_reviews %&gt;% \n  mutate(rating_good = as.factor(rating_good)) |&gt; \n  select(matches('sc|rating_good'))\n\ndf_train = df_movie_reviews_[train_idx, , drop = FALSE]\ndf_test  = df_movie_reviews_[-train_idx, , drop = FALSE]\n\n\n# Define task\ntask = TaskClassif$new(\"movie_reviews\", df_train, target = \"rating_good\")\n\n# Define learner\nlearner = lrn(\"classif.glmnet\", alpha = 0, predict_type = \"response\")\n\n# Define resampling strategy\nresampling &lt;- rsmp(\"cv\", folds = 5)\n\n# Define measure\nmeasure &lt;- msr(\"classif.acc\")\n\n# Define parameter space\nparam_set = ParamSet$new(\n  list(\n    ParamDbl$new(\"lambda\", lower = 1e-3, upper = 1)\n  )\n)\n\n# Define tuner\ntuner = AutoTuner$new(\n  learner = learner,\n  resampling = resampling,\n  measure = measure,\n  search_space = param_set,\n  tuner = tnr(\"grid_search\", resolution = 10),\n  terminator = trm(\"evals\", n_evals = 10)\n)\n\n# Tune hyperparameters\ntuner$train(task)\n\n\n\nBest lambda: 0.223\nAccuracy on train set: 0.681333333333333\nAccuracy on test set: 0.684\n\n\n\n\n\nSo there you have it. We searched a parameter space, chosen the best one via k-fold cross validation, and have an assessment of generalization error in just a couple lines of code. Neat!\n\n\n6.4.6.2 Search Spaces\nIn the previous example, we used a grid search to search over a range of values for the penalty parameter. This is a very simple approach, but it can be computationally expensive. We can do better by using a more sophisticated approach to search over the parameter space. For example, we can use a random search, where we randomly sample from the parameter space. This is generally faster than a grid search, and can be just as effective. Other methods are available that better explore the space and do so more efficiently.\n\n\n\n\n\n\nGrid search can work to some extent and is a quick an easy way to get started, but generally we want something that can search a true space rather than a limited grid. Typical options are random, bayesian optimization, hyperband, and genetic algorithms. Most of these are available in scikit-learn and mlr3.\n\n\n\n\n\n\n6.4.7 Pipelines\nFor production-level work, or just for reproducibility, it is often useful to create a pipeline for your modeling work. A pipeline is a series of steps that are performed in sequence. For example, we might want to perform the following steps:\n\nImpute missing values\nTransform features\nCreate new features\nSplit the data into training and test sets\nFit the model on the training set\nAssess the model’s performance on the test set\nCompare the model with others\nSave the ‘best’ model\nUse the model for prediction on future data (sometimes called ‘scoring’)\nRedo the whole thing from time to time\n\nWe can create a pipeline that performs all of these steps in sequence. This is useful for a number of reasons. First, doing so makes it far easier to reproduce the results as needed. Second, it is relatively easy to change the steps in the pipeline. For example, we might want to try a different imputation method, or add a new model. Third, it is relatively easy to apply the pipeline. For example, we might want to use the model on new data. We can just apply the pipeline to the new data, and it will perform all of the steps in sequence, including fitting the model. Fourth, having a pipeline facilitates model comparison, as we can ensure that the models are receiving the same data process. Finally, we can save the pipeline for later use- we just save the pipeline as a file, and then load it later when we want to use it again.\nHere is an example of a pipeline in Python. We use the make_pipeline function from the sklearn package. This function takes a series of steps as arguments, and then performs them in sequence. We can then use the pipeline to fit the model, assess its performance, and save it for later use9. With R, mlr3 works in a very similar fashion, which is why we use it for demonstration. We create a pipeline with the po (pipe operator) function, which takes a series of steps as arguments, and then performs them in sequence.\n\nPythonR\n\n\n\n# import necessary libraries\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.metrics import accuracy_score\n\n# create pipeline\npipeline = make_pipeline(\n    SimpleImputer(strategy='mean'),\n    StandardScaler(),\n    LogisticRegressionCV(penalty='l2', Cs=[1], cv=5, max_iter=1000),\n)\n\n# fit the pipeline\npipeline.fit(X_train, y_train)\n\n# assess the pipeline\ny_pred = pipeline.predict(X_test)\naccuracy_score(y_test, y_pred)\n\n# save the pipeline\n# from joblib import dump, load\n# dump(pipeline, 'pipeline.joblib')\n\n\n\n\n# Load necessary libraries\nlibrary(mlr3verse)\n# library(mlr3learners)\n# library(mlr3pipelines)\n\n# Define task\ntask = TaskClassif$new(\"movie_reviews\", df_movie_reviews, target = \"rating_good\")\n\n# Define learner\nlearner = lrn(\"classif.cv_glmnet\", predict_type = \"response\")\n\n# Define pipeline\npipeline = po(\"scale\") %&gt;&gt;%\n  po(\"imputemean\") %&gt;&gt;%\n  po(\"learner\", learner)\n\n# Fit pipeline\npipeline$train(task)\n\n# Assess pipeline\npipeline$predict(task)[[1]]$score(msr(\"classif.acc\"))\n\n# Save pipeline\n# saveRDS(pipeline, \"pipeline.rds\")\n\n\n\n\nDevelopment and deployment of pipelines will depend on your specific use case, and can get notably complicated. Think of your model data being the culmination of features drawn from dozens of wildly different databases, and the model itself being a complex ensemble of models, each with their own hyperparameters. You can imagine the complexity of the pipeline that would be required to handle all of that, but it is possible. In any event, the basic idea is the same, and pipelines are a great way to organize your modeling work."
  },
  {
    "objectID": "machine_learning.html#commentary",
    "href": "machine_learning.html#commentary",
    "title": "6  Machine Learning",
    "section": "6.5 Commentary",
    "text": "6.5 Commentary\nWhen machine learning began to take off, it seemed many in the field of statistics sat on their laurels, and often scoffed at these techniques that didn’t bother to test their assumptions10! ML was, after all, mostly just a rehash of old ideas right? But the machine learning community, which actually comprised both computer scientists and statisticians, was able to make great strides in predictive performance, and the application of machine learning in myriad domains continues to enable us to push the boundaries of what is possible. Statistical analysis wasn’t going to provide ChatGPT or self-driving cars, but it remains vitally important whenever we need to understand the uncertainty of our predictions, or when we need to make inferences about the data world. A more general field of data science became the way people used statistics and machine learning to solve their data challenges. So the two fields are complementary and overlapping, and the best data scientists will be able to draw from both. In the end, use the best tool for the job, worry less about what it’s called or whether it’s the hot thing right now, and importantly, just have fun!\nMOVE TO APPENDIX"
  },
  {
    "objectID": "machine_learning.html#using-r-and-python-in-ml",
    "href": "machine_learning.html#using-r-and-python-in-ml",
    "title": "6  Machine Learning",
    "section": "6.6 Using R and Python in ML",
    "text": "6.6 Using R and Python in ML\n\n6.6.1 Python\nPython is the king of ML. Many other languages can perform ML and maybe even well, but Python is the most popular, and has the most packages, and it’s where tools are typically implemented and developed first. Even if it isn’t your primary language, it should be for any implementation of machine learning.\nPros:\n\npowerful and widely used tools\ntypically very efficient on memory and fast\nmany modeling packages try to use the sklearn API for consistency11\neasy pipeline/reproducibility setup\n\nCons:\n\nEverything beyond getting a prediction can be difficult: e.g. good model summaries and visualizations, interpretability tools, extracting key estimated model features, etc. For example, getting features names as part of the output is a recent development for scikit-learn and other modeling packages.\nData processing beyond applying simple functions to columns can be notably tedious. Pandas, to put it simply, is not tidyverse.\nThe ML ecosystem is fragile, and one package’s update will often break another package’s functionality, meaning your work will typically be frozen in time to whenever you first started model exploration. Many corporate modeling environments are still based on versions of Python that may be years old, and the model pacakges will contain all the bugs from the time of release that was compatible with that version of Python.\nPackage documentation is often quite poor, even for some important model aspects of the model, and there is no consistency from one package to another. Demos may work or not, and you may have to dig into the source code to figure out what’s actually going on. This hopefully will be alleviated in the future with modern AI tools that can write the documentation for you.\nInteractive model development with Jupyter has not been close to the level with alternatives like RMarkdown for years. However, Quarto has already shown great promise, as this book was written with it, so in the end, the R folks may bail out this issue for the Python folks.\n\n\n\n6.6.2 R\nSpeaking as folks who’ve used tools like mlr3, tidyverse, and more on millions of data points for very large and well-known companies, we can say definitively that R is actually great at ML and at production level. The tools are not as fast or memory efficient relative to Python, but they are typically more user friendly, and usually have good to even excellent documentation, as package development has been largely standardized for some time. As far as some drawbacks, some Python packages such as xgboost and lightgbm have concurrent development in R, but even then the R development typically lags with feature implementation. And when it comes to ML with deep learning models, R packages merely wrap the underlying Python packages. In general though, for everything before and after ML, from feature engineering to visualization to reporting, R has much more to offer.\nPros:\n\nvery user friendly and fast data processing\neasy to use objects that contain the things you’d need to use for further processing\npractically every tool you’d use works with data frames\nsaving models does not require any special effort\neasy post-processing of models with many packages designed to work with the output of other modeling packages (e.g. broom, tidybayes, etc.)\ndocumentation is standardized for any CRAN and most non-CRAN packages, and will only improve with AI tools. Unlike Python, examples are expected for documented functions, and the package will fail to build if any example fails, and warn if examples are empty. This is a great way to ensure that examples are present and actually work.\nML tools can be used on tabular data of millions of instances in memory and in production, and on data that is too large to fit in memory using disk-backed data structures.\n\nCons:\n\nrelativley slow\nmemory intensive\npipeline/reproducibility has only recently been of focus\n\ntidymodels is a great but somewhat non-standard way of doing things\nmlr3 is much more sklearn-like- fast and memory efficient, but not as widely used\n\ndevelopers often don’t do enough testing\n\nIn summary, Python is the best tool for ML, but you can use R for pretty much everything else if you want, including ML if it’s not too computationally expensive or you don’t have to worry about that aspect. Quarto makes it easy to use both, including simultaneously, so the great thing is you don’t have to choose!"
  },
  {
    "objectID": "machine_learning.html#refs",
    "href": "machine_learning.html#refs",
    "title": "6  Machine Learning",
    "section": "6.7 Refs",
    "text": "6.7 Refs\nESL for R/Python\nridge as Bayesian WIKILINK: https://en.wikipedia.org/wiki/Ridge_regression#Bayesian_interpretation\ndropout https://d2l.ai/chapter_multilayer-perceptrons/dropout.html\nbv tradeoff\nhttps://hastie.su.domains/Papers/ESLII.pdf\nhttps://machinelearningmastery.com/gentle-introduction-to-the-bias-variance-trade-off-in-machine-learning/\nRF/boosting https://developers.google.com/machine-learning/decision-forests\n“Reconciling modern machine-learning practice and the classical bias–variance trade-off”, 2019, by Belkin, Hsu, Ma, Mandal, https://www.pnas.org/doi/10.1073/pnas.1903070116.\nCV https://scikit-learn.org/stable/auto_examples/model_selection/plot_nested_cross_validation_iris.html\nDL\nAnnotated History of Modern AI and Deep Learning, Juergen Schmidhuber\nInterpretation\nMolnar\nTechniques to Improve Ecological Interpretability of Black-Box Machine Learning Models https://link.springer.com/article/10.1007/s13253-021-00479-7"
  },
  {
    "objectID": "ml_common_models.html#key-ideas",
    "href": "ml_common_models.html#key-ideas",
    "title": "7  Common Models",
    "section": "7.1 Key Ideas",
    "text": "7.1 Key Ideas\nThe take home messages from this section include the following:\n\nAny model can be used with machine learning\nA good and simple baseline is essential for interpreting your performance results\nOne only needs a small set of tools (models) to go very far with machine learning\n\n\n7.1.1 Why this matters\nHaving good choices in your data science toolbox means you don’t have to waste time with nuance and can get down to what matters- performance! Furthermore, using these common tools means you’ll know you’re in good company, and that you’ll be able to find many resources to help you along the way. Additionally, you’ll be able to focus on the data and the problem at hand, rather than the model, which in the end, is just a tool to help you understand the data. If you can get a good understanding of the data with a simple model, then that may be all you need for your situation. If you decide you need a more complex modeling approach, then using these models will still give you a good idea of what you should expect in terms of performance."
  },
  {
    "objectID": "ml_common_models.html#data-setup",
    "href": "ml_common_models.html#data-setup",
    "title": "7  Common Models",
    "section": "7.2 Data setup",
    "text": "7.2 Data setup\nFIXME: add appendix link for dataset!\nFor our demonstration here, we’ll switch things up and use the heart disease dataset. This is a binary classification problem, where we want to predict whether a patient has heart disease, given information such as age, sex, resting heart rate etc. For more details see the appendix. We have done some initial data processing so that you can dive right in. There are two forms of the data, one which is purely numeric, where the binary category features are changed to dummies, and one with labels for those features. The purely numeric version will save any additional data processing for some model/package implementations. In this data, roughly 54% suffered a death, so that is an initial baseline if we’re interested in accuracy- we could get 46% correct by just guessing the majority class.\n\nPythonR\n\n\n\nimport pandas as pd\nimport numpy as np\n\ndf_heart = pd.read_csv('data/heart_disease_processed.csv')\ndf_heart_num = pd.read_csv('data/heart_disease_processed_numeric_sc.csv')\n\n# convert appropriate features to categorical\nfor col in df_heart.select_dtypes(include='object').columns:\n    df_heart[col] = df_heart[col].astype('category')\n\nX = df_heart_num.drop(columns=['heart_disease']).to_numpy()\ny = df_heart_num['heart_disease'].to_numpy()\n\n# some models can't automatically handle missing data\ny_complete = df_heart_num.dropna()['heart_disease'].to_numpy().astype(int)\nX_complete = df_heart_num.dropna().drop(columns='heart_disease').to_numpy()\n\n\n\n\nlibrary(tidyverse)\n\ndf_heart = read_csv(\"data/heart_disease_processed.csv\")\ndf_heart_num = read_csv(\"data/heart_disease_processed_numeric_sc.csv\")\n\ndf_heart = df_heart |&gt; \n    mutate(across(where(is.character), as.factor))\n\n# as a data.frame for mlr3\nX_num_df = df_heart_num %&gt;%\n    as_tibble() |&gt; \n    mutate(heart_disease = factor(heart_disease)) |&gt; \n    janitor::clean_names() # remove some symbols"
  },
  {
    "objectID": "ml_common_models.html#do-better-than-the-baseline",
    "href": "ml_common_models.html#do-better-than-the-baseline",
    "title": "7  Common Models",
    "section": "7.3 Do Better than the Baseline",
    "text": "7.3 Do Better than the Baseline\nCAN WE GET A VISUAL IN HERE SOMEWHERE?\nBefore getting carried away with models, we should try and get something that gives us a good reference point for performance - a baseline model. The baseline model should serve as a way to gauge how much better your model performs over one that is simpler, probably more computationally efficient, and more interpretable. Or maybe it’s one that is sufficiently complex to capture something about the data you are exploring, but not as complex as the models you’re also interested in. Take a classification model for example. We use a logistic regression as abseline, which is as simple as it gets, but is often too simple to be adequately performant for many situations. Even so, we should still be able to beat it with more complex models, or there is little justification for using them.\n\n7.3.1 Why do we do this?\nYou can actually find articles in which deep learning models do not even beat a logistic regression on some datasets, but the fact of which did not stop the authors writing several pages hyping the more complex technique. Probably the most important reason to have a baseline is so that you can avoid wasting time and resources implementing more complex tools, or simply getting excited for no good reason. It is probably rare, but sometimes relationships for the chosen features and target are mostly or nearly linear and have little interaction, and no amount of fancy modeling will make it come about. Furthermore, if our baseline is a complex linear model that actually incorporates nonlinear relationships and interactions (e.g. a GAMM), you’ll often find that the more complex models don’t significantly improve on the baseline by much, if at all. In addition, in time series settings, a moving average or last target value can often be a very good predictor. So in general, you may find that the initial baseline model is good enough for the time being, and you can then move on to other problems to solve, like acquiring data that is functionally predictive. This is especially true if you are working in a business setting where you have limited time and resources.\nA final note. In many (most?) settings, it often isn’t enough to merely beat the baseline model. You should look to do statistically better. For example, if your complex model accuracy is 75% and your baseline is 73%, that’s great, but you should check to see if that difference is statistically significant1, because those metrics are estimates, and they have uncertainty, which means you can get a range for them as well as test whether they are different from one another. If the difference is not notable, then you should probably stick with the baseline model or try something else, because the next time you run the model, the baseline may actually perform better, or at least you can’t be sure that it won’t.\nThat said, in some situations any performance increase is worth it, and even if we can’t be certain a result is statistically better, any sign of improvement is worth pursuing. For example, if you are trying to predict the next word in a sentence, and your baseline is 10% accurate, and your complex model is 11% accurate, that’s a 10% increase in accuracy, which may be a big deal for user experience. You should still work to show that this is a consistent increase and not a fluke."
  },
  {
    "objectID": "ml_common_models.html#penalized-linear-models",
    "href": "ml_common_models.html#penalized-linear-models",
    "title": "7  Common Models",
    "section": "7.4 Penalized Linear Models",
    "text": "7.4 Penalized Linear Models\nTODO: ADD LINK TO ESTIMATION CHAPTER\nSo let’s get on with some models already! Let’s use the classic linear model as our starting point for ML, just because we can. We show explictly how to estimate models like lasso and ridge regression in the estimation chapter. Those work well as a baseline, and so should be in your ML toolbox.\n\n7.4.1 Elastic Net\nAnother common linear model approach is elastic net, which is a combination of lasso and ridge. We will not show how to estimate elastic net by hand here, but all you have to know is that it combines two penalties, the same ones for lasso and one for ridge, along with the standard objective for a numeric or categorical target. The relative size of the two penalties is controlled by a mixing parameter, and the optimal value of that parameter is determined by cross-validation. So for example, you might end up with a 75% lasso penalty and 25% ridge penalty.\nLet’s apply this to the heart disease data. We’ll used the ‘processed version’ which has dummy codes and has dropped the few observations with missing values. We are only doing simple cross-validation here to get a better performance assessment, but you are more than welcome to tune both the penalty parameter and the mixing ratio as we have demonstrated before. We’ll revist hyperparameter tuning towards the end of this chapter.\n\nPythonR\n\n\n\nfrom sklearn.linear_model import LogisticRegression, LogisticRegressionCV\nfrom sklearn.model_selection import cross_validate, KFold, cross_val_score\nfrom sklearn.metrics import accuracy_score\n\n\nmodel_elastic = LogisticRegression(\n    penalty='elasticnet',\n    solver='saga',\n    l1_ratio=0.5,\n    random_state=42,\n    max_iter=10000,\n    verbose=False,\n)\n\n# model_elastic.fit(X_complete, y_complete)\n\n # use cross-validation to estimate performance\ncv_elastic = cross_validate(\n    model_elastic,\n    X_complete,\n    y_complete,\n    cv=5,\n    scoring='accuracy',\n)\n\n\n\nTraining accuracy:  0.829 \nBaseline:  0.541\n\n\n\n\n\n#|\n\nlibrary(mlr3verse)\n\ntsk_elastic = as_task_classif(\n    X_num_df |&gt; drop_na(),\n    target = \"heart_disease\"\n)\n\nlrn_elastic = lrn(\n    \"classif.cv_glmnet\", \n    nfolds = 5, \n    type.measure = \"class\", \n    alpha = 0.5\n)\n\ncv_elastic = resample(\n    task       = tsk_elastic,\n    learner    = lrn_elastic,\n    resampling = rsmp(\"cv\", folds = 5)\n)\n\n\n\nTraining Accuracy: 0.839\nBaseline Prevalence: 0.541\n\n\n\n\n\nSo we’re starting off with what seems to be a good model, and we’re definitely doing better than guessing. Now let’s see if we can do better!\n\n\n7.4.2 Strengths & Weaknesses\nStrengths\n\nIntuitive approach. In the end, it’s still just a standard regression model you’re already familiar with.\nWidely used for many problems. Lasso/Ridge/ElasticNet would be fine to use in any setting you would use linear or logistic regression.\n\nWeaknesses\n\nDoes not automatically seek out interactions and non-linearity, and as such will generally not be as predictive as other techniques.\nVariables have to be scaled or results will largely reflect data types.\nMay have issues with correlated predictors\n\n\n\n7.4.3 Additional Thoughts\nIncorporating regularization as done with penalized regression would be fine as your default linear model method, and is something to strongly consider for even statistical model settings. Furthermore, these approaches will have better prediction on new data than their standard, nonregularized complements. As such they are a nice balance between staying interpretable while enhancing predictive capability. However, in general they are not going to be as strong of a method as others in the ML universe, and possibly not even competitive without a lot of feature engineering. If prediction is all you care about for a particular modeling setting, you’ll likely want to try something else."
  },
  {
    "objectID": "ml_common_models.html#tree-based-methods",
    "href": "ml_common_models.html#tree-based-methods",
    "title": "7  Common Models",
    "section": "7.5 Tree-based methods",
    "text": "7.5 Tree-based methods\nLet’s move beyond standard linear models and get into a notably different type of approach. Tree-based methods are a class of models that are very popular in machine learning, and for good reason, they work very well. To get a sense of how they are derived, consider the following classification example where we want to predict a binary target as ‘Yes’ or ‘No’. We have two numeric features, \\(X_1\\) and \\(X_2\\). At the start we take \\(X_1\\) and make a split at the value of 5. Any observation less than 5 on \\(X_1\\) goes to the right with a prediction of No. Any observation greater than or equal to 5 goes to the left, where we then split based on values of \\(X_2\\), and specifically at 3. Any observation less than 3 goes to the right with a prediction of Yes. Any observation greater than or equal to 3 (and greater than or equal to 5 on \\(X_1\\)) goes to the left with a prediction of No. So in the end, we see relatively lower on \\(X_1\\), or relatively higher on both, results in a prediction of No, and high on \\(X_1\\) and low on \\(X_2\\) results in a prediction of Yes. We can see this visually in the following graph.\n\n\n\n\n\n\n\n\n\n\n\n\nA simple classification tree\n\n\nThis is a simple example, but it illustrates the basic idea of a tree-based model, where the tree reflects the total process, and branches are represented by the splits going down, ultimately ending at leaves where predictions are made. We can also think of the tree as a series of if-then statements, where we start at the top and work our way down until we reach a leaf node, which is a prediction for all observations that qualify for that leaf.\nIf we just use a single tree, this would be the most interpretable model we could probably come up with, and it incorporates nonlinearities (multiple branches on a single feature), interactions (branches across features), and feature selection all in one (some features may not result in useful splits for the objective). However, a single tree is not a very stable model unfortunately, and so does not generalize well. For example, just a slight change in data, or even just starting with a different feature, might produce a very different tree2. The solution is straightforward though - by using the power of a bunch of trees, we can get predictions for each observation from each tree, and then average the predictions, result in a most stable estimate. This is the concept behind both random forests and gradient boosting, which can be seen as different algorithms to produce a bunch of trees, and then average the predictions. They also fall under the heading of ensemble models, which are models that combine the predictions of multiple models, in this case individual trees, to ultimately produce a single prediction for each observation.\nRandom forests and boosting methods are very easy to implement, to a point. However, there are typically a several hyperparameters to consider for tuning. Here are just a few to think about:\n\nNumber of trees\nLearning rate (GB)\nMaximum depth of each tree\nMinimum number of observations in each leaf\nNumber of features to consider at each tree/split\nRegularization parameters (GB)\nOut-of-bag sample size (RF)\n\nThose are the ones that you’ll usually be trying to figure out via cross-validation for boosting or random forests, but there are others. The number of trees and learning rate kind of play off of each other, where having more trees allows for a smaller rate3, which might work better but will take longer to train, and can lead to overfitting if other steps are not taken. The depth of each tree refers to the number of levels down the branches we allow the model to go, as well as how wide we let things get in some implementations. This is important because it controls the complexity of each tree, and thus the complexity of the overall model- less depth helps to avoid overfitting, but too little depth and you won’t be able to capture the nuances of the data. The minimum number of observations in each leaf is also important for the same reason. It’s also generally a good idea to take a random sample of features for each tree (or possibly even each branch), to also help reduce overfitting, but it’s not obvious what proportion to take. The regularization parameters are typically less important in practice, but in general you can use them to reduce overfitting as we would in other modeling circumstances.\n\nHere is an example of gradien boosting with the heart disease data. Although boosting methods are available in scikit-learn for Python, in general we recommend using lightgbm or xgboost packages directly for boosting implementation, which have a sklearn API anyway (as demonstrated). Also, they both provide R and Python implementations of the package, making it easy to not lose your place when switching between languages. We’ll use lightgbm here, but xgboost is also a very good option 4.\n\nPythonR\n\n\n\n# potential models you might use\nfrom sklearn.ensemble import HistGradientBoostingClassifier\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier, DMatrix\n\nfrom sklearn.metrics import accuracy_score\n\nmodel_boost = LGBMClassifier(\n    n_estimators=1000,\n    learning_rate=1e-3,\n    max_depth = 5,\n    verbose = -1\n)\n\ncv_boost = cross_validate(\n    model_boost,\n    df_heart.drop(columns='heart_disease'),\n    df_heart_num['heart_disease'],\n    cv=5,\n    scoring='accuracy',\n)\n\n\n\nTraining accuracy:  0.838 \nBaseline Prevalence:  0.541\n\n\n\n\nNote that as of writing, the mlr3 implementation of lightgbm doesn’t seem to handle factors even though the R package does. So we’ll use the numeric version of the data here.\n\nlibrary(mlr3verse)\n\nset.seed(1234)\n\n# Define task\ntsk_boost = as_task_classif(\n    X_num_df,\n    target = \"heart_disease\"\n)\n\n# Define learner\nlearner_boost = lrn(\n  \"classif.lightgbm\",\n  num_iterations = 1000,\n  max_depth = 5,\n  learning_rate = 1e-3\n)\n\n\n# Cross-validation\ncv_boost = resample(\n    task       = tsk_boost,\n    learner    = learner_boost,\n    resampling = rsmp(\"cv\", folds = 5)\n)\n\n\n\nTraining Accuracy: 0.828\nBaseline Prevalence: 0.541\n\n\n\n\n\nSo here we have a model that is also performing well even without tuning, though not significantly better/worse than our elastic net model. For most situations, this would not be the case, but it also shows why we want a good baseline or simpler model. We’ll revisit hyperparameter tuning using this model later. If you’d like to see an example of how we could implement a form of gradient boosting by hand, see the appendix.\nADD GBLINEAR BY HAND TO APPENDIX\n\n7.5.0.1 Strengths & Weaknesses\nRandom forests and boosting methods, though not new, are still ‘state of the art’ in terms of performance on tabular data like the type we’ve been using for our demos here. As of this writing, you’ll find that it will usually take considerable effort to beat them on tabular data.\nStrengths\n\nA single tree is highly interpretable.\nEasily incorporates features of different types (the scale of numeric features, or using categoricals, doesn’t matter).\nTolerance to irrelevant features.\nSome tolerance to correlated inputs.\nHandling of missing values. Missing values are just another value to potentially split on.\n\nWeaknesses\n\nHonestly few, but like all techniques, it might be relatively less predictive in certain situations. There is no free lunch.\nIt does take more effort to tune relative to linear model methods."
  },
  {
    "objectID": "ml_common_models.html#deep-learning-and-neural-networks",
    "href": "ml_common_models.html#deep-learning-and-neural-networks",
    "title": "7  Common Models",
    "section": "7.6 Deep Learning and Neural Networks",
    "text": "7.6 Deep Learning and Neural Networks\n\nDeep learning has fundametally transformed the world of data science. It has been used to solve problems in image recognition, speech recognition, natural language processing, and more, from assisting with cancer diagnosis to summarizing entire novels. Deep learning has also been used to solve problems with tabular data of the kind we’ve been focusing on. As yet, it is not a panacea for every problem, and is not always the best tool for the job, but it is a tool that should be in your toolbox. Here we’ll provide brief overview of the key concepts behind neural networks, the underlying technology behind deep learning, and then demonstrate how to implement a simple neural network to get things started.\n\n7.6.1 What is a neural network?\nNeural networks have actually been around a while. Computationally, since the 80s, and conceptually even much further back. They were not very popular for a long time, but this was mostly a computing limitation, much the same reason Bayesian methods were slower to develop relative to related alternatives. But now neural networks have recently become the go-to method for many problems. They still can be very computationally expensive, but we at least have the hardware to pull it off now.\nAt its core, a neural network can be seen as complex series of matrix multiplications exactly as we’ve done with a basic linear model. One notable difference is that neural networks actually implement multiple combinations of features (often referred to as hidden nodes or units), and we add in nonlinear transformations between the matrix multiplications, typically referred to as activations. In fact, you can actually think of neural networks as nonlinear extensions of linear models5. The linear part is just like a standard linear model, where we have a set of features, each with a corresponding weight, and we multiply each feature by its weight and sum them up. The activation part is where things start to get more interesting, where we take the output of the linear part and apply a transformation to it, allowing the model to incoporate noninearities. Furthermore, by combining multiple linear parts and activations together, then repeating the whole process for yet another layer of the model but using the hidden nodes as inputs for the subsequent combinations, we can incorporate interactions between features.\nBefore getting carried away, let’s simplify things a bit. We have multiple options for our activation functions, the most common one being what’s called the rectified linear unit or ReLU. But, we could also use the sigmoid function, which is exactly the same as the logistic link function used in logistic regression. In logistic regression, we take the linear combination of features and weights, and then apply the sigmoid function to it. Because of this, we can actually think of logistic regression as a very simple neural network, with a the linear combination as a single hidden node and a sigmoid activation function adding the nonlinear transformation!\nThe following shows a logistic regression as a neural network. The input features are \\(X_1\\), \\(X_2\\), and \\(X_3\\), and the output is the probability of a positive outcome of a binary target. The weights are \\(w_1\\), \\(w_2\\), and \\(w_3\\), and the bias6 is \\(w_0\\). The hidden node is just our linear predictor which we can create via matrix multiplication of the input matrix and weights. The sigmoid function is the activation function, and the output is the probability of the chosen label.\n\n\n\n\n\n\n\n\n\n\n\n\nA logistic regression as a neural network\n\n\n\n7.6.1.1 Trying it out\nTODO: ADD LINK TO DATA CHAPTER re EMBEDDINGS\nFor simplicity we’ll use the same approach and tools as before, but do know this is probably the very bare minimimum approach for a neural network, and generally you’d prefer on alternative. Too begin with, you’d likely want to tune the architecture a bit. Also, as noted in the data discussion, we’d usually want to use embeddings for categorical features as opposed to the one-hot approach used here. For this task, it likely doesn’t matter much, but in many cases you’d prefer the embeddings7.\nThese provide an easy way to get started with neural networks, and provide a sense of what to expect. For our example, we’ll use the processed data with one-hot encoded features. For our architecture, we’ll use three hidden layers with 200 nodes each. As noted, these and other settings are hyperparameters that you’d prefer to tune.\n\nPythonR\n\n\nFor our demonstration we’ll use sklearn’s builtin MLPClassifier. We set the learning rate to 0.001. We’ll also use a validation set of 20% of the data to help with early stopping. We set an adaptive learning rate, which is a way to automatically adjust the learning rate as the model trains. The relu activation function is default. We’ll also use the nesterov momentum approach, which is a way to help the model avoid local minima. We use a warm start, which allows us to train the model in stages, which is useful for early stopping. We’ll also set the validation fraction, which is the proportion of data to use for the validation set. And finally, we’ll use shuffle to randomly select observations for each batch.\n\nfrom sklearn.neural_network import MLPClassifier\n\nmodel_mlp = MLPClassifier(\n    hidden_layer_sizes=(200, 200, 200),  \n    learning_rate='adaptive',\n    learning_rate_init=0.001,\n    shuffle=True,\n    random_state=123,\n    warm_start=True,\n    nesterovs_momentum=True,\n    validation_fraction= .2,\n    verbose=False,\n)\n\n# with the above settings, this will take a few seconds\ncv_mlp = cross_validate(\n  model_mlp, \n  X_complete, \n  y_complete, \n  cv=5\n) \n\n\n\nTraining accuracy:  0.829 \nBaseline Prevalence:  0.541\n\n\n\n\nFor R, we’ll use mlr3torch, which calls pytorch directly under the hood. We’ll use the same architecture as was done with the Python example. It uses the relu activation function as a defualt. We’ll also use adam as the optimizer, which is a popular choice and the default for the sklearn approach also. We’ll also use cross entropy as the loss function, which is the same as the log loss objective function used in logistic regression and other ML classification models. We use a batch size of 16, which is the number of observations to use for each batch of training. We’ll also use epochs of 200, which is the number of times to train on the entire dataset. We’ll also use predict type of prob, which is the type of prediction to make. Finally, we’ll use both logloss and accuracy as the metrics to track. As specified, this took over a minute.\n\nlibrary(mlr3torch)\n\nlearner_mlp = lrn(\n    \"classif.mlp\",\n    # defining network parameters\n    layers = 3,\n    d_hidden = 200,\n    # training parameters\n    batch_size = 16,\n    epochs = 200,\n    # Defining the optimizer, loss, and callbacks\n    optimizer = t_opt(\"adam\", lr = 1e-3),\n    loss = t_loss(\"cross_entropy\"),\n    # # Measures to track\n    measures_train = msrs(c(\"classif.logloss\")),\n    measures_valid = msrs(c(\"classif.logloss\", \"classif.ce\")),\n    # predict type (required by logloss)\n    predict_type = \"prob\",\n    seed = 123\n)\n\ntsk_mlp = as_task_classif(\n    backend = X_num_df |&gt; drop_na(),\n    target = 'heart_disease'\n)\n\n# this will potentially take about a minute\ncv_mlp = resample(\n    task       = tsk_mlp,\n    learner    = learner_mlp,\n    resampling = rsmp(\"cv\", folds = 5),\n)\n\n\n\nTraining Accuracy: 0.826\nBaseline Prevalence: 0.541\n\n\n\n\n\nThis actually did pretty well, which is somewhat suprising given the nature of the data- small number of observations with different data types- a type of situation in which neural networks don’t usually do as well as others. Just goes to show, you never know until you try.\n\n\n7.6.1.2 Strengths & Weaknesses\nStrengths\n\nGood prediction generally.\nIncorporates the predictive power of different combinations of inputs.\nSome tolerance to correlated inputs.\n\nWeaknesses\n\nSusceptible to irrelevant features.\nDoesn’t outperform other methods that are easier to implement on tabular data."
  },
  {
    "objectID": "ml_common_models.html#a-tuned-example",
    "href": "ml_common_models.html#a-tuned-example",
    "title": "7  Common Models",
    "section": "7.7 A Tuned Example",
    "text": "7.7 A Tuned Example\nAs we noted in the chapter on machine learning concepts, there are typically multiple hyperparameters we are concerned with. For the linear model, we might want to tune the penalty parameter and the mixing ratio and/or penalty value. For a boosting method, we might want to tune the number of trees, the learning rate, the maximum depth of each tree, the minimum number of observations in each leaf, and the number of features to consider at each tree/split. And for a neural network, we might want to tune the number of hidden layers, the number of nodes in each layer, the learning rate, the batch size, the number of epochs, and the activation function. And so on.\nHere is an example using the boosted model from before. We’ll use the same data and settings as before, but we’ll tune the number of trees, the learning rate, and the maximum depth of each tree. We’ll use a randomized search approach, which is a way to randomly sample from a set of hyperparameters, rather than searching every possible combination. This is a good approach when you have a lot of hyperparameters to tune, and/or when you have a lot of data.\n\nPythonR\n\n\n\nfrom sklearn.model_selection import RandomizedSearchCV, train_test_split\nfrom sklearn.metrics import accuracy_score\n\nfrom lightgbm import LGBMClassifier\n\n# train-test split\nX_train, X_test, y_train, y_test = train_test_split(\n    df_heart.drop(columns='heart_disease'), df_heart_num['heart_disease'], test_size=0.2, random_state=42\n)\n\nmodel_boost = LGBMClassifier(\n    verbose = -1\n)\n\nparam_grid = {\n    'n_estimators': [500, 1000],\n    'learning_rate': [1e-3, 1e-2, 1e-1],\n    'max_depth': [3, 5, 7, 9],\n    'min_child_samples': [1, 5, 10],\n}\n\n# this will take a few seconds\ncv_boost_tune = RandomizedSearchCV(\n    model_boost, \n    param_grid, \n    n_iter = 10,\n    cv=5, \n    scoring='accuracy', \n    n_jobs=-1\n)\n\ncv_boost_tune.fit(X_train, y_train)\n\n\n\n\nTest Accuracy 0.82 \nBaseline Prevalence:  0.541\n\n\n\n\n\n# train test split\n\nset.seed(123)\n\nlibrary(mlr3verse)\nlibrary(rsample)\n\nsplit = initial_split(X_num_df, prop = .75)\n\ndf_train = training(split)\ndf_test  = testing(split)\n\ntsk_lgbm_tune = as_task_classif(\n    df_train,\n    target = \"heart_disease\"\n)\n\nlrn_lgbm_tune = lrn(\n    \"classif.lightgbm\",\n    num_iteration = to_tune(c(500, 1000)),\n    learning_rate = to_tune(1e-3, 1e-1),\n    max_depth = to_tune(c(3, 5, 7, 9)),\n    min_data_in_leaf = to_tune(c(1, 5, 10))\n)\n\n# set up the validation process\ninstance_lgbm_tune = ti(\n    task = tsk_lgbm_tune,\n    learner = lrn_lgbm_tune,\n    resampling = rsmp(\"cv\", folds = 5),\n    measures = msr(\"classif.acc\"),\n    terminator = trm(\"evals\", n_evals = 10)\n)\n\n# instance\ntuner = tnr(\"random_search\")\n\ntuner$optimize(instance_lgbm_tune)\n\n\n\nTest Accuracy: 0.882\nBaseline Prevalence: 0.541"
  },
  {
    "objectID": "ml_common_models.html#compare-models",
    "href": "ml_common_models.html#compare-models",
    "title": "7  Common Models",
    "section": "7.8 Compare models",
    "text": "7.8 Compare models\nLet’s compare the results of our models head to head. In general, our fastest/best results were the elastic net and boosting models, though not by much for this data set. This is not surprising. Even the simplest neural network takes notable effort to do well, and often longer to train. And while we would definitely use deep learning tools for image classification, natural language processing and other domains, they still struggle to do consistently as well as boosting for the type of data we’ve been using. In general though, this was a good example of having an adequate baseline, and where complexity didn’t really help much, though all our approaches did well.\n\nPythonR\n\n\n\n\n        model  fit_time  val. acc.\n0       boost      0.84       0.84\n1  elasticnet      0.01       0.83\n2        nnet      1.71       0.83\n\n\n\n\n\n\n\n\n\n  \n    \n    \n      model\n      classif.acc\n    \n  \n  \n    elasticnet\n0.84\n    boost\n0.83\n    deep\n0.83\n  \n  \n  \n\n\n\n\n\n\n\nFor Python and R our results on a holdout set were about as good or even better with the tuned model. For the following we first show the cross-validation results of training the models with tuned parameters for each, using 10-fold cross validation so that we can get a better sense of the uncertainty. Then we show the results on the holdout set. Results are in keeping with what we saw with our initial training with no tuning.\n\n\n\n\n\nFigure 7.1: Training results for tuned models.\n\n\n\n\n\n\n\n\n\nTable 7.1:  Holdout results for tuned models. \n  \n    \n    \n      model\n      Test Acc.\n    \n  \n  \n    Elastic Net\n0.88\n    LGBM\n0.84\n    MLP\n0.85\n  \n  \n  \n\n\n\n\n\n\n\n\n\n\n\nSome may wonder why the holdout results are better than the cross-validation results. This can happen, and at least in this case may mostly reflect the small sample size. The holdout set is a random sample of 20% of the complete data, roughly 61 observations. Just a couple different predictions could result in a several percentage points difference in accuracy. Also, the holdout set is a random sample that is not the same data, so this could happen just by chance. In general though, you’d expect the holdout results to be a bit, or even significantly, worse than the cross-validation results, but not always."
  },
  {
    "objectID": "ml_common_models.html#interpretation",
    "href": "ml_common_models.html#interpretation",
    "title": "7  Common Models",
    "section": "7.9 Interpretation",
    "text": "7.9 Interpretation\nJust because we have some models that don’t readily lend themselves to interpretation with simple coefficients, it doesn’t mean we can’t still figure out what’s going on. Let’s use the boosting model as an example.\n\n7.9.1 Feature Importance\nThe default importance metric for a lightgbm model is the number of splits in which a feature is used across trees, and this will depend notably on your settings and the chosen parameters of the best model. You could also use the Shap approach for variable importance as well, where importance is determined by average absolute Shap value. For this data and the model, depending on the settings, you might see that the most important features are age, cholesterol, and max heart rate.\n\nPythonR\n\n\n\n# load the model\nimport joblib\n\ncv_boost_tune = joblib.load('ml/data/tune-boost-py-model.pkl')\n\n# Get feature importances\ncv_boost_tune.feature_importances_\n\n\n\nR shows the porportion of splits in which a feature is used across trees rather than the raw number.\n\n# load the tuned model\nload(\"ml/data/tune-boost-r-results.RData\")\n\n# Get feature importances\nlrn_lgbm_tuned$importance()\n\n\n\n\n\n\n  \n    \n    \n      Feature\n      value\n    \n  \n  \n    chest_pain_type_asymptomatic\n0.22\n    num_major_vessels\n0.13\n    age\n0.13\n    max_heart_rate\n0.13\n  \n  \n  \n\n\n\n\n\n\n\nNow let’s think about a visual display. Here we demonstrate a quick partial dependence plot to see the effects of cholesterol and being male. We can see that males are expected to have a higher probability of heart disease, and that cholesterol has a positive relationship with heart disease, such that a notable rise begins around the mean value for cholesterol. The plot shown is a prettier version of what you’d get with the following code.\n\nPythonR\n\n\n\nfrom sklearn.inspection import PartialDependenceDisplay\n\nPartialDependenceDisplay.from_estimator(\n    cv_boost_tune, \n    df_heart.drop(columns='heart_disease'), \n    features=['cholesterol', 'male'], \n    categorical_features=['male'], \n    percentiles=(0, .9),\n    grid_resolution=75\n)\n\n&lt;sklearn.inspection._plot.partial_dependence.PartialDependenceDisplay object at 0x29e151a50&gt;\n\n\n\n\nFor R we’ll use the IML package.\n\nlibrary(iml)\n\nprediction = Predictor$new(\n    lrn_lgbm_tuned, \n    data = df_train, \n    type = 'prob', \n    class = '1'\n)\n\neffect_dat &lt;- FeatureEffect$new(\n    prediction, \n    feature = c('cholesterol', 'male'), \n    method = \"pdp\", \n)\n\neffect_dat$plot(show.data = TRUE)"
  },
  {
    "objectID": "ml_common_models.html#other-ml-models",
    "href": "ml_common_models.html#other-ml-models",
    "title": "7  Common Models",
    "section": "7.10 Other ML Models",
    "text": "7.10 Other ML Models\nWhen you look up models used in classical machine learning applied to data of the type we’ve been exploring, you’ll potentially see a lot of different kinds. Popular methods from the past include k-nearest neighbors regression, support vector machines, and more. You don’t see these used in practice much though, as these have mostly been made obsolete due to not being as predictive as other options in general (k-nn regression), making strong assumptions about the data distribution (linear discriminant analysis), maybe only works well with ‘pretty’ data situations (SVM), are computationally infeasible for larger datasets (most of them), or just being less interpretable.\nWhile some of these models might still work well in unique situations, when you have tools that can handle a lot of data complexity and predict very well (and typically better) like tree-based methods, there’s not much reason to use the historical alternatives these days. If you’re interested in learning more about them or think one of them is just ‘neat’8, you could potentially use it as a baseline model. Alternatively, you could maybe employ them as part of an ensemble model, where you combine the predictions of multiple models to produce a single prediction. This is a common approach in machine learning, and is often used in Kaggle competitions. We won’t go into detail here, but it’s worth looking into if you’re interested. There are also many other methods that are more specialized, such as those for text, image, and audio data. We will provide an overview of these in another chapter."
  },
  {
    "objectID": "ml_common_models.html#commentary",
    "href": "ml_common_models.html#commentary",
    "title": "7  Common Models",
    "section": "7.11 Commentary",
    "text": "7.11 Commentary\nIn this chapter we’ve provided a few common and successful models you can implement with much success in machine learning. You don’t really need much beyond these for tabular data unless your unique data condition somehow requires it. But a couple things are worth mentinoing before moving on…\n\nFeature engineering will typically pay off more in performance than the model choice.\n\n\nThinking hard about the problem and the data is more important than the model choice.\n\n\nThe best model is simply the one that works best.\n\nYou’ll always get more payoff by coming up with better features to use in the model, as well as just using better data that’s been ‘fixed’ because you’ve done some good exploratory data analysis. Thinking harder about the problem means you won’t waste time going down dead ends, and you typically can find better data to use to solve the problem by thinking more clearly about the question at hand. And finally, it’s good to not be stuck on one model, and be willing to use whatever it takes to get things done efficiently."
  },
  {
    "objectID": "ml_common_models.html#exercise",
    "href": "ml_common_models.html#exercise",
    "title": "7  Common Models",
    "section": "7.12 Exercise",
    "text": "7.12 Exercise\nTune a model of your choice to predict whether a movie is good or bad with the movie review data. Use the processed data with one-hot encoded features if needed. Make sure you use a good baseline model for comparison!"
  },
  {
    "objectID": "ml_common_models.html#refs",
    "href": "ml_common_models.html#refs",
    "title": "7  Common Models",
    "section": "7.13 refs",
    "text": "7.13 refs\nRadfordM.Neal.Priorsforinfinitenetworks(tech.rep.no.crg-tr-94-1).UniversityofToronto, 1994a. https://arxiv.org/abs/1711.00165\nhttps://en.wikipedia.org/wiki/Activation_function\nhttps://stats.stackexchange.com/questions/164876/what-is-the-trade-off-between-batch-size-and-number-of-iterations-to-train-a-neu"
  },
  {
    "objectID": "ml_more.html#key-ideas",
    "href": "ml_more.html#key-ideas",
    "title": "8  More ML",
    "section": "8.1 Key Ideas",
    "text": "8.1 Key Ideas\nSome things to keep in mind when thinking about ML as we wrap up our discussion:\n\nThere is practically no modeling or data domain where ML cannot potentially be applied.\nOther widely used techniques include unsupervised settings, reinforcement learning, computer vision, natural language processing, and more generally, artificial intelligence.\nTabular data has historically been the most common data setting for modeling by far, but this may not always be the case moving forward.\n\n\n8.1.1 Why this matters\nIt’s very important to know just how unlimited the modeling universe is, but also how there is a tie that binds. Even when we get into other data situations and complex models, we can always fall back on the core approaches we’ve already seen and know well at this point, and know that those ideas can potentially be applied in any modeling situation."
  },
  {
    "objectID": "ml_more.html#unsupervised-learning",
    "href": "ml_more.html#unsupervised-learning",
    "title": "8  More ML",
    "section": "8.2 Unsupervised Learning",
    "text": "8.2 Unsupervised Learning\nAll the models considered thus far would fall under the name of supervised learning. That is, we have a target variable that we are trying to predict, and we use the data to train a model to predict the target. However, there are settings in which we do not have a target variable, or we do not have a target variable for all of the data. In these cases, we can still use what’s often referred to as unsupervised learning to learn about the data. Unsupervised learning is a type of machine learning that involves training a model without an explicit target variable in the sense that we’ve seen. But to be clear, a model is still definitely there! Unsupervised learning attempts learn patterns in the data in a general sense, and can be used in a wide range of applications, including clustering, anomaly detection, and dimensionality reduction, though it’s best to think of these as different flavors of a more general approach.\nTraditionally, one of the more common applications of unsupervised learning falls under the heading of dimension reduction, or data compression, such that we reduce features to a smaller latent, or hidden, or unobserved, subset that accounts for most of the (co-)variance of the larger set. Alternatively, we reduce the rows to a small number of hidden, or unobserved, clusters. For example, we start with 100 features and reduce them to 10 features that still account for most of what’s important in the original set, or we classify each observation as belong to 2-3 clusters. Either way, the primary goal is to reduce the dimensionality of the data, not predict an explicit target.\n\n\n\n\n\nFigure 8.1: Two Variables with Three Overlapping Clusters\n\n\n\n\nClassical methods in this domain include principal components analysis (PCA), singular value decomposition (SVD), and factor analysis, which are geared toward reducing column dimensions, as well as cluster methods such as k-means and hierarchical clustering for reducing observations into clusters. Sometimes these methods are often used as preprocessing steps for supervised learning problems, or as a part of exploratory data analysis, but often they are end in themselves. Most of us our familiar with recommender systems, whether via Netflix or Amazon, which suggest products or movies, and we’re all now becoming extremely familiar with text analysis methods via chat bots. While the underlying models are notably more complex these days, they actually just started off as SVD (recommender systems) or a form of factor analysis (text analysis via latent semantic analysis/latent dirichlet allocation). Having a conceptual understanding of the simpler methods can aid in understanding the more complex ones.\n\n\n\n\n\n\nIn general, do not use a dimension reduction technique as a preprocessing step for a supervised learning problem. Instead, use a supervised learning technique that can handle high-dimensional data, has a built-in way to reduce features (e.g. lasso, boosting), or use a dimension reduction technique that is specifically designed for supervised learning (e.g. partial least squares). Creating a reduced set of features without regard to the target will generally be suboptimal for the supervised learning problem.\n\n\n\n\n8.2.1 Connections\n\n8.2.1.1 Clusters are categorical latent features\nIt turns out that whether we are clustering rows or reducing columns we’re actually just using different methods to reduce the features. For methods like PCA and factor analysis, we’re reducing the columns to a smaller set of numeric features. For example, we might take answers to dozens of questions of a personality inventory, and reduce them to five key features that represent general aspects of personality. These new features are on their own scale, often standardized, but still reflect the variability originally seen in the original items to some extent1. However, think about a case where we just reduce the features to a single variable, and that variable was categorical. Now you have cluster analysis! You can discretize anything, e.g. from a nicely continuous feature to a coarse couple of categories, and this goes for latent variables as well as those we actually see in our data. For example, if we do a factor analysis with one latent feature, we could either convert it to a probability of some class with an appropriate transformation, or just say that scores higher than some cutoff are in cluster A and the others are in cluster B. Indeed, there is a whole class of clustering models called mixture models that do just that, i.e. estimate the latent probability of class membership. The point is that the underlying approach can be conceptually similar, and the bigger difference is how we interpret the results.\n\n\n\n\n\n\n\n\n\n\n\n8.2.1.2 PCA as a neural network\nConsider the following neural network, called an autoencoder. The goal of an autoencoder is to learn a representation of the data that is smaller than the original data, but can be used to reconstruct the original data. It’s trained by minimizing the error between the original data and the reconstructed data. The autoencoder is a special case of a neural network used as a component of many larger architectures, but can be used for dimension reduction in and of itself.\n\n\n\nPCA or Autoencoder\n\n\nConsider the following setup for such a situation:\n\nSingle hidden layer\nNumber of hidden nodes = number of inputs\nLinear activation function\n\nAn autoencoder in this case would be equivalent to PCA. In this approach, PCA perfectly reconstructs the original data when considering all components, and so the error would be zero. But that doesn’t give us any dimension reduction, so we often only retain a small number of components that capture the data variance by some arbitrary amount.\nNeural networks however are not bound to linear activation functions, the size of the inputs or even a single layer, and so they provide a much more flexible approach that can compress the data at a certain layer, but still have very good reconstruction error. Typical autoencoders, would have multiple layers with notably more nodes than inputs. It’s not as easily interpretable as typical factor analytic techniques, and we still have to sort out the architecture. However, it’s a good example of how the same underlying approach can be used for different purposes.\n\n\n\n\nFigure 8.2: Conceptual Diagram of an Autoencoder\n\n\nTODO: find a way to make graphviz allow for labels/backgrounds on subgraphs with same rank\n\n\n\n\n\n\n\nAutoencoders are special cases of encoder-decoder models, which are used in many applications, including machine translation, image captioning, and more. Autoencoders have the same inputs and outputs, but in other scenarios, a similar type of architecture might be used to classify or generate text, as with large language models.\n\n\n\n\n\n8.2.1.3 Latent Linear Models\nAnother thing to be aware of is that factor analytic techniques can be thought of latent linear models. Here is a factor analysis as a latent linear model. The ‘targets’ are the observed features, and we predict each one by some linear combination of latent variables.\n\\[\n\\begin{aligned}\nx_1 &= \\beta_{11} h_1 + \\beta_{12} h_2 + \\beta_{13} h_3 + \\beta_{14} h_4 + \\epsilon_1 \\\\\nx_2 &= \\beta_{21} h_1 + \\beta_{22} h_2 + \\beta_{23} h_3 + \\beta_{24} h_4 + \\epsilon_2 \\\\\nx_3 &= \\beta_{31} h_1 + \\beta_{32} h_2 + \\beta_{33} h_3 + \\beta_{34} h_4 + \\epsilon_3 \\\\\n\\end{aligned}\n\\]\nIn this scenario, the \\(h\\) are estimated latent variables, and \\(\\beta\\) are the coefficients, which in some contexts are called loadings. The \\(\\epsilon\\) are the residuals, which are assumed to be independent and normally distributed as with a standard linear model. The \\(\\beta\\) are usually estimated by maximum likelihood, and the model is fit by iterative methods. The latent variables are not observed, but are to be estimated as part of the modeling process, or are derived in post-processing depending on the estimation approach, and typically restricted to have a mean of zero, and possibly standard deviation of 1. The number of latent variables we use is a hyperparameter, and so can be determined by the usual means2. To tie some more common models together:\n\nPCA is a factor analysis with no (residual) variance, and the latent variables are orthogonal (independent).\nProbabilisitic PCA is a factor analysis with constant residual variance.\nFactor analysis is a factor analysis with varying residual variance.\nIndependent component analysis is a factor analysis that does not assume an underlying gaussian data generating process.\nNon-negative matrix factorization and latent dirichlet allocation are factor analyses applied to counts (think poisson and multinomial regression).\n\n\n\n\n8.2.2 Other classical unsupervised learning techniques\nThere are several techniques that are used to visualize high-dimensional data in a low-dimensional spaces, hopefully to identify clusters or aid with interpretability. These include methods like multidimensional scaling, t-SNE, and (H)DBSCAN. These are often used as a part of exploratory data analysis.\nCluster analysis generally speaking has a very long history and you’ll see many different approaches, including hierarchical clustering algorithms (agglomerative, divisive), k-means, and more. Distance matrices are often the first step for these clustering approaches, and there are many ways to calculate distances between observations. Converesely, adjacency matrices, which focus on similarity of observations rather than differences, are often used for graph-based approaches, which may also used for clustering.\nAnomaly/outlier detection is an approach to find data points of interest. This is often done by looking for data points that are far from the rest of the data, or that are not well explained by the model. This is often used for fraud detection, network intrusion detection, and more. Standard clustering or modeling techniques might be used to identify outliers, or specialized techniques might be used.\n\n\n\nFigure 8.3: Network Graph\n\n\nNetwork analysis is a type of unsupervised learning that involves analyzing the relationships between entities. It is a graph-based approach that involves identifying nodes (e.g. people) and edges (e.g. do they know each other) in a network. It is used in a wide range of applications, like identifying communities within a network, or to see how they evolve over time. It is also used to identify relationships between entities, such as people, products, or documents. One might be interested in such things as which nodes that have the most connections, or the general ‘connectedness’ of a network. Network analysis or similar graphical models typically have their own clustering techniques that are based on the edge (connection) weights between individuals, such as modularity, or the number of edges between individuals, such as k-clique.\nIn short, there’s a lot out there that might fall under the umbrella of unsupervised learning, but even when you don’t think you have a target variable, you can still understand or frame these as models similar or even identically to how we have been. One should be less hung up on trying to distinguish modeling approaches with somewhat arbitrary labels, and focus more on what their modeling goal is and how best to achieve it!"
  },
  {
    "objectID": "ml_more.html#reinforcement-learning",
    "href": "ml_more.html#reinforcement-learning",
    "title": "8  More ML",
    "section": "8.3 Reinforcement Learning",
    "text": "8.3 Reinforcement Learning\nPLACEHOLDER IMAGE\n\n\n\n\nReinforcement Learning\n\n\nReinforcement learning (RL) is a type of modeling approach that involves training an ‘agent’ to make decisions in an environment. The agent learns by receiving feedback in the form of rewards or punishments for its actions. The goal of the agent is to maximize its rewards over time by learning which actions lead to positive or negative outcomes.\nIn reinforcement learning, the agent interacts with the environment by taking actions and receiving feedback in the form of rewards or punishments. The agent’s goal is to learn a policy, which is a set of rules that dictate which actions to take in different situations. The agent learns by trial and error, adjusting its policy based on the feedback it receives from the environment. The classic example is a game like chess or simple video games- the agent learns which actions lead to positive outcomes (e.g. winning the game, higher scores) and which actions lead to negative outcomes (e.g. losing the game). The agent then adjusts its policy based on the feedback it receives from the environment. A key aspect of RL is the balance between exploration and exploitation, i.e. trying new things that might lead to greater rewards vs. sticking with what works.\nReinforcement learning has many applications, including robotics, game playing, and autonomous driving, but there is little restriction on where it might be applied. It is often a key part of some deep learning models, where reinforcement is supplied via human feedback or other means. In general, RL is a powerful tool that might be useful where traditional programming approaches may not be as feasible."
  },
  {
    "objectID": "ml_more.html#non-tabular-data-applications",
    "href": "ml_more.html#non-tabular-data-applications",
    "title": "8  More ML",
    "section": "8.4 Non-Tabular Data Applications",
    "text": "8.4 Non-Tabular Data Applications\nWhile our focus in this book is on tabular data due to its ubiquity, there are many other types of data that can be used for modeling, some of which can still potentially be used in that manner, but which often start as a different format or must be considered in a special way. Here we’ll briefly discuss some of the other types of data you’ll potentially come across.\n\n8.4.1 Spatial\nSpatial data such as geographic information can sometimes be quite complex. Oftentimes it is housed in its own data format (e.g. shapefiles), and there are many specialized tools for working with it. Spatial specific features may include continuous types such as latitude and longitude, or the telemetry of a person’s movements recorded from a watch. Others are more discrete such as states within a country. In general, we’d used these features as we would others in the tabular setting, but we often want to take into account the uniqueness of a particular region or the correlation of spatially regions. Historically, most spatial data can be incorporated into models like mixed models or generalized additive models, but in certain applications, such as satellite imagery, deep learning models are more the norm, and the models often transition into image processing techniques.\n\n\n8.4.2 Audio\nAudio data is a type of time series data that is also the focus for many modeling applications. It is often represented as a waveform, which is a plot of the amplitude of the sound wave over time. The goal of modeling the data may include speech recognition, music generation, and more. This sort of data, like spatial data, is typically housed in specific formats, and is often of a very large size. Also like spatial data, the specific type and research question may allow for a tabular format, and the modeling approaches are similar to those for other time series data. As in other domains where the data is of a singular type at its core, deep learning has proved very useful, and can even create songs people actually like, even recently helping the Beatles to release one more song.\n\n\n8.4.3 Image Processing\nDL CONVNETS IMAGE PLACEHOLDER\n\n\n\n\nFigure 8.4: Convolutional Neural Network\n\n\nImage processing involves a range of models and techniques for analyzing images. These include image classification, object detection, image segmentation, tracking, and more. Image classification is the task of assigning a label to an image. Object detection involves identifying the location of objects in an image. Image segmentation is the task of identifying the boundaries of objects in an image. Tracking requires following objects over time.\nIn general, your base data is an image, which is represented as a matrix of pixel values. For example, each row of the matrix could be a grayscale value for a pixel, or it could be a vector of RGB values for a pixel, such that each row is an image, the matrix is a collection of images, while the third dimension is the color channel of red, green and blue. The modeling goal then is to extract features from the image that can be used for the task at hand. For example, you might extract features such as color, texture, and shape. You can then use these features to train a model to classify images or whatever your task may be.\nImage processing is a broad field with many applications. It is used in medical imaging, satellite imagery, self-driving cars, and more. And while it can be really fun to classify objects such as cats and dogs, or generate images from text and vice versa, it can be quite challenging due to the size of the data, issues specific to video/image quality, and the model complexity. Even if your base data is often the same or very similar, the model architecture and training process can vary widely depending on the task at hand.\n\n\n8.4.4 Natural Language Processing\nSOME SORT OF CHAT RELATED IMAGE\n\n\n\nFigure 8.5: Demo for GPT4\n\n\nIt’s safe to say that the hottest area of modeling development in recent times regards natural language processing, as evidenced by the runaway success of models like ChatGPT. Natural language processing (NLP) is a field of study that focuses on understanding human language, and can be seen as a very visible subfield of artificial intelligence. NLP is used in a wide range of applications, including machine translation, speech recognition, text classification, and more. NLP is behind some of the most exciting applications today, with tools that continues to amaze with their capabilities to generate summaries of articles, answering questions, write code, and even pass the bar exam with flying colors!\nEarly efforts in this field were based on statistical models, and then variations on things like PCA, but it took a lot of data pre-processing work to get much from those approaches, and results could still be unsatisfactory. However, more recently, deep learning models have become the standard application, and there is no looking back in that regard. Current state of the art models have been trained on massive amounts of data, even the entire internet, and can be used for a wide range of tasks. But you don’t have to train such a model yourself- now you can simply use a pre-trained model like GPT-4 for many NLP tasks, and in some cases much of the trouble comes with generating the best prompt to produce the desired results. However, the field and the models are evolving extremely rapidly, and things are getting easier all the time3.\n\n\n8.4.5 Pre-trained Models & Transfer Learning\nPre-trained models are models that have been trained on a large amount of data, and can be used for a wide range of tasks. They are widely employed in image and natural language processing. The basic idea is that, if you can use a model that was trained on the entire internet of text, why start from scratch? Image processing models already understand things like edges and colors, so there is little need to reinvent the wheel when you know those features would be useful for your own task. These are viable in tasks where the inputs are similar to the data the model was trained on, as is the case with images and text.\nYou can use a pre-trained model as a starting point for your own model, and then fine-tune it for your specific task, and this is more generally called transfer learning. The gist is that you only need to train part of the model on your specific data, or possibly even not at all. You can just feed your data in and get predictions from the ready-to-go model! This obvioulsy can save a lot of time and resources, assuming you don’t have to pay much to use the model in the first place, and can be especially useful when you don’t have a lot of data to train your model on.\n\n\n8.4.6 Combining Models\nIt’s also important to note that these types of data and their associated models are not mutually exclusive. For example, you might have a video that contains both audio and visual information pertinent to the task. Or you might want to produce images from text inputs. In these cases, you can use a combination of models to extract features from the data, which may just be more features in a tabular format, or be as complex as a multimodal deep learning architecture. Many vision, audio, natural language and othe modeling approaches incorporate transformers. They are based on the idea of attention, which is a mechanism that allows the model to focus on certain parts of the input sequence and less on others. Transformers are used in many state-of-the-art models with different data types such as those that combine text and images. The transformer architecture is a bit complex, but it’s worth knowing about as it’s used in many of the most advanced models today."
  },
  {
    "objectID": "ml_more.html#artificial-intelligence",
    "href": "ml_more.html#artificial-intelligence",
    "title": "8  More ML",
    "section": "8.5 Artificial Intelligence",
    "text": "8.5 Artificial Intelligence\n\n\n\nAI\n\n\nThe prospect of combining models for computer vision, natural language processing, audio processing, and other domains can produce tools that mimic many aspects of what we call intelligence4. Current efforts in AI produce models that can pass law and medical exams, create better explanations of images and text than average human effort, and produce conversation on par with humans. AI even helped to create this book!\nIn many discussions of ML and AI, many put ML as a subset of AI, but this is a bit off the mark from a modeling perspective in our opinion5. For example, model-wise, any aspect of what we’d call modern AI almost exclusively employs deep learning models (although it didn’t in the past, and may supplement a DL model with non-DL models), while the ML approach to training and evaluating models can be used for any underlying model from simple linear models to the most complex deep learning models, whether the application falls under the heading of AI or not. Furthemore, statistical model applications have never seriously attempted what we might call AI. If AI is some ‘autonomous and general tools that attempt to engage the world in a human-like way or better’, it’s not clear why it’d be compared to ML in the first place. That’s kind of like saying the brain is a subset of cognition. The brain does the work, much like ML does the modeling work with data, and gives rise to what we call cognition, but generally we would not compare the brain to cognition.\nMany of the non-AI settings we use modeling for may well be things we can eventually rely on AI to do, but the computational limits, whether the amount of data that would be required for AI models to be better than others, or the ability of AI to be able to deal with situations in which there is only small bits of data, are still hinderances in current applications of AI. However, it’s likely these will eventually be overcome.\nDL models and ML in general can be used for non-AI settings, and the models still employ the perspective of the ML approach when ultimately used for AI - i.e. they are still trained on data, and the model is still a function of the data. Furthermore, ML is not a set of models, but a general approach to modeling, so it’s not clear how it would be defined as a subset of AI. One aspect of AI is that the underlying models are very complex and trained on a great deal more data than typical ML settings, but otherwise, we implement ML to train the models that give rise to AI. So don’t get too hung up on the labels, and focus on the modeling goal and how best to achieve it.\nArtificial general intelligence (AGI) is the holy grail of AI, and like AI itself is not consistently defined. In general, the idea behind AGI is the creation of some autonomous agent that can perform any task that a human can perform, many that humans cannot, and generalize abilities to new problems that have not even been seen yet. It seems we are getting closer to AGI all the time, but it’s not yet clear when it will be achieved, or even what it will look like when it is achieved, especially since no one has an agreed upon definition of what intelligence is in the first place.\nThat said, to be frank, you are very likely reading a history book. Given recent advancements just in the last year or so, it seems unlikely that the data science being performed five years from now will resemble much of how things are done today6. We are already capable of making faster and further advancements to do AI, and it’s likely that the next generation of data scientists will be able to do so even more easily. The future is here, and it is amazing. Buckle up!\n\n8.5.0.1 refs\nRashcka https://nostarch.com/machine-learning-and-ai-beyond-basics\nVaswani, Ashish; Shazeer, Noam; Parmar, Niki; Uszkoreit, Jakob; Jones, Llion; Gomez, Aidan N; Kaiser, Łukasz; Polosukhin, Illia (2017). “Attention is All you Need” (PDF). Advances in Neural Information Processing Systems. Curran Associates, Inc. 30.\nUnsupervised: [https://cloud.google.com/discover/what-is-unsupervised-learning]\nVisuals and deeper understanding: https://colah.github.io/\nembeddings https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/"
  },
  {
    "objectID": "data.html#dealing-with-data",
    "href": "data.html#dealing-with-data",
    "title": "9  Data Issues in Modeling",
    "section": "9.1 Dealing with Data",
    "text": "9.1 Dealing with Data\nIt’s an inescapable fact that models need data to work, even if it’s simulated data. In this chapter we’ll discuss some of the most common data issues, with brief overviews so that you have an idea of why you’d care."
  },
  {
    "objectID": "data.html#key-ideas",
    "href": "data.html#key-ideas",
    "title": "9  Data Issues in Modeling",
    "section": "9.2 Key Ideas",
    "text": "9.2 Key Ideas\n\nData transformations can provide many modeling benefits.\nCategorical data still needs a numeric representation, and this can be done in a variety of ways.\nThe data type for the target may suggest a particular model, but does not necessitate one.\nThe data structure, e.g. temporal or structural, likewise may suggest a particular model.\nLatent variables are everywhere!"
  },
  {
    "objectID": "data.html#sec-data-transfromations",
    "href": "data.html#sec-data-transfromations",
    "title": "9  Data Issues in Modeling",
    "section": "9.3 Standard Feature & Target Transformations",
    "text": "9.3 Standard Feature & Target Transformations\nTransforming variables can provide several benefits in modeling, whether applied to the target, covariates, or both, and should regularly be used for most model situations. Some of these benefits include:\n\nInterpretable intercepts\nMore comparable covariate effects\nFaster estimation\nEasier convergence\nHelp with heteroscedasticity\n\nFor example, merely centering predictor variables, i.e. subtracting the mean, provides a more interpretable intercept that will fall within the actual range of the target variable, telling us what the value of the target variable is when the covariates are at their means (or reference value if categorical). But even if easier interpretation isn’t a major concern, variable transformations can help with convergence and speed up estimation, so can always be of benefit.\n\n9.3.1 Numeric variables\nThe following table shows the interpretation of some very common transformations applied to numeric variables- logging and standardization, (i.e. standardizing to mean zero, standard deviation one).\n\n\n\n\n\nTable 9.1:  Common numeric transformations \n  \n    \n    \n      Target\n      Feature\n      Change in X\n      Change in Y\n      Benefits\n    \n  \n  \n    y\nx\n1 unit\nB unit\nInterpretation\n    log(y)\nx\n1 unit\n100 * (exp(B) -1) \nHeteroscedasticity in y\n    log(y)\nlog(x)\n1% change\nB% change\nInterpretation, deal with feature extremes\n    y\nscale(x)\n1 standard deviation\nB unit\nInterpretation, estimation\n    scale(y)\nscale(x)\n1 standard deviation\nB standard deviation\nInterpretation, estimation\n  \n  \n  \n\n\n\n\n\nFor example, it is very common to use standardized variables, or simply ‘scaling’ them. Some also call this normalizing but this can mean a lot of things, so one should be clear in their communication. If \\(y\\) and \\(x\\) are both standardized, a one unit (i.e. one standard deviation) change in \\(x\\) leads to a \\(\\beta\\) standard deviation change in \\(y\\). Again, if \\(\\beta\\) was .5, a standard deviation change in \\(x\\) leads to a half standard deviation change in \\(y\\). In general, there is nothing to lose by standardizing, so you should employ it often.\nAnother common transformation, particularly in machine learning, is min-max scaling, changing variables to range from some minimum to some maximum, which is almost always zero to one. This can make numeric and categorical indicators more comparable, or at least put the on the same scale for estimation purposes, and so can help with convergence and speed up estimation.\n\nPythonR\n\n\nWhen using sklearn it’s a very verbose process to do a simple transformatin, but this is beneficial when you want to do more complicated things, especially when usind data pipelines.\n\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nimport numpy as np\n\n# Create a sample dataset\nimport numpy as np\n\n# Create a random sample of integers\ndata = np.random.randint(low=0, high=100, size=(5, 3))\n\n# Apply StandardScaler\nscaler = StandardScaler()\nscaled_data = scaler.fit_transform(data)\n\n# Apply MinMaxScaler\nminmax_scaler = MinMaxScaler()\nminmax_scaled_data = minmax_scaler.fit_transform(data)\n\n\n\nR being made for statistics, it’s much easier to do simple transformations, but you can also use tools like recipes to and mlr3 pipeline operations when needed to make sure your preprocessing is applied appropriately.\n\n# Create a sample dataset\ndata &lt;- matrix(sample(1:100, 15), nrow = 5)\n\n# Standardization\nscaled_data &lt;- scale(data)\n\n# Min-Max Scaling\nminmax_scaled_data &lt;- apply(data, 2, function(x) {\n    (x - min(x)) / (max(x) - min(x))\n})\n\n\n\n\nUsing a log transformation for numeric targets and features is straightforward, and comes with several benefits. For example, it can help with heteroscedasticity, i.e. when the variance of the target is not constant across the range of the predictions1 (demonstrated below), keeping predictions positive after transformation, allows for interpretability gains, and more. One issue with logging is that it is not a linear transformation, and so can make certain more complicated transformations in post-modeling more less straightforward. Also if you have a lot of zeros, log plus one transformations are not going to be enough to help you overcome that hurdle. It also won’t help much when the variables in question have few distinct values, like ordinal variables, which we’ll discuss later in Section 9.3.3.\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.5323  1.8196  2.7330  3.1763  4.0295 12.1670 \n\n\n\n\n\nFigure 9.1: Log transformation and heteroscedasticity\n\n\n\n\n\n\n\n\n\n\nIt is rarely necessary or a good idea to transform a numeric feature to a categorical one. This is because you are potentially throwing away useful information by making the feature a less reliable measure of the underlying construct. For example, discretizing age to ‘young’ and ‘old’ does not help your model, and you can always get predictions for what you would consider ‘young’ and ‘old’ after the fact. The primary reasons for doing this are not statistically sound, and can actually hinder interpretation by creating arbitrary groups.\n\n\n\n\n\n9.3.2 Categorical variables\nA raw character string is not an analyzable unit, so character strings and labeled variables like factors must be converted for analysis to be conducted on them. For categorical variables, we can employ what is called effects coding to test for specific types of group differences. Far and away the most common approach is called dummy coding or one-hot encoding2. In these situations we create columns for each category, and the value of the column is 1 if the observation is in that category, and 0 otherwise. Here is a one-hot encoded version of the season feature.\n\n\n\n\n\nTable 9.2:  One-hot encoding \n  \n    \n    \n      seasonFall\n      seasonSpring\n      seasonSummer\n      seasonWinter\n      season\n    \n  \n  \n    1.00\n0.00\n0.00\n0.00\nFall\n    1.00\n0.00\n0.00\n0.00\nFall\n    1.00\n0.00\n0.00\n0.00\nFall\n    1.00\n0.00\n0.00\n0.00\nFall\n    0.00\n0.00\n1.00\n0.00\nSummer\n    0.00\n0.00\n1.00\n0.00\nSummer\n  \n  \n  \n\n\n\n\n\n\n\n\n\n\n\nWhen doing statistical models, when doing one-hot encoding all relevant information is incorporated in k-1 groups, so one category will be dropped. For example, in a linear model, the intercept is the mean of the target for the dropped category, and the coefficients for the other categories are the difference between the mean for the dropped, a.k.a. reference, category and the mean the category being considered. As an example, in the case of the season feature, if the dropped category is winter, the intercept tells us the mean rating for winter, and the coefficients for the other categories are the difference between the value for winter and the mean of the target for the included category. For other modeling approaches, all categories are included, and the model will learn the best way to use them, and may even only consider some or one of them at a particular iteration of estimation.\n\n\n\nAnother important way to encode categorical information is through an embedding. This is a way of representing the categories as a vector of numbers, at which point the embedding feature is used in the model like anything else. The way to do this usually involves a model itself, one that learns the best way to represent the categories as numbers. This is a very common approach in deep learning, and can be done simultaneously with the rest of the model, but can potentially be used in any modeling situation as an initial data processing step.\n\n\n9.3.3 Ordinal Variables\nSo far in our discussion, our categorical data has been assumed to have no order. However you may find yourself with orders labels like “low”, “medium”, and “high”, or “bad” … to “good”, or simply are a few numbers, like ratings from 1 to 5. Ordinal data is categorical data that has a known ordering, but which still has arbitrary labels. Let us repeat that, ordinal data is categorical data.\n\n9.3.3.1 Ordinal Features\nThe simplest way to treat ordinal features is as if they were numeric. If you do this, then you’re just pretending that it’s not categorical, and this is usually fine. Most of the transformations we mentioned probably aren’t going to be as useful, but you can still use them if you want. For example, logging five values of ratings 1-5 isn’t going to do anything for you, but it technically doesn’t hurt anything. But you should know that typical statistics like means and standard deviations don’t really make sense for ordinal data, so the main reason for treating them as numeric is for modeling convenience.\nIf you choose to treat it as categorical, you can ignore the ordering and do the same as you would with categorical data. There are some specific approaches to coding ordinal data for use in linear models, but they are not common, and they generally aren’t going to help the model or interpreting it, so we do not recommend them. You could however use old-school contrast encodings that you would in traditional ANOVA approaches, but again, you’d need a good reason to do so.\nTake home message: treat ordinal features as you would numeric or non-ordered categorical. Either is fine.\n\n\n9.3.3.2 Ordinal Targets\nOrdinal targets are a bit more complicated. If you treat them as numeric, you’re assuming that the difference between 1 and 2 is the same as the difference between 2 and 3, and so on. This is probably not true. If you treat them as categorical, you’re assuming that there is no connection between categories, e.g. that in order to get to category three you have to have gone through category 2. So what should you do?\nThere are a number of approaches to modeling ordinal targets, but the most common is the proportional odds model. This model can be seen as a generalization of the logistic regression model, and is very similar to it, and actually identical if you only had two categories. But others are also possible, and your results could return something that gives coefficients for the model for the 1-2 category change, the 2-3 category change, and so on.\nOrdinality of a categorical outcome is largely ignored in machine learning approaches. The outcome is either treated as numeric or multi-category classification. This is not necessarily a bad thing, especially if prediction is the primary goal.\n\n\n\n\n\n\nSome are a little too eager to jump to simultaneously modeling multiple target variables, e.g. in structural equation modeling or mutivariate regression. It’s not wrong to do so, but given the difficulty our brains have with interpreting results for a single target, you might think twice about doing so. However, for scenarios focused much more on prediction performance that involve method like deep learning, it makes more sense, and is actually required."
  },
  {
    "objectID": "data.html#sec-missing-data",
    "href": "data.html#sec-missing-data",
    "title": "9  Data Issues in Modeling",
    "section": "9.4 Missing Data",
    "text": "9.4 Missing Data\nMissing data is a common challenge in data science, and there are a number of ways to deal with it, usually by substituting, or imputing some value for the missing one. The most common approaches are:\n\nComplete case analysis: Only use observations that have no missing data.\nSingle value imputation: Replace missing values with the mean, median, mode or some other value of the feature.\nModel-based imputation: Use a model based on complete cases to predict the missing values, and use those predictions are the imputed values.\nMultiple imputation: Create multiple imputed datasets based on the predictive distribution of the model used in model-based imputation. Estimates of coefficients and variances are averaged in some fashion over the imputations.\nBayesian imputation: Treat the missing values as parameters to be estimated.\n\nThe first approach to drop missing data is the simplest, but can lead to a lot of lost data, and can lead to biased statistical results if the data is not missing completely at random. There are special cases of some models that by their nature can ignore the missingness under an assumption of missing at random, but even those models would likely benefit from some sort of imputation. If you don’t have much missing data, this would be fine. How much is too much? Unfortunately that depends on the context, but if you have more than 10% missing, you should probably be looking at alternatives.\nThe second approach where we just plug in a single value like a mean is also simple, but will probably rarely help your model. Consider a numeric feature that is 50% missing, and for which you replace the missing with the mean. How good do you think that feature will be when at least half the values are identical? Whatever variance it normally would have and share with the target is probably reduced, and possibly dramatically. Furthermore, you’ve also attenuated correlations it has with the other features, which could potentially further hamper interpretation or cause other issues depending on the type of model you’re implementing. Single value imputation makes perfect sense if you know that the missingness means a specific value, like a count feature where missing means a count of zero. If you don’t have much missing data, it’s unlikely this would have any real benefit over complete case analysis, except if it allows you to use all the other features that would otherwise be dropped. But then, why not just drop this feature and keep the others?\nModel-based imputation is more complicated, but can be very effective. In essence, you run a model for complete cases in which the target is now the feature with missing values, and the covariates are all the other features and target. You then use that model to predict the missing values, and use those predictions as the imputed values. After these predictions are made, you move on to the next feature and do the same. There are no restrictions on which model you use for which feature. If the other features in the imputation model also have missing data, you can use something like mean imputation to get more complete data if necessary as a first step, and then when their turn comes, impute those values.\nAlthough the implication is that you would have one model per feature and then be done, you can do this iteratively for several rounds, such that the initial imputed values are then used in subsequent models to reimpute the missing values. You can do this as many times as you want, but the returns will diminish.\nMultiple imputation (MI) is the most complicated, but can be the most effective under some situations, depending on what you’re willing to sacrifice for having better uncertainty estimates vs. a deeper dive into the model. The idea is that you create multiple imputed datasets, each of which is based on the predictive distribution of the model used in model-based imputation. Say we use a linear regression assuming a normal distribution to impute feature A. We would then draw from the predictive distribution of that model to create a dataset with imputed values for feature A, then do it a gain, say a total of 10 times.\nYou now have 10 imputed data sets. You then run your actual model of interest on each of these datasets, and your final model results are a kind of average of the parameters of interest (or exactly an average, say for regression coefficients). This main thing this approach provides is that it acknowledges that your single imputation methods have uncertainty in those model predictions being used as imputed values, and that uncertainty is incorporated into the final model results.\nMI can in theory handle the any source of missingness and as such is a very powerful approach. But it has several drawbacks. One is that you need statistical or generative model and distribution for all models used, and that distribution is something you have to assume is appropriate. Your final model presumably is also a probabilistic model with coefficients and variances you are trying to estimate and understand. MI isn’t really going to help an XGBoost or deep learning model for example, or at least offer little if anything over single value imputation. If you have very large data and a complicated model, you could be waiting a long time, and as modeling is an iterative process itself, this can be rather tedious to work through. Finally, few data or post-model processing tools that you commonly use will work with MI results, especially visualization ones, and so you will have to hope that whatever package you use for MI has what you need. As an example, you’d have to figure out how you’re going to impute interaction terms if you have them, practically nothing will work with cross-validation approaches,\nPractically speaking, MI takes a lot of effort to often come to the same conclusions you would have with a single imputation approach, or possibly fewer conclusions for anything beyond GLM coefficients and their standard errors. But if you want your uncertainty estimate for those models to be better, MI can be an option.\nOne final option is to run a Bayesian model where the missing values are treated as parameters to be estimated. MI basically is a poor man’s Bayesian imputation approach. A package like brms can do this, and it can be very effective, but it is also very computationally intensive, and can be very slow. At least it would be more fun than standard MI!"
  },
  {
    "objectID": "data.html#sec-class-imbalance",
    "href": "data.html#sec-class-imbalance",
    "title": "9  Data Issues in Modeling",
    "section": "9.5 Class Imbalance",
    "text": "9.5 Class Imbalance\n\n\n\n\n\nFigure 9.2: Class Imbalance\n\n\n\n\nClass imbalance refers to the situation where the target variable has a large difference in the number of observations in each class. For example, if you have a binary target, and 90% of the observations are in one class, and 10% in the other, you would have class imbalance. You’ll almost never see a 50/50 split in the real world, but the issue is that as we move further away from that point, we can start to see problems in model estimation, prediction, and interpretation. As a starting point, if we just predict the majority class in a binary classification problem, we’ll be right 90% of the time in terms of accuracy. So right off the bat one of our favorite metrics isn’t going to help us assess model performance as much as we’d like.\nIn classification problems, class imbalance is the rule, not the exception. This is because nature just doesn’t sort itself into nice and even bins. For example, the majority of people of a random sample do not have cancer, the vast majority of people have not had a heart attack in the past year, most people do not default on their loans, and so on.\nThere are a number of ways to help deal with class imbalance, and the best approach depends on the context. Some of the most common approaches are:\n\nUse different metrics: Use metrics that are less affected by class imbalance, such as area under a receiver operating characteristic curve (AUC), or those that balance the\nOversampling/Undersampling: Randomly sample from the minority (majority) class to increase (decrease) the number of observations in that class.\nWeighted objectives: Weight the loss function to give more weight to the minority class. Although commonly employed, and a simple thing to use with models like lightgbm and xgboost, it often fails to help, and can cause other issues.\nThresholding: Change the threshold for classification to be more sensitive to the minority class. Nothing says you have to use 0.5 as the threshold for classification, and you can change it to be more sensitive to the minority class. This is a very simple approach, and may be all you need.\n\nThese are not necessarily mutually exclusive. For example, it’s probably a good idea to switch to a metric besides accuracy as you employ other techniques.\n\n9.5.1 Calibration issues in classification\nProbability calibration is often an issue in classification problems, and is a bit more complicated than just class imbalance but is often discussed in the same setting. Having calibrated probabilities refers to the situation where the predicted probabilities of the target match up well to the actual probabilities. For example, if you predict that 10% of people will default on their loans, and 10% of people actually do default on their loans, one would say your model is well calibrated. Conversely, if you predict that 10% of people will default on their loans, but 20% of people actually do default on their loans, your model is not so well-calibrated.\nOne of the most common approaches to assessing calibration is to use a calibration curve, which is a plot of the predicted probabilities vs. the observed proportions. In the following, one model seems to align well with the observed proportions based on the chosen bins. The other model is not so well calibrated, and is overshooting with its predictions.\n\n\n\n\n\nCalibration Plot\n\n\n\n\nWhile the issue is an important one, it’s good to keep the issue of calibration and imbalance separate. As miscalibration implies bias, bias can happen irrespective of the class proportions and can be due to a variety of factors related to the model, target, or features, and miscalibration is not inherent to a particular model.\nFurthermore, the assessment of calibration with this approach has a few issues. For one, the observed ‘probabilities’ are proportions based on arbitrarily chosen bins and observed values that are measured with some measurement error as well as having natural variance with will partly reflect sample size3. These plots are often presented such that observed proportions are labeled as the “true” probabilities. However, you do not have the true probabilities outside of simulation settings, just the observed class labels, so whether your model’s predicted probabilities match observed proportions is a bit of a different question. The predictions obviously have uncertainty as well, and this will depend on modeling approach, sample size, etc. And finally, the number of bins chosen can also affect the appearance of the plot in a notable way.\nAll this is to say that each point in a calibration plot has some error bar around it, and the differences between models and the ‘best case scenario’ would need additional steps to suss out. Some methods are available to calibrate probabilities, but they are not commonly implemented in practice, and often involve a model-based technique, with all of its own assumptions and limitations. It’s also not exactly clear that forcing your probabilities to be on the line is helping solve the actual modeling goal in any way4. But if you are interested, you can read more here."
  },
  {
    "objectID": "data.html#sec-data-censoring",
    "href": "data.html#sec-data-censoring",
    "title": "9  Data Issues in Modeling",
    "section": "9.6 Censoring and Truncation",
    "text": "9.6 Censoring and Truncation\n\n\n\n\n\nFigure 9.3: Potential Censoring\n\n\n\n\nSometimes, we just don’t see all the data there is to see. Censoring is a situation where the target variable is not fully observed. This is common in survival analysis5, where the target is the time to an event, but the event has not yet occurred for some observations. This is called right censoring, and is the most common type of censoring, and depicted in the above plot, where several individuals are only observed to a certain age and were still alive at that time. There is also left censoring, where the censoring happens from the other direction data before a certain point is unknown. Finally, there is interval censoring, where the event of interest occurs within some interval, but the exact value is unknown.\nSurvival analysis is a common modeling technique in this situation, but you may also be able to keep things even more simple via something like tobit regression. In this approach, you assume that the target is fully observed, but that the values are censored, and you model the probability of censoring. This is a very common approach in econometrics, and can keep you in a traditional linear model context.\n\n\n\n\n\nFigure 9.4: Truncation\n\n\n\n\nTruncation is a situation where the target variable is only observed if it is above or below some value. One of the issues is that default distributional methods, e.g., via maximum likelihood, assume a distribution that is not necessarily bounded. In our plot above, we restrict our data to 70 and below, but typical modeling methods with default distributions would not respect that.\nYou could truncate predictions after the fact, but this is a bit of a hack, and often results in lumpiness in the predictions at the boundary that isn’t realistic in most situations. Alternatively, Bayesian approaches allow you to model the target as a distribution with truncated distributions, and so you can model the probability of the target being above or below some value. This is a very flexible approach. There are also models such as hurdle models that might prove useful where the truncation is theoretically motivated, e.g. a zero-inflated Poisson model for count data where the zero counts are due to a separate process than the non-zero counts.\n\n\n\n\n\n\nOne way to distinguish censored and truncated data is that censored data is usually due to some external process such that the target is not observed but could be possible (capping reported income at $1 million), whereas truncated data is due to some internal process that prevents the target from being observed, and is often derived from sample selection (we only want to model non-millionaires). We would not want observations past the censored point to be unlikely, but we would want observations past the truncated point to be impossible. Trickier still is that for bounded or truncated distributions that might be applied to the truncated scenario, such as folded vs. truncated distributions, they would not result in the same probability distributions even if they can be applied to the same situation.\n[]\nImage from StackExchange"
  },
  {
    "objectID": "data.html#sec-data-time",
    "href": "data.html#sec-data-time",
    "title": "9  Data Issues in Modeling",
    "section": "9.7 Time Series",
    "text": "9.7 Time Series\nTODO: ADD CHAPTER LINKS HERE\nTime series data is any data that incorporates values over time. This could be something like the a state’s population over years, or the max temperature of an area over days, etc. Time series data is very common, and there are a number of approaches to modeling it, and we usually take special approaches to account for the fact that observations are not independent of one another. The most common approach is to use a time series regression approach, where the target is some value that varies time, and the covariates are other features that can be time-varying or not. There are old school approaches like ARIMA that can still serve as decent [baseline models]LINK THE TO ML CHAPTER, and newer approaches like more sophisticated Bayesian ones that can get quite complex.\nLongitudinal data6 is a special case of time series data, where the target is a function of time, but it is typically grouped in some fashion. An example is would be school performance for students over several semesters, where values are clustered within students over time. In this case, you can use a time series regression approach, but you can also use a mixed model (LINK TO THE EXTENSIONS CHAPTER) approach, where you model the target as a function of time, but also include a random effect for the grouping variable, in this case, students. This is a very common approach in many areas, and can be very effective. It can also be used for time series data that is not longitudinal, where the random effects are based on autoregressive covariance matrices.\n\n\n\n\n\n\nThe primary distinguishing feature for referring data as ‘time-series’ and ‘longitudinal’ is the number of time points, where the latter typically has relatively few. This arbitrary though.\n\n\n\n\n9.7.1 Time-based Features\nWhen it comes to time-series features, we can apply time-based transformations. One PCA like approach is the fourier transform, which can be used to decompose a time series into its component frequencies. This can be useful for identifying periodicity in the data, and can be used as a feature in a model. In marketing contexts, some perform adstocking on features, which is a way of modeling the lagged of the effect of features over time, such that they may have their most important impact immediately, but still can impact the present target from past values. This avoids directly putting lags for each time point as additional features in the model, though that is an option. In that case you have a feature at present time t, the same feature representing the previous time point t-1, the feature at t-2, etc.\nAnother thing to note about feature transformations, if you have year as a feature, you can use it as a numeric feature or as a categorical feature. In the former case, if you are only considering a linear effect, you should make the zero meaningful, typically by starting the year values at zero. This makes the intercept in linear models reference the first year rather than year 0, which can actually make it harder to estimate along with the other coefficients in a model. The same goes if you are using months or days as a numeric feature, since there is no ‘zero’ month. It doesn’t really matter which year/month/day is zero, just that zero refers to one of the actual time points observed.\nDates and/or times can be it bit trickier. Often you can just split dates out into year, month, day, etc., and proceed as discussed. In other cases you’d want to track the day to assess potential seasonal effects, where it makes sense to use a cyclic approach (e.g. cyclic spline or sine/cosine transformation) to get at yearly or within-day seasonal effects.\n\n\n\n\n\n\nWeeks are not universal. Some start on Sunday, others Monday. Some data contexts only consider weekdays. Some systems may have 52 or 53 weeks in a year, and dates may not be in the same week from one year to the next, etc. So use caution when considering weeks as a feature."
  },
  {
    "objectID": "data.html#sec-data-spatial",
    "href": "data.html#sec-data-spatial",
    "title": "9  Data Issues in Modeling",
    "section": "9.8 Spatial Data",
    "text": "9.8 Spatial Data\nTODO: ADD CHAPTER LINKS HERE\nWe also visit spatial data in a discussion on non-tabular data, but here we want to talk about it from a modeling perspective, especially within the tabular domain. Say you have a target that is a function of location, such as the proportion of people voting a certain way in a county, or the number of crimes in a city. You can use a spatial regression approach, where the target is a function of location among other features that may or may not be spatially oriented. Two approaches already discussed may be applied in the case of having continous spatial features, such as latitude and longitude, or discrete features like county. One model approach to the continuous case include a GAM, where we use a smooth interaction of latitude and longitude. For the discrete setting, we can use a mixed model approach, where we include a random effect for county.\nThere are other traditional approaches to spatial regression, especially in the continuous spatial domain, such as using a spatial lag approach, where we incorporate information about the neighborhood of a observation’s location into the model. Such models fall under names such as CAR (conditional autoregressive), SAR (spatial autoregressive), BYM, kriging, and so on. These models can be very effective, but are in general different forms of random effects models, and can be seen as special cases of gaussian processes. We feel it’s probably unnecessary to get into details of the traditional spatial models unless you know other standard techniques like GAMs or mixed models won’t work, or more general approach like gaussian process regression isn’t more feasible."
  },
  {
    "objectID": "data.html#sec-data-latent",
    "href": "data.html#sec-data-latent",
    "title": "9  Data Issues in Modeling",
    "section": "9.9 Latent Variables",
    "text": "9.9 Latent Variables\nTODO: ADD CHAPTER LINKS HERE\nLatent variables are a fundamental aspect of modeling, and simply put, are variables that are not directly observed, but are inferred from other variables. Here are some examples of what might be called latent variables:\n\nThe linear combination of features in a linear regression model is a latent variable, but usually we only think of it as such before the link transformation in GLMs.\nThe error term in any model is a latent variable representing all the unknown/unobserved/unmodeled factors that influence the target.\nThe principal components in PCA.\nThe factor scores in a factor analysis model or structural equation.\nThe true target underlying the censored values.\nThe clusters in a cluster in cluster analysis.\nThe random effects in a mixed model.\nThe hidden layers in a deep learning model.\n\nIt’s easy to see from such a list that latent variables are very common in modeling, so it’s good to get comfortable with the concept. Whether they’re appropriate to your specific situation will depend on a variety of factors, but they can be very useful in many settings, if not a required part of the modeling approach.\nTODO: Measurement error?"
  },
  {
    "objectID": "data.html#commentary",
    "href": "data.html#commentary",
    "title": "9  Data Issues in Modeling",
    "section": "9.10 Commentary",
    "text": "9.10 Commentary\nThere’s a lot going on with data before you ever get to modeling, and which will affect every aspect of your modeling approach. This chapter outlines common data types, issues, and associated modeling aspects, but in the end, you’ll always have to make decisions based on your specific situation, and they will often not be easy. These are only some of the things to consider, so be ready for surprises, and, be ready to learn from them!"
  },
  {
    "objectID": "data.html#refs",
    "href": "data.html#refs",
    "title": "9  Data Issues in Modeling",
    "section": "9.11 refs",
    "text": "9.11 refs\nTransformations\n\nmin-max vs. standardization https://sebastianraschka.com/Articles/2014_about_feature_scaling.html\n\nclass imbalance - https://towardsdatascience.com/handling-imbalanced-datasets-in-machine-learning-7a0e84220f28 - Monroe & Clark - https://developers.google.com/machine-learning/data-prep/construct/sampling-splitting/imbalanced-data - https://machinelearningmastery.com/what-is-imbalanced-classification/\ncalibration\nhttps://machinelearningmastery.com/probability-calibration-for-imbalanced-classification/ https://stats.stackexchange.com/questions/452483/why-some-algorithms-produce-calibrated-probabilities\nNiculescu-Mizil, A., & Caruana, R. (2005). Predicting good probabilities with supervised learning. In Proceedings of the 22nd international conference on Machine learning (pp. 625-632)."
  },
  {
    "objectID": "dataset_descriptions.html#sec-dd-hear-failure",
    "href": "dataset_descriptions.html#sec-dd-hear-failure",
    "title": "Appendix B — Dataset Descriptions",
    "section": "B.1 Heart Failure",
    "text": "B.1 Heart Failure\nDataset from Davide Chicco, Giuseppe Jurman: â€œMachine learning can predict survival of patients with heart failure from serum creatinine and ejection fraction alone. BMC Medical Informatics and Decision Making 20, 16 (2020)\nhttps://www.kaggle.com/datasets/andrewmvd/heart-failure-clinical-data\nCardiovascular diseases (CVDs) are the number 1 cause of death globally, taking an estimated 17.9 million lives each year, which accounts for 31% of all deaths worlwide. Heart failure is a common event caused by CVDs and this dataset contains 12 features that can be used to predict mortality by heart failure.\nMost cardiovascular diseases can be prevented by addressing behavioural risk factors such as tobacco use, unhealthy diet and obesity, physical inactivity and harmful use of alcohol using population-wide strategies.\nPeople with cardiovascular disease or who are at high cardiovascular risk (due to the presence of one or more risk factors such as hypertension, diabetes, hyperlipidaemia or already established disease) need early detection and management wherein a machine learning model can be of great help.\nage: Age\nanaemia: Decrease of red blood cells or hemoglobin (boolean)\ncreatinine_phosphokinase: Level of the CPK enzyme in the blood (mcg/L)\ndiabetes: If the patient has diabetes (boolean)\nejection_fraction: Percentage of blood leaving the heart at each contraction (percentage)\nhigh_blood_pressure: If the patient has hypertension (boolean)\nplatelets: Platelets in the blood (kiloplatelets/mL)\nserum_creatinine: Level of serum creatinine in the blood (mg/dL)\nserum_sodium: Level of serum sodium in the blood (mEq/L)\nsex: Woman or man (binary)\nsmoking: If the patient smokes or not (boolean)\ntime: Follow-up period (days)\nDEATH_EVENT: If the patient deceased during the follow-up period (boolean)\nFor booleans: Sex - Gender of patient Male = 1, Female =0 (renamed for our data) Age - Age of patient Diabetes - 0 = No, 1 = Yes Anaemia - 0 = No, 1 = Yes High_blood_pressure - 0 = No, 1 = Yes Smoking - 0 = No, 1 = Yes DEATH_EVENT - 0 = No, 1 = Yes"
  },
  {
    "objectID": "dataset_descriptions.html#sec-dd-heart-disease-uci",
    "href": "dataset_descriptions.html#sec-dd-heart-disease-uci",
    "title": "Appendix B — Dataset Descriptions",
    "section": "B.2 Heart Disease UCI",
    "text": "B.2 Heart Disease UCI\nDataset from Kaggle: https://www.kaggle.com/ronitf/heart-disease-uci\nThis database contains 76 attributes, but all published experiments refer to using a subset of 14 of them. In particular, the Cleveland database is the only one that has been used by ML researchers to this date.\nAttribute Information:\n    name     role         type demographic                                        description  units missing_values\n0 age Feature Integer Age None years no 1 sex Feature Categorical Sex None None no 2 cp Feature Categorical None None None no 3 trestbps Feature Integer None resting blood pressure (on admission to the ho… mm Hg no 4 chol Feature Integer None serum cholestoral mg/dl no 5 fbs Feature Categorical None fasting blood sugar &gt; 120 mg/dl None no 6 restecg Feature Categorical None None None no 7 thalach Feature Integer None maximum heart rate achieved None no 8 exang Feature Categorical None exercise induced angina None no 9 oldpeak Feature Integer None ST depression induced by exercise relative to … None no 10 slope Feature Categorical None None None no 11 ca Feature Integer None number of major vessels (0-3) colored by flour… None yes 12 thal Feature Categorical None None None yes 13 num Target Integer None diagnosis of heart disease None no\nFeatures and target were renamed for our data."
  },
  {
    "objectID": "matrix_operations.html#addition",
    "href": "matrix_operations.html#addition",
    "title": "Appendix C — Matrix Operations",
    "section": "C.1 Addition",
    "text": "C.1 Addition\nMatrix addition, along with subtraction, is the easiest concept when dealing with matrices. While it is easy to grasp, you will not find it featured as prominently as matrix multiplication.\nThere is one rule for matrix addition: the matrices need to have the same dimensions.\nLet’s check out these two matrices:\n\\[\n\\stackrel{\\mbox{Matrix A}}{\n\\begin{bmatrix}\n1_{11} & 2_{12} & 3_{13}\\\\\n4_{21} & 5_{22} & 6_{23}\n\\end{bmatrix}\n}  \n\\\n\\stackrel{\\mbox{Matrix B}}{\n\\begin{bmatrix}\n7_{11} & 8_{12} & 9_{13}\\\\\n9_{21} & 8_{22} & 7_{23}\n\\end{bmatrix}\n}\n\\]\nYou probably noticed that we gave each scalar within the matrix a label associated with its row and column position. We can use these to see how we will produce the new matrix:\nNow, we can set this up as an addition problem to produce Matrix C:\n\\[\n\\stackrel{\\mbox{Matrix A}}{\n\\begin{bmatrix}\n1_{11} & 2_{12} & 3_{13}\\\\\n4_{21} & 5_{22} & 6_{23}\n\\end{bmatrix}\n}  \n+\n\\stackrel{\\mbox{Matrix B}}{\n\\begin{bmatrix}\n7_{11} & 8_{12} & 9_{13}\\\\\n9_{21} & 8_{22} & 7_{23}\n\\end{bmatrix}\n}\n=\n\\stackrel{\\mbox{Matrix C}}{\n\\begin{bmatrix}\nA_{11} + B_{11}& A_{12} + B_{12} & A_{13} + B_{13}\\\\\nA_{21} + B_{21}& A_{22} + B_{22} & A_{23} + B_{23}\n\\end{bmatrix}\n}\n\\]\nNow we can pull in the real numbers:\n\\[\n\\stackrel{\\mbox{Matrix A}}{\n\\begin{bmatrix}\n1_{11} & 2_{12} & 3_{13}\\\\\n4_{21} & 5_{22} & 6_{23}\n\\end{bmatrix}\n}  \n+\n\\stackrel{\\mbox{Matrix B}}{\n\\begin{bmatrix}\n7_{11} & 8_{12} & 9_{13}\\\\\n9_{21} & 8_{22} & 7_{23}\n\\end{bmatrix}\n}\n=\n\\stackrel{\\mbox{Matrix C}}{\n\\begin{bmatrix}\n1 + 7  & 2 + 8 & 3 + 9\\\\\n4 + 9 & 5 + 8 & 6 + 7\n\\end{bmatrix}\n}\n\\]\nGiving us Matrix C:\n\\[\n\\stackrel{\\mbox{Matrix A}}{\n\\begin{bmatrix}\n1_{11} & 2_{12} & 3_{13}\\\\\n4_{21} & 5_{22} & 6_{23}\n\\end{bmatrix}\n}  \n+\n\\stackrel{\\mbox{Matrix B}}{\n\\begin{bmatrix}\n7_{11} & 8_{12} & 9_{13}\\\\\n9_{21} & 8_{22} & 7_{23}\n\\end{bmatrix}\n}\n=\n\\stackrel{\\mbox{Matrix C}}{\n\\begin{bmatrix}\n8 & 10 & 12 \\\\\n13 & 13 & 13\n\\end{bmatrix}\n}\n\\]"
  },
  {
    "objectID": "matrix_operations.html#subtraction",
    "href": "matrix_operations.html#subtraction",
    "title": "Appendix C — Matrix Operations",
    "section": "C.2 Subtraction",
    "text": "C.2 Subtraction\nTake everything that you just saw with addition and replace it with subtraction!\nJust like addition, every matrix needs to have the same dimensions if you are going to use subtraction.\nLet’s see those two matrices again and cast it as subtraction problem:\n\\[\n\\stackrel{\\mbox{Matrix A}}{\n\\begin{bmatrix}\n1_{11} & 2_{12} & 3_{13}\\\\\n4_{21} & 5_{22} & 6_{23}\n\\end{bmatrix}\n}\n-\n\\stackrel{\\mbox{Matrix B}}{\n\\begin{bmatrix}\n7_{11} & 8_{12} & 9_{13}\\\\\n9_{21} & 8_{22} & 7_{23}\n\\end{bmatrix}\n}\n=\n\\stackrel{\\mbox{Matrix C}}{\n\\begin{bmatrix}\nA_{11} - B_{11}& A_{12} - B_{12} & A_{13} - B_{13}\\\\\nA_{21} - B_{21}& A_{22} - B_{22} & A_{23} - B_{23}\n\\end{bmatrix}\n}\n\\]\nAnd now we can substitute in the real numbers:\n\\[\n\\stackrel{\\mbox{Matrix A}}{\n\\begin{bmatrix}\n1_{11} & 2_{12} & 3_{13}\\\\\n4_{21} & 5_{22} & 6_{23}\n\\end{bmatrix}\n}\n-\n\\stackrel{\\mbox{Matrix B}}{\n\\begin{bmatrix}\n7_{11} & 8_{12} & 9_{13}\\\\\n9_{21} & 8_{22} & 7_{23}\n\\end{bmatrix}\n}\n=\n\\stackrel{\\mbox{Matrix C}}{\n\\begin{bmatrix}\n1 - 7 & 2 - 8 & 3 - 9\\\\\n4 - 9 & 5 - 8 & 6 - 7\n\\end{bmatrix}\n}\n\\]\nAnd end with this matrix:\n\\[\n\\stackrel{\\mbox{Matrix A}}{\n\\begin{bmatrix}\n1_{11} & 2_{12} & 3_{13}\\\\\n4_{21} & 5_{22} & 6_{23}\n\\end{bmatrix}\n}\n-\n\\stackrel{\\mbox{Matrix B}}{\n\\begin{bmatrix}\n7_{11} & 8_{12} & 9_{13}\\\\\n9_{21} & 8_{22} & 7_{23}\n\\end{bmatrix}\n}\n=\n\\stackrel{\\mbox{Matrix C}}{\n\\begin{bmatrix}\n-6 & -6 & -6 \\\\\n-5 & -3 & -1\n\\end{bmatrix}\n}\n\\]\nAdding and subtracting matrices in R and Python is pretty simple.\nIn R, we can create a matrix a few ways: with the matrix function or by row binding numeric vectors.\n\nmatrix_A &lt;- rbind(1:3, \n                  4:6)\n\n# The following is an equivalent\n# to rbind:\n# matrix_A &lt;- matrix(c(1:3, 4:6), \n#                    nrow = 2, \n#                    ncol = 3, byrow = TRUE)\n\nmatrix_B &lt;- rbind(7:9, \n                  9:7)\n\nOnce we have those matrices created, we can use the standard + and - signs to add and subtract:\n\nmatrix_A + matrix_B\n\n     [,1] [,2] [,3]\n[1,]    8   10   12\n[2,]   13   13   13\n\nmatrix_A - matrix_B\n\n     [,1] [,2] [,3]\n[1,]   -6   -6   -6\n[2,]   -5   -3   -1\n\n\nThe task is just as easy in Python. We will import numpy and then use the matrix method to create the matrices:\n\nimport numpy as np\n\nmatrix_A = np.matrix('1 2 3; 4 5 6')\n\nmatrix_B = np.matrix('7 8 9; 9 8 7')\n\nJust like R, we can use + and - on those matrices.\n\nmatrix_A + matrix_B\n\nmatrix([[ 8, 10, 12],\n        [13, 13, 13]])\n\nmatrix_A - matrix_B\n\nmatrix([[-6, -6, -6],\n        [-5, -3, -1]])"
  },
  {
    "objectID": "matrix_operations.html#transpose",
    "href": "matrix_operations.html#transpose",
    "title": "Appendix C — Matrix Operations",
    "section": "C.3 Transpose",
    "text": "C.3 Transpose\nAs you progress through this book, you might see a matrix denoted as \\(A^T\\); here the superscripted T stands for transpose. If we transpose a matrix, all we are doing is flipping the rows and columns along the matrix’s main diagonal. A visual example is much easier:\n\\[\n\\stackrel{\\mbox{Matrix A}}{\n\\begin{bmatrix}\n1_{11} & 2_{12} & 3_{13}\\\\\n4_{21} & 5_{22} & 6_{23}\n\\end{bmatrix}\n}\n-&gt;\n\\stackrel{\\mbox{Matrix A}^T}{\n\\begin{bmatrix}\n1 & 4 \\\\\n2 & 5 \\\\\n3 & 6\n\\end{bmatrix}\n}\n\\]\nLike any matrix operation, a transpose is pretty easy to do when the matrix is small; you’re best bet is to rely on software to do anything beyond a few rows or columns.\nIn R, all we need is the t function:\n\nt(matrix_A)\n\n     [,1] [,2]\n[1,]    1    4\n[2,]    2    5\n[3,]    3    6\n\n\nIn Python, we can use numpy’s transpose method:\n\nmatrix_A.transpose()\n\nmatrix([[1, 4],\n        [2, 5],\n        [3, 6]])"
  },
  {
    "objectID": "matrix_operations.html#multiplication",
    "href": "matrix_operations.html#multiplication",
    "title": "Appendix C — Matrix Operations",
    "section": "C.4 Multiplication",
    "text": "C.4 Multiplication\nNow, you probably have some confidence in doing matrix operations. Just as quickly as we built that confidence, it will be crushed when learning about matrix multiplication.\nWhen dealing with matrix multiplication, we have a huge change to our rule. No longer can our dimensions be the same! Instead, the matrices need to be conformable – the first matrix needs to have the same number of columns as the number of rows within the second matrix. In other words, the inner dimensions must match.\nLook one more time at these matrices:\n\\[\n\\stackrel{\\mbox{Matrix A}}{\n\\begin{bmatrix}\n1_{11} & 2_{12} & 3_{13}\\\\\n4_{21} & 5_{22} & 6_{23}\n\\end{bmatrix}\n}\n.\n\\stackrel{\\mbox{Matrix B}}{\n\\begin{bmatrix}\n7_{11} & 8_{12} & 9_{13}\\\\\n9_{21} & 8_{22} & 7_{23}\n\\end{bmatrix}\n}\n\\]\nMatrix A has dimensions of \\(2x3\\), as does Matrix B. Putting those dimensions side by side – \\(2x3 * 2x3\\) – we see that our inner dimensions are 3 and 2 and do not match.\nWhat if we transpose Matrix B?\n\\[\n\\stackrel{\\mbox{Matrix B}^T}{\n\\begin{bmatrix}\n7_{11} & 9_{12} \\\\\n8_{21}& 8_{22}\\\\\n9_{31} & 7_{32}\n\\end{bmatrix}\n}\n\\]\nNow we have something that works!\n\\[\n\\stackrel{\\mbox{Matrix A}}{\n\\begin{bmatrix}\n1_{11} & 2_{12} & 3_{13}\\\\\n4_{21} & 5_{22} & 6_{23}\n\\end{bmatrix}\n}\n.\n\\stackrel{\\mbox{Matrix B}^T}{\n\\begin{bmatrix}\n7_{11} & 9_{12} \\\\\n8_{21}& 8_{22}\\\\\n9_{31} & 7_{32}\n\\end{bmatrix}\n}\n=\n\\stackrel{\\mbox{Matrix C}}{\n\\begin{bmatrix}\n. & . \\\\\n. & . \\\\\n\\end{bmatrix}\n}\n\\]\nNow we have a \\(2x3 * 3x2\\) matrix multiplication problem! The resulting matrix will have the same dimensions as our two matrices’ outer dimensions: \\(2x2\\)\nHere is how we will get at \\(2x2\\) matrix:\n\\[\n\\stackrel{\\mbox{Matrix A}}{\n\\begin{bmatrix}\n1_{11} & 2_{12} & 3_{13}\\\\\n4_{21} & 5_{22} & 6_{23}\n\\end{bmatrix}\n}\n.\n\\stackrel{\\mbox{Matrix B}^T}{\n\\begin{bmatrix}\n7_{11} & 9_{12} \\\\\n8_{21}& 8_{22}\\\\\n9_{31} & 7_{32}\n\\end{bmatrix}\n}\n=\n\\] \\[\n\\stackrel{\\mbox{Matrix C}}{\n\\begin{bmatrix}\n(A_{11}*B_{11})+(A_{12}*B_{21})+(A_{13}*B_{31}) & (A_{11}*B_{12})+(A_{12}*B_{22})+(A_{13}*B_{32}) \\\\\n(A_{21}*B_{11})+(A_{22}*B_{21})+(A_{23}*B_{31}) & (A_{21}*B_{12})+(A_{22}*B_{22})+(A_{23}*B_{32})\n\\end{bmatrix}\n}\n\\]\nThat might look like a horrible mess and likely isn’t easy to commit to memory. Instead, we’d like to show you a way that might make it easier to remember how to multiply matrices. It also gives a nice representation of why your matrices need to be conformable.\nWe can leave Matrix A exactly where it is, flip Matrix B\\(^T\\), and stack it right on top of Matrix A:\n\\[\n\\begin{bmatrix}\n9_{b} & 8_{b} & 7_{b} \\\\\n7_{b} & 8_{b} & 9_{b} \\\\\n\\\\\n1_{a} & 2_{a} & 3_{a} \\\\\n4_{a} & 5_{a} & 6_{a}\n\\end{bmatrix}\n\\]\nNow, we can let those rearranged columns from Matrix B\\(^T\\) “fall down” through the rows of Matrix A:\n\\[\n\\begin{bmatrix}\n9_{b} & 8_{b} & 7_{b} \\\\\n\\\\\n1_{a}*7_{b} & 2_{a}*8_{b} & 3_{a}*9_{b}\\\\\n4_{a} & 5_{a} & 6_{a}\n\\end{bmatrix}\n=\n\\stackrel{\\mbox{Matrix C}}{\n\\begin{bmatrix}\n50 & .\\\\\n. & .\n\\end{bmatrix}\n}\n\\]\nAdding those products together gives us 50 for \\(C_{11}\\).\nLet’s move that row down to the next row in the Matrix A, multiply, and sum the products.\n\\[\n\\begin{bmatrix}\n9_{b} & 8_{b} & 7_{b} \\\\\n\\\\\n1_{a} & 2_{a} & 3_{a}\\\\\n4_{a}*7_{b} & 5_{a}*8_{b} & 6_{a}*9_{b}\n\\end{bmatrix}\n=\n\\stackrel{\\mbox{Matrix C}}{\n\\begin{bmatrix}\n50 & .\\\\\n122 & .\n\\end{bmatrix}\n}\n\\]\nWe have 122 for \\(C_{21}\\). That first column from Matrix B\\(^T\\) won’t be used any more, but now we need to move the second column through Matrix A.\n\\[\n\\begin{bmatrix}\n1_{a}*9_{b} & 2_{a}*8_{b} & 3_{a}*7_{b}\\\\\n4_{a} & 5_{a} & 6_{a}\n\\end{bmatrix}\n=\n\\stackrel{\\mbox{Matrix C}}{\n\\begin{bmatrix}\n50 & 46\\\\\n122 & .\n\\end{bmatrix}\n}\n\\]\nThat gives us 46 for \\(C_{12}\\).\nAnd finally:\n\\[\n\\begin{bmatrix}\n1_{a} & 2_{a} & 3_{a}\\\\\n4_{a}*9_{b} & 5_{a}*8_{b} & 6_{a}*7_{b}\n\\end{bmatrix}\n=\n\\stackrel{\\mbox{Matrix C}}{\n\\begin{bmatrix}\n50 & 46\\\\\n122 & 118\n\\end{bmatrix}\n}\n\\]\nWe have 118 for \\(C_{22}\\).\nNow that you know how these work, you can see how easy it is to handle these tasks in R and Python.\nIn R, we need to use a fancy operator: %*%. This is just R’s matrix multiplication operator. We will also use the transpose function: t.\n\nmatrix_A %*% t(matrix_B)\n\n     [,1] [,2]\n[1,]   50   46\n[2,]  122  118\n\n\nIn Python, we can just use the regular multiplication operator and the transpose method:\n\nmatrix_A * matrix_B.transpose()\n\nmatrix([[ 50,  46],\n        [122, 118]])\n\n\nYou can see that whether we do this by hand, R, or Python, we come up with the same answer! While these small matrices can definitely be done by hand, we will always trust the computer to handle larger matrices."
  },
  {
    "objectID": "matrix_operations.html#inversion",
    "href": "matrix_operations.html#inversion",
    "title": "Appendix C — Matrix Operations",
    "section": "C.5 Inversion",
    "text": "C.5 Inversion\nYou might want to think of matrix inversion as the reciprocal of the matrix, usually noted as \\(A^{-1}\\). The biggest reason that we might invert a matrix is because there is no matrix division.\nInversion can only be performed on square matrices (e.g., \\(2x2\\), \\(3x3\\), \\(4x4\\)) and the determinant of a matrix cannot be 0. Since the determinant is important for finding the inverse, we should probably have an idea about how to find the determinant.\n\nC.5.1 Matrix Determinant\nWhile we’ve been using the matrix row/column positions in our examples, we are going to shift to letters to label the positions. We can start with a \\(2x2\\) matrix:\n\\[\n\\stackrel{\\mbox{Matrix C}}{\n\\begin{bmatrix}\nA & B\\\\\nC & D\n\\end{bmatrix}\n}\n\\]\nTo find the determinant, we would take \\(\\mid C \\mid = (A*D) - (B*C)\\).\nReturning back to Matrix C, we have \\(\\mid C \\mid = (50_a*118_d) - (46_b*122_c) = 288\\)\n\\[\n\\stackrel{\\mbox{Matrix C}}{\n\\begin{bmatrix}\n50 & 46\\\\\n122 & 118\n\\end{bmatrix}\n}\n\\]\nA \\(3x3\\) matrix doesn’t pose much more of a challenge.\n\\[\n\\stackrel{\\mbox{Matrix D}}{\n\\begin{bmatrix}\nA & B & C\\\\\nD & E & F\\\\\nG & H & I\n\\end{bmatrix}\n}\n\\]\nThe canonical form might not be as intuitive, but it is worth seeing:\n\\[\n\\mid D \\mid = A\\begin{vmatrix}\nE & I\\\\\nF & H\n\\end{vmatrix}  -\nB\\begin{vmatrix}\nD & I\\\\\nF & G\n\\end{vmatrix} +\nC\\begin{vmatrix}\nD & H\\\\\nE & G\n\\end{vmatrix}\n\\]\nBreaking it down a bit further will help to see where all of the values go:\n\\[\n\\mid D \\mid = A(E*I - F*H) - B(D*I - F*G) + C(D*H - E*G)\n\\] Now we can work that out with a real matrix:\n\\[\n\\stackrel{\\mbox{Matrix D}}{\n\\begin{bmatrix}\n2 & 1 & 3\\\\\n6 & 5 & 4\\\\\n7 & 8 & 9\n\\end{bmatrix}\n}\n\\]\nTo get our determinant:\n\\[\n\\mid D \\mid = 2(5*9 - 4*8) - 1(6*9 - 4*7) + 3(6*8 - 5*7) = 39\n\\]\nAnd just to confirm that our math is correct, we can check for the determinant in R and Python.\nR has a handy function called det:\n\nmatrix_D &lt;- matrix(c(2, 1, 3,\n                     6, 5, 4,\n                     7, 8, 9), \n                   nrow = 3, \n                   ncol = 3, \n                   byrow = TRUE)\n\ndet(matrix_D)\n\n[1] 39\n\n\nWe can keep using numpy, but we will have to use det within the linalg module.\n\nmatrix_D = np.matrix('2 1 3; 6 5 4; 7 8 9')\n\nnp.linalg.det(matrix_D)\n\n38.99999999999999\n\n\nJust to show you how this pattern would continue\nYou can find a lot of examples online on how to do \\(2x2\\) and \\(3x3\\) matrix inversions, mostly because they are the easiest to do.\nHow do you know that you properly inverted your matrix? You multiply the original matrix by the inverse matrix and you will get an identity matrix.\nWe have a nice figure in Figure @ref(fig:hello), and also a table in Table @ref(tab:iris)."
  },
  {
    "objectID": "appendix_placeholder.html#simulation",
    "href": "appendix_placeholder.html#simulation",
    "title": "Appendix D — Other to come",
    "section": "D.1 Simulation",
    "text": "D.1 Simulation"
  },
  {
    "objectID": "appendix_placeholder.html#bayesian-demonstration",
    "href": "appendix_placeholder.html#bayesian-demonstration",
    "title": "Appendix D — Other to come",
    "section": "D.2 Bayesian Demonstration",
    "text": "D.2 Bayesian Demonstration\nMetropolis-Hastings demo\n\n# Define the log-likelihood function for linear regression\nlog_likelihood &lt;- function(beta, X, y, sigma_sq) {\n    y_hat &lt;- X %*% beta\n    residuals &lt;- y - y_hat\n    log_likelihood &lt;- -0.5 * length(y) * log(2 * pi * sigma_sq) - 0.5 * sum(residuals^2) / sigma_sq\n    return(log_likelihood)\n}\n\n# Define the prior distribution for beta\nprior_beta &lt;- function(beta) {\n    prior_mean &lt;- rep(0, length(beta))\n    prior_sd &lt;- rep(10, length(beta))\n    log_prior &lt;- sum(dnorm(beta, mean = prior_mean, sd = prior_sd, log = TRUE))\n    return(log_prior)\n}\n\n# Define the prior distribution for sigma\nprior_sigma &lt;- function(sigma_sq) {\n    alpha &lt;- 2\n    beta &lt;- 2\n    # log_prior &lt;- dgamma(1/sigma_sq, shape = alpha, rate = beta, log = TRUE)\n    log_prior &lt;- extraDistr::dinvgamma(sigma_sq, alpha = alpha, beta = beta, log = TRUE) \n\n    return(log_prior)\n}\n\n# Define the proposal distribution for beta\nproposal_beta &lt;- function(beta, scale) {\n    beta_proposal &lt;- rnorm(length(beta), mean = beta, sd = scale)\n    return(beta_proposal)\n}\n\n# Define the proposal distribution for sigma\nproposal_sigma &lt;- function(sigma_sq, scale) {\n    # sigma_proposal &lt;- rgamma(1, shape = sigma_sq / scale, rate = scale)\n    sigma_proposal &lt;- extraDistr::rinvgamma(1, alpha = sigma_sq / scale, beta = scale)\n    return(sigma_proposal)\n}\n\n# Set up the data\n# set.seed(123)\n# n &lt;- 100\n# X &lt;- cbind(1, rnorm(n), rnorm(n), rnorm(n))\n# beta_true &lt;- c(1, 2, 3, 4)/4\n# sigma_true &lt;- 1\n# y &lt;- X %*% beta_true + rnorm(n, sd = sigma_true)\n\n# Set up the Metropolis-Hastings algorithm\n# n_iter &lt;- 10000\n\n\n# Run the Metropolis-Hastings algorithm\nmh = function(\n    X,\n    y,\n    beta = rep(0, ncol(X)), \n    sigma_sq = .5, \n    scale_beta = 0.1, \n    scale_sigma = 1,\n    chains = 2,\n    warmup = 1000,\n    n_iter = 2000,\n    seed = 123\n) {\n    set.seed(seed)\n\n    result &lt;- list()\n    beta_start &lt;- beta\n    sigma_sq_start &lt;- sigma_sq\n\n    for (c in 1:chains){\n        acceptance_beta &lt;- 0\n        acceptance_sigma &lt;- 0\n        beta_samples &lt;- matrix(0, n_iter, ncol(X))\n        sigma_sq_samples &lt;- rep(0, n_iter)\n\n        if (c &gt; 1) {\n            beta &lt;- beta_start\n            sigma_sq &lt;- sigma_sq_start\n        }       \n\n        for (i in 1:n_iter) {\n            # Update beta\n            beta_proposal &lt;- proposal_beta(beta, scale_beta)\n            log_ratio_beta &lt;- log_likelihood(beta_proposal, X, y, sigma_sq) + prior_beta(beta_proposal) -\n                log_likelihood(beta, X, y, sigma_sq) - prior_beta(beta)\n            if (log(runif(1)) &lt; log_ratio_beta) {\n                beta &lt;- beta_proposal\n                acceptance_beta &lt;- acceptance_beta + 1\n            }\n            beta_samples[i, ] &lt;- beta\n\n            # Update sigma_sq\n            sigma_sq_proposal &lt;- proposal_sigma(sigma_sq, scale_sigma)\n            log_ratio_sigma &lt;- log_likelihood(beta, X, y, sigma_sq_proposal) + prior_sigma(sigma_sq_proposal) -\n                log_likelihood(beta, X, y, sigma_sq) - prior_sigma(sigma_sq)\n            if (log(runif(1)) &lt; log_ratio_sigma) {\n                sigma_sq &lt;- sigma_sq_proposal\n                acceptance_sigma &lt;- acceptance_sigma + 1\n            }\n            sigma_sq_samples[i] &lt;- sigma_sq\n        }\n    \n        message(\"Acceptance rate for beta:\", acceptance_beta / n_iter, \"\\n\")\n        message(\"Acceptance rate for sigma:\", acceptance_sigma / n_iter, \"\\n\")\n\n        result[[c]] = list(\n            beta = beta_samples[-(1:warmup), ], \n            sigma_sq = sigma_sq_samples[-(1:warmup)],\n            # y_rep = X %*% t(beta_samples[-(1:warmup), ])\n            # +rnorm(n_iter - warmup, sd = sqrt(sigma_sq_samples[-(1:warmup)]))\n            y_rep = t(X %*% t(beta_samples[-(1:warmup), ]) + rnorm(n_iter - warmup, sd = sqrt(sigma_sq_samples[-(1:warmup)])))\n        )\n    } \n    result\n}\n\nX_train = df_happiness |&gt;\n    select(life_exp, gdp_pc, corrupt) |&gt;\n    as.matrix()\n\nour_result = mh(\n    X = cbind(1, X_train),\n    y = df_happiness$happiness, \n    beta = c(mean(df_happiness$happiness), rep(0, ncol(X_train))),\n    sigma_sq = var(df_happiness$happiness),\n    scale_sigma = .5,\n    warmup = 1000,\n    n_iter = 2000\n)\n\nstr(our_result)"
  },
  {
    "objectID": "appendix_placeholder.html#linear-programming",
    "href": "appendix_placeholder.html#linear-programming",
    "title": "Appendix D — Other to come",
    "section": "D.3 Linear Programming",
    "text": "D.3 Linear Programming"
  },
  {
    "objectID": "generalized_linear_models.html#distributions-link-functions",
    "href": "generalized_linear_models.html#distributions-link-functions",
    "title": "6  Generalized Linear Models",
    "section": "6.1 Distributions & Link Functions",
    "text": "6.1 Distributions & Link Functions\nRemember how linear models really enjoy the whole Gaussian distribution scene? The essential form of the linear model can be expressed as follows:\n\\[y \\sim Normal(\\mu,\\sigma) \\\\ \\mu = \\alpha + \\beta X\\]\nNot all data follows a Gaussian distribution. Instead, we often find some other form of an exponential distribution. So, we need a way to incorporate different distributions of the target into our model. Distributions cannot do it alone! We also need a link function to connect the linear model to the distribution.\nFrom a theoretical perspective, link functions are tricky to get your head around.\n\nFind the exponential of the response’s density function and derive the canonical link function…\n\nFrom a conceptual perspective, all they are doing is allowing the linear feature to “link” to a distribution function’s mean. If you know a distribution’s canonical link function, that is all the deeper you will probably every need.\nAt the end of the day, these link functions will convert the target to an unbounded continuous variable. The take-away here is that the link function describes how the mean is generated from the predictors."
  },
  {
    "objectID": "generalized_linear_models.html#logistic-regression",
    "href": "generalized_linear_models.html#logistic-regression",
    "title": "6  Generalized Linear Models",
    "section": "6.2 Logistic Regression",
    "text": "6.2 Logistic Regression\n\n6.2.1 Why Should You Care\nYou will often have a binary variable that you might want to use as a target – it could be dead/alive, lose/win, quit/retain, etc. You might be tempted to use a linear regression, but you will quickly find that it is not the best option. You are going to be figuring out the probability of moving from “failure” to “success”, given the features in your model.\n\n\n6.2.2 The Binomial Distribution\nLogistic regression is substantially different than linear regression. It is also a bit confusing, because it is named after its link function (logit) instead of its distribution (binomial). Instead of that nice continuous target, we are dealing with a binomially-distributed target and the target takes the form of a binary variable.\nWe don’t have a \\(\\mu\\) or \\(\\sigma^2\\) to identify the shape of the binomial distribution; instead we have p and n, where p is a probability and n is the number of trials. We tend to talk about p with regard to the probability of a specific event happening (heads, wins, defaulting, etc.).\nLet’s see how the binomial distribution looks with 100 trials and probabilities of “success” at p =  .25, .5, and .75:\n\n\n\n\n\nFigure 6.1: Binomial distributions for different probabilities\n\n\n\n\nIf we examine the distribution for a probability of .5 in Figure 6.1, we will see that it is centered over 50 – this would suggest that we have the highest probability of encountering 50 successes if we ran 100 trials. If we run 100 trials 100 times and the outcome is 50/50, the most common outcome from those 100 trials would be 50 successes. with a decreasing probability of observing more or less successes as we move away from 50. Shifting our attention to a .75 probability of success, we see that our density is sitting over 75. Again running 100 trials, would give us the highest probability of observing 75 successes. Some of those 100 trials produce more or less than 75 successes, but with lower probabilities as you get further away from 75.\nSince we are dealing with a number of trials, it is worth noting that the binomial distribution is a discrete distribution. If you have any interest in knowing the probability for a number of success under the binomial distribution, we can use the following formula:\n\\[P(x) = \\frac{n!}{(n-x)!x!}p^xq^{n-x}\\]\nWhile we don’t need to dive into finding those specific values for the binomial distribution, we can spend our time exploring how it looks in linear model space:\n\\[y \\sim Binomial(n, p) \\\\ logit(p) = \\alpha + \\beta X\\]\nThe logit function is defined as:\n\\[log\\frac{p}{1-p}\\]\nWe are literally just taking the log of the odds (the log odds becomes important later).\nNow we can map this back to our model:\n\\[log\\frac{p}{1-p} = \\alpha + \\beta X\\]\nAnd finally we can take that logistic function and invert it (the inverse-logit) to produce the probabilities.\n\\[p = \\frac{exp(\\alpha + \\beta X)}{1 + exp(\\alpha + \\beta X)}\\]\nWhenever we get coefficients for the logistic regression model, we are always going to get them as log odds. We can exponentiate them to get the odds ratio, but we can also exponentiate them and divide by 1 + that value to get the probability.\n\n\n6.2.3 Probability, Odds, and Log Odds\nProbability lies at the heart of all of this. We can look at the relationship between the probability, odds, and log odds. We can start with a set of probability values where \\(0 &lt; p &gt; 1\\)\nWith that list of probability values, we can convert them to odds with \\(\\\\p\\, / 1 - p\\).\n\n\n\n\n\nFigure 6.2: Log odds and odds values for a range of probabilities\n\n\n\n\nWe can see how those probability values map to odds in Figure 6.2.\nNow, we can take those odds values and convert them to log odds.\n\n\n\n\n\nFigure 6.3: Log odds and probability values\n\n\n\n\nIf you’ve ever seen the sigmoid featured in Figure 6.3 before, it is the classic logistic function!\nWe can clearly go back and forth between the 3, but the main message here is that we took a bounded variable in probability and transformed it to continuous space.\nWe will see more about how this happens after playing with the model.\n\n\n6.2.4 Data Import and Preparation\nWe are going to return to our movie reviews data and we are going to use rating_good as our target. Before we get to modeling, see if you can find out the frequency of “good” and “bad” reviews. We will use word_count and gender as our predictors. Before we move on, though, find the probability of getting a “good” review.\n\nRPython\n\n\n\nreviews &lt;- read.csv(\"data/movie_reviews_processed.csv\")\n\n\nX &lt;- reviews[, c(\"word_count\", \"gender\")]\n\nX = cbind(1, X)\n\nX$gender &lt;- ifelse(X$gender == \"male\", 1, 0)\n\nX &lt;- as.matrix(X)\n\ny &lt;- reviews$rating_good\n\n\n\n\nimport pandas as pd\n\nreviews = pd.read_csv(\"data/movie_reviews_processed.csv\")\n\n\nX = reviews[['word_count', 'gender']]\n\ny = reviews[\"rating_good\"]\n\n\n\n\n\n\n6.2.5 Standard Functions\nTo get started with our first logistic regression model, let’s use the glm function from R and Python’s statsmodels function.\n\nRPython\n\n\n\nlogistic_regression &lt;- glm(\n    rating_good ~ word_count + gender, \n    data = reviews,\n    family = binomial\n)\n\nsummary(logistic_regression)\n\n\nCall:\nglm(formula = rating_good ~ word_count + gender, family = binomial, \n    data = reviews)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  1.71240    0.18136   9.442   &lt;2e-16 ***\nword_count  -0.14639    0.01551  -9.436   &lt;2e-16 ***\ngendermale   0.11891    0.13751   0.865    0.387    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1370.4  on 999  degrees of freedom\nResidual deviance: 1257.4  on 997  degrees of freedom\nAIC: 1263.4\n\nNumber of Fisher Scoring iterations: 4\n\n\n\n\n\nimport statsmodels.api as sm\n\nX = sm.add_constant(X)\n\nX = pd.get_dummies(X, drop_first = True)\n\nlogistic_regression = sm.Logit(y, X.astype(float)).fit()\n\nOptimization terminated successfully.\n         Current function value: 0.628697\n         Iterations 5\n\nlogistic_regression.summary()\n\n\nLogit Regression Results\n\n\nDep. Variable:\nrating_good\nNo. Observations:\n1000\n\n\nModel:\nLogit\nDf Residuals:\n997\n\n\nMethod:\nMLE\nDf Model:\n2\n\n\nDate:\nSat, 09 Dec 2023\nPseudo R-squ.:\n0.08245\n\n\nTime:\n10:08:17\nLog-Likelihood:\n-628.70\n\n\nconverged:\nTrue\nLL-Null:\n-685.19\n\n\nCovariance Type:\nnonrobust\nLLR p-value:\n2.925e-25\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nconst\n1.7124\n0.181\n9.442\n0.000\n1.357\n2.068\n\n\nword_count\n-0.1464\n0.016\n-9.436\n0.000\n-0.177\n-0.116\n\n\ngender_male\n0.1189\n0.138\n0.865\n0.387\n-0.151\n0.388\n\n\n\n\n\n\n\n\n\n\n6.2.6 Interpretation and Visualization\nWe need to know what those results mean. The coefficients that we get from our model are in log odds. We can exponentiate them to get the odds ratio, but we can also exponentiate them and divide by 1 + that value to get the probability. Interpretting log odds is a fool’s errand, but we can at least get a feeling for them directionally. A log odds of 0 would indicate no relationship between the feature and target. A positive log odds would indicate that an increase in the feature will increase the log odds of moving from “bad” to “good”, whereas a negative log odds would indicate that a decrease in the feature will decrease the log odds of moving from “bad” to “good”. We can convert those log odds to help make some more sense from them.\nWhen we exponentiate the log odds coefficients, we are given the odds ratio. This is the ratio of the odds of the outcome (i.e., success from our binomial distribution) occurring for a one unit increase in the predictor.\n\n\n(Intercept)  word_count  gendermale \n  5.5422663   0.8638229   1.1262687 \n\n\nFortunately, the intercept is easy – it is the odds of a “good” review when word count is 0 and gender is “female”. We see that we’ve got an odds ratio of .86 for the word_count variable and 1.12 for the male variable. An odds ratio of 1 means that there is no change in the odds of the outcome occurring – essentially that the predictor does not influence the target. An odds ratio of less than 1 means that the odds of the outcome occurring decrease as the predictor increases (while a bit more complicated to wrap your head around, it captures the idea of the odds of moving from a “bad” review to a “good” review decreasing). An odds ratio of greater than 1 means that the odds of the outcome occurring increase as the predictor increases (again, the odds of moving from a “bad” review to a “good” review increasing).\nIt is far more intuitive to interpret the probability. We can do this by exponentiating the coefficients and dividing by 1 + that value. This will give us the probability of the outcome occurring for a one unit increase in the predictor.\n\n\n(Intercept)  word_count  gendermale \n  0.8471478   0.4634683   0.5296925 \n\n\nWe would say that our probability of moving from a “bad” review to a “good” review is .84 when there are 0 words in the review and the gender is female. Since word_count is below .5, we know that it will have a negative relationship with the probability of moving from “bad” to “good”; being a male reviewer will have a positive relationship with the probability of moving from “bad” to “good”.\nAnd visualizing those probabilities is absolutely the best way to see how the features influence the target:\n\n\n\n\n\nFigure 6.4: Logistic regression predictions for word count feature\n\n\n\n\nIn Figure 6.4, we can see a clear negative relationship between the number of words in a review and the probability of being considered a “good” movie. As we get over 20 words, the predicted probability of being a “good” movie is less than .2.\n\n\n\n\n\nFigure 6.5: Logistic regression predictions for gender feature\n\n\n\n\nWe can also see the gender effect in Figure 6.5 (it doesn’t look like gender is that interesting in this model).\nThere are interesting issues at play here with regard to our predictor coefficients (what can be considered a relative effect) and the model’s effect as a whole on the probability (the absolute effect). In circumstances where the intercept is very large (essentially promising a success), the relative effect of a coefficient is practically meaningless. Similarly, very negative coefficients render the relative effects useless.\n\n\n6.2.7 Loss Function\nLet’s see how we can pick that work apart to create our own functions. We can use maximum likelihood estimation to estimate the parameters of our model.\n\nRPython\n\n\n\nlogreg_ml &lt;- function(par, X, y) {\n  beta = par\n  N = nrow(X)\n  LP = X %*% beta                           \n  mu = plogis(LP)                           \n  L = dbinom(y, size = 1, prob = mu, log = TRUE)   \n  -sum(L)                                   \n}\n\n\n\n\ndef logreg_ml(par, X, y):\n    beta = par\n    N = X.shape[0]\n    LP = X.dot(beta).to_numpy()  \n    mu = [1 / (1 + np.exp(-x)) for x in LP]\n    mu_minus_1 = [1 - x for x in mu]\n    L = y*np.log(mu) + (1 - y)*np.log(mu_minus_1)   \n    return -np.sum(L)   \n  \n\n\n\n\n\n\n6.2.8 Model Fitting\nNow that we have our loss function, we can fit our model. We will use the optim function in R and the minimize function in Python.\n\nRPython\n\n\n\ninit = rep(0, ncol(X))\n\nnames(init) = c('intercept', 'b1', 'b2')\n\nfit_ml = optim(\n  par = init,\n  fn  = logreg_ml,\n  X   = X,\n  y   = y,\n  control = list(reltol = 1e-8)\n)\n\npars_ml = fit_ml$par\n\npars_ml\n\n intercept         b1         b2 \n 1.7121816 -0.1463750  0.1189308 \n\n\n\n\n\nimport numpy as np\nfrom scipy.optimize import minimize\n\ninit = np.zeros(X.shape[1])\n\nfit_ml = minimize(\n    fun = logreg_ml,\n    x0 = init,\n    args = (X, y),\n    method = 'BFGS',\n    options = {'disp': True}\n)\n\nOptimization terminated successfully.\n         Current function value: 628.696593\n         Iterations: 11\n         Function evaluations: 68\n         Gradient evaluations: 17\n\nfit_ml.x\n\narray([ 1.71240414, -0.14638763,  0.11891015])\n\n\n\n\n\n\n\n\n\n\n\nIn theory, there is no such thing as 0 or 1 probability. When your model encounters such a value, you will receive a warning, not an error. The most likely cause of this warning is separation: a variable is perfectly separating the target. In other words, once a feature gets below/above a certain value, the target is always 0/1. While that variable is no doubt valuable, it can’t be used in a logistic regression model. More evidence of separation comes when you see your log odds coefficients return something comically large.\n\n\n\n\n\n\n\n\n\nLogistic regression does not have an \\(R^2\\) value in the way that a linear regression model does. Instead, there are pseudo-\\(R^2\\) values, but they are not the same as the \\(R^2\\) value that you are used to seeing. Here is a great breakdown of different pseudo methods."
  },
  {
    "objectID": "generalized_linear_models.html#poisson-regression",
    "href": "generalized_linear_models.html#poisson-regression",
    "title": "6  Generalized Linear Models",
    "section": "6.3 Poisson Regression",
    "text": "6.3 Poisson Regression\n\n6.3.1 Why Should You Care\nLike logistic regression, poisson regression belongs to a broad class of generalized linear models. Poisson regression is used when you have a count variable as your target. The nature of a count variable is very different, since it starts at 0 and can only be a whole number. We need a model that will not produce negative predictions and poisson regression will do that for us.\n\n\n6.3.2 The Poisson Distribution\nThe Poisson distribution is very similar to the binomial distribution, but has some key differences. The biggest difference is in its parameter: Poisson has a single parameter noted as \\(\\lambda\\). This rate parameter is going to estimate the expected number of events during a time interval. This can be accidents in a year, pieces produced in a day, or hits during the course of a baseball season. We can find the rate by determining the number of events per interval, multiplied by the interval length.\n\\[\\frac{\\text{event}}{\\text{interval}}*\\text{interval length} \\]\nTo put some numbers to that, if we have 1 accident per week in a factory and we are observing a whole year, we would have a rate of \\((1 / 7) * 28 = 4\\) accidents per month.\nLet’s see what that particular distribution might look like in Figure 6.6:\n\n\n\n\n\nFigure 6.6: Poisson distribution for a rate of 4\n\n\n\n\nWe can also see what it looks like for different rates (some places might be safer than others) in Figure 6.7:\n\n\n\n\n\nFigure 6.7: Poisson distributions for different rates\n\n\n\n\n\n\n\n\n\n\nA cool thing about these distributions is that they can deal with different exposure rates. You don’t need observations recorded over the same interval length, because you can adjust for them appropriately. They can also be used to model inter-arrival times and time-until events.\n\n\n\nLet’s make a new variable that will count the number of times a person uses a personal pronoun word.\n\nRPython\n\n\n\nreviews$poss_pronoun &lt;- stringr::str_count(\n  reviews$review_text, \n  \"\\\\bI\\\\b|\\\\bme\\\\b|\\\\b[Mm]y\\\\b|\\\\bmine\\\\b|\\\\bmyself\\\\b\"\n)\n\n\n\n\nreviews['poss_pronoun'] = reviews['review_text'].str.count(\n  '\"\\\\bI\\\\b|\\\\bme\\\\b|\\\\b[Mm]y\\\\b|\\\\bmine\\\\b|\\\\bmyself\\\\b\"'\n  )"
  },
  {
    "objectID": "generalized_linear_models.html#the-sometimes-thin-line",
    "href": "generalized_linear_models.html#the-sometimes-thin-line",
    "title": "6  Generalized Linear Models",
    "section": "6.4 The (Sometimes) Thin Line",
    "text": "6.4 The (Sometimes) Thin Line\nThis gets into an area where we need to think long and hard about our dependent variable and what it actually might be. Since Poisson regression gets its name from the Poisson distribution, we should probably see if it follows the Poisson distribution.\n\n\n\n     Goodness-of-fit test for poisson distribution\n\n                      X^2 df  P(&gt; X^2)\nLikelihood Ratio 2.283728  3 0.5156453\n\n\nThis is a \\(\\chi^2\\) to test if the distribution deviates from a Poisson. If we see a significant value, we would say that it deviates from the tested distribution. In this case, it is pretty clear that poss_pronoun could come from a Poisson distribution.\nWe can also plot that test using a hanging rootogram:\n\n\n\n\n\nFigure 6.8: Hanging rootogram for Poisson distribution\n\n\n\n\nIn Figure 6.8, the bars are the observed counts and the red line/points are the fitted counts (i.e., how many would be expected). If a bar does not reach the 0 line, then the model would over-predict for that particular count; if the bar dips below the 0 line, the model under-predicts that count. It looks like we are pretty close for our counts.\n\n6.4.1 Standard Functions\nRecall that every distribution has a link function (or several) that tend to work well for it. The poisson distribution uses a log link function:\n\\[y = Poisson(\\lambda) \\\\ \\text{log}(\\lambda) = \\alpha + \\beta X\\]\nUsing the log link keeps the outcome positive (we cannot deal with negative counts). Logs, as they are prone to do, are going to tend towards an exponential relationship; just be sure that it makes sense over the entire range of your data.\n\nRPython\n\n\n\npoisson_test &lt;- glm(poss_pronoun ~ word_count,\n                  data = reviews,\n                  family = poisson)\n\nsummary(poisson_test)\n\n\nCall:\nglm(formula = poss_pronoun ~ word_count, family = poisson, data = reviews)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -1.848982   0.099409  -18.60   &lt;2e-16 ***\nword_count   0.103126   0.006433   16.03   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 996.21  on 999  degrees of freedom\nResidual deviance: 776.19  on 998  degrees of freedom\nAIC: 1699.7\n\nNumber of Fisher Scoring iterations: 5\n\nexp(poisson_test$coefficients)\n\n(Intercept)  word_count \n  0.1573974   1.1086314 \n\n\n\n\n\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\npoisson_test = smf.glm(formula = \"poss_pronoun ~ word_count\",\n                       data = reviews,\n                       family = sm.families.Poisson()).fit()\n\npoisson_test.summary()        \n\n\nGeneralized Linear Model Regression Results\n\n\nDep. Variable:\nposs_pronoun\nNo. Observations:\n1000\n\n\nModel:\nGLM\nDf Residuals:\n998\n\n\nModel Family:\nPoisson\nDf Model:\n1\n\n\nLink Function:\nLog\nScale:\n1.0000\n\n\nMethod:\nIRLS\nLog-Likelihood:\n-583.89\n\n\nDate:\nSat, 09 Dec 2023\nDeviance:\n689.32\n\n\nTime:\n10:08:19\nPearson chi2:\n785.\n\n\nNo. Iterations:\n5\nPseudo R-squ. (CS):\n0.008438\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n-1.7900\n0.144\n-12.415\n0.000\n-2.073\n-1.507\n\n\nword_count\n0.0344\n0.011\n3.011\n0.003\n0.012\n0.057\n\n\n\n\nnp.exp(poisson_test.params)\n\nIntercept     0.166965\nword_count    1.035001\ndtype: float64\n\n\n\n\n\nWe are going to interpret this almost the same as a linear regression. The slight wrinkle here, though, is that we are looking at the log counts (remember that we specified the log link function). In other words, an increase in one one review word leads to an expected log count increase of ~.01. Just like our logisitc regression, we could exponentiate this to get 1.108 – every added word in a review gets us a ~1% increase in the number of possessive pronouns. Let’s see what this looks like in action in Figure 6.9:\n\n\n\n\n\nFigure 6.9: Poisson regression predictions for word count feature\n\n\n\n\nWith everything coupled together, we have a meaningful coefficient for word_count, a clear plot, and adequate model fit. Therefore, we might conclude that there is a positive relationship between number of words in a review on the number of times a person uses a personal possessive.\n\n\n\n    Overdispersion test\n\ndata:  poisson_test\nz = -8.0493, p-value = 1\nalternative hypothesis: true dispersion is greater than 1\nsample estimates:\ndispersion \n 0.7606014 \n\n\nThe dispersion value that we see returned (0.7606014 in our case) should be under 1. A dispersion value over 1 means that we have overdispersion. Our dispersion value, coupled with our high p-value, indicates that we would fail to reject the null hypothesis of equidispersion.\nWe can also look back to our model results to compare our residual deviance to our residual deviance degrees of freedom; if our deviance is greater than our degrees of freedom, we might have an issue with overdispersion. Since we are just a bit over and our overdispersion tests do not indicate any huge issue, we can be relatively okay with our model. If we had some more extreme overdispersion, we would want to flip to a quasi-poisson distribution – our coefficients would not change, but we would have improved standard errors.\n\n\n6.4.2 Model Specification\n\nRPython\n\n\n\npois_ll &lt;- function(y, X, par) {\n  beta &lt;- par\n  lambda &lt;- exp(beta%*%t(X))\n  loglik &lt;- -sum(dpois(y, lambda, log = TRUE))\n  return(loglik)\n}\n\n\n\n\nfrom scipy.stats import poisson\n\ndef pois_ll(par, X, y):\n    beta = par\n    lambda_ = np.exp(X.dot(beta))\n    loglik = -np.sum(poisson.logpmf(y, lambda_))\n    return loglik\n\n\n\n\n\n\n6.4.3 Model Fitting\n\nRPython\n\n\n\nform &lt;- as.formula(\"poss_pronoun ~ word_count\")\nmodel &lt;- model.frame(form, data = reviews)\nX &lt;- model.matrix(form, data = reviews)\ny &lt;- model.response(model)\n\nstarts &lt;- c(0, 0)\n\nfit = optim(\n  par = starts ,\n  fn  = pois_ll,\n  X   = X,\n  y   = y,\n  method  = \"BFGS\",\n  control = list(maxit = 5000, reltol = 1e-12),\n  hessian = TRUE\n)\n\nfit$par\n\n[1] -1.8487431  0.1031103\n\n\n\n\n\nX = reviews[['word_count']]\n\ny = reviews[\"poss_pronoun\"]\n\ninit = np.zeros(X.shape[1])\n\nfit = minimize(\n    fun = pois_ll,\n    x0 = init,\n    args = (X, y),\n    method = 'BFGS',\n    options = {'disp': True}\n)\n\nOptimization terminated successfully.\n         Current function value: 667.282560\n         Iterations: 9\n         Function evaluations: 22\n         Gradient evaluations: 11\n\nfit.x\n\narray([-0.11938671])"
  },
  {
    "objectID": "generalized_linear_models.html#wrapping-up",
    "href": "generalized_linear_models.html#wrapping-up",
    "title": "6  Generalized Linear Models",
    "section": "6.5 Wrapping Up",
    "text": "6.5 Wrapping Up\nThese are just two of the many models that fall under the broad umbrella of generalized linear models. Depending on your data situation, you might want to keep Figure 6.10 in mind:\n\n\n\n\n\n\n\n\n\nTarget\nDistribution\n\n\n\n\nProportions\nbeta\n\n\nExponential response\ngamma\n\n\n3+ categories\nmultinomial\n\n\nCount\nnegative binomial\n\n\n\n\n\nFigure 6.10: Targets and distributions for generalized linear models\n\n\n\nThat is, however, just a tiny slice of the potential distributions that you might find yourself needing to use in a GLM. While you could always use the general linear model, the key is to understand the distribution of your target and then find the appropriate link function to connect it to the linear model. Using the proper distribution will always yield better results and get your model a little closer to the “truth”."
  },
  {
    "objectID": "generalized_linear_models.html#additional-resources",
    "href": "generalized_linear_models.html#additional-resources",
    "title": "6  Generalized Linear Models",
    "section": "6.6 Additional Resources",
    "text": "6.6 Additional Resources\nIn any given graduate coursework, you might find a whole semester dedicated to GLMs. We’ve only scratched the surface here, but there are some great resources out there to help you dig deeper. If you are itching for a text book, there isn’t any shortage of them out there and you can essentially take your pick. If you are looking for something a bit more applied, you might want to check out Roback and Legler’s Beyond Multiple Linear Regression, available for free at https://bookdown.org/roback/bookdown-BeyondMLR/"
  },
  {
    "objectID": "linear_model_extensions.html#quantile-regression",
    "href": "linear_model_extensions.html#quantile-regression",
    "title": "7  Beyond the Basics",
    "section": "7.1 Quantile Regression",
    "text": "7.1 Quantile Regression\n\n\nOh, you think the mean is your ally. But you merely adopted the mean; I was born in it, molded by it. I didn’t see anything interesting until I was already a man. And by then, it was nothing to me but illuminating. – Bane (probably)\n\nPeople generally understand the concept of the arithmetic mean. You see it some time during elementary school, it gets tossed around in daily language (usually using the word “average”), and it is statistically important. After all, where would the normal distribution be without a mean? Why, though, do we feel so tied to it from a regression modeling perspective? Yes, it has handy features, but it is also a bit restrictive to the types of relationships that it can actually model well.\nIn this chapter, we want to show you what to do when the mean betrays you – and trust us, the mean will betray you at some point.\n\n7.1.1 Why Should You Care?\nWhen fitting a line through the mean doesn’t make sense, whether due to extreme scores or you’re interested at how your model is performing in different parts of your data, quantile regression can be helpful. Quantile regression will give you the ability to model the relationship between your features and target at different quantiles of your target. Maybe people who are older are more likely to rate movies higher, but that relationship is stronger for people who rate movies higher than the median. Quantile regression will let you model that relationship.\n\n\n7.1.2 When The Mean Breaks Down\nIn a perfect data world, the mean is equal to the middle observation of the data: the median. That is only in the perfect world, though, and usually our data comes loaded with challenges. Extreme scores in your data will cause a rift between the median and the mean.\nLet’s say we take the integers between 1 and 10, and find the mean.\n\\[\\frac{1+2+3+4+5+6+7+8+9+10}{10} =  5.5\\]\nThe middle value in that vector of numbers would also be 5.5.\nWhat happens we replace the 1 with a more extreme value, like -10?\n\\[\\frac{-10+2+3+4+5+6+7+8+9+10}{10} =  4.5\\]\nWith just one dramatic change, our mean went down by a whole point. The median observation, though, is still 5.5. In short, the median is invariant to wild swings out in the tails of your numbers.\nYou might be saying to yourself, “Why should I care about this central tendency chicanery?” Let us tell you why you should care – the least squares approach to the standard linear model dictates that the regression line needs to be fit through the means of the variables. If you have extreme scores that influence the mean, then your regression line will also be influenced by those extreme scores.\nLet’s look at a few different regression lines:\n\n\n\n\n\nNow, what would happen if we replaced a few of our observations with extreme scores?\n\n\n\n\n\nWith just a casual glance, it doesn’t look like our two regression lines are that different. They both look like they have a similar positive slope, so all should be good. To offer a bit more clarity, though, let’s put those lines in the same space:\n\n\n\n\n\nWith 1000 observations, we see that having just 10 extreme scores is enough to change the regression line, even if just a little. There are a few approaches we could take here, with common approaches being dropping those observations or Windsorizing them. Throwing away data because you don’t like the way it behaves is nearing on statistical abuse and Windsorization is just replacing those extreme values with numbers that you like a little bit better.\nA better answer to this challenge might be to not fit the regression line through the mean, but the median instead. This is where quantile regression becomes handy. Formally, this model can be expressed as:\n\\[\nQ_{Y\\vert X}(\\tau) = X\\beta_\\tau\n\\]\nWhere we can find the estimation of \\(\\beta_\\tau\\) as:\n\\[\n\\hat\\beta_\\tau = \\arg \\min_{\\beta \\in \\mathbb{R}^k} \\sum_{i-1}^n(\\rho_\\tau(Y_i-X_i\\beta))\n\\]\nWith quantile regression, we are given an extra parameter for the model: \\(\\tau\\) or tau. The tau parameter let’s us choose which quantile we want to use for our line fitting. Since the median splits the data in half, we can translate that to a quantile of .5.\n\n\n7.1.3 Data Import and Preparation\nLet’s bring in our movie reviews data. Let’s say that we are curious about the relationship between the total_reviews variable and the rating variable. First, we need to get our data ready for our home-brewed functions.\n\nRPython\n\n\n\nreviews &lt;- read.csv(\"data/movie_reviews_processed.csv\")\n\nreviews &lt;- na.omit(reviews)\n\nX &lt;- reviews$total_reviews\nX &lt;- cbind(1, X)\ny &lt;- reviews$rating\n\n\n\n\nimport pandas as pd\nimport numpy as np\n\nreviews = pd.read_csv(\"data/movie_reviews_processed.csv\")\n\nreviews = reviews.dropna()\n\nX = pd.DataFrame(\n  {'intercept': 1, \n  'total_reviews': reviews['total_reviews']}\n)\ny = reviews['rating']\n\n\n\n\n\n\n7.1.4 Standard Functions\nWe can see how we can use quantreg and statsmodels to create a quantile regression. For both, we will start with a median regression; in other words, a quantile of .5.\n\nRPython\n\n\n\nlibrary(quantreg)\n\nmedian_test &lt;- rq(rating ~ total_reviews, tau = .5, \n                data = reviews)\n\nsummary(median_test)\n\n\nCall: rq(formula = rating ~ total_reviews, tau = 0.5, data = reviews)\n\ntau: [1] 0.5\n\nCoefficients:\n              coefficients lower bd upper bd\n(Intercept)   2.70278      2.53409  2.77655 \ntotal_reviews 0.00007      0.00006  0.00010 \n\n\n\n\n\nimport statsmodels.formula.api as smf\n\nmedian_test = smf.quantreg('rating ~ total_reviews', \n                           data = reviews).fit(q = .5)\n                           \nmedian_test.summary()                           \n\n\nQuantReg Regression Results\n\n\nDep. Variable:\nrating\nPseudo R-squared:\n0.05241\n\n\nModel:\nQuantReg\nBandwidth:\n0.3092\n\n\nMethod:\nLeast Squares\nSparsity:\n1.645\n\n\nDate:\nSat, 09 Dec 2023\nNo. Observations:\n1000\n\n\nTime:\n17:54:36\nDf Residuals:\n998\n\n\n\n\nDf Model:\n1\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n2.6929\n0.052\n51.706\n0.000\n2.591\n2.795\n\n\ntotal_reviews\n7.361e-05\n9.17e-06\n8.029\n0.000\n5.56e-05\n9.16e-05\n\n\n\nThe condition number is large, 1.14e+04. This might indicate that there arestrong multicollinearity or other numerical problems.\n\n\n\n\n\nFortunately, our interpretation of this result isn’t all that different from a standard linear model – the ratings should increase by .00007 for every additional review. However, this is at the median, not the mean, like the standard linear model.\nQuantile regression is not a one-trick-pony. Remember, it is called quantile regression – not median regression. Being able to compute a median regression is just a nice by-product. What we can do with quantile regression is to model different quantiles of the same data. It gives us the ability to answer brand new questions – does the relationship between user age and their ratings change at different quantiles of rating?\n\n\n\n\n\nQuantile regression lines\n\n\n\n\nInstead of a single model to capture the trend through the mean of the data, we can now examine the trends within 5 different quantiles of the data (we aren’t limited to just those quantiles, though, and you can examine any of them that you might find interesting). If we had to put some words to our visualization, we could say that all of the quantiles show a positive relationship. While they all appear to have roughly the same slope, it appears that the 90th quantile has a slightly steeper slope than the other quantiles, if only modestly so.\n\n\n7.1.5 Quantile Loss Function\nNow that we know how to use standard functions for quantile regression, let’s see one way that we can create a least squares loss function for fitting a linear regression model and compare it with a function for quantile loss.\n\nRPython\n\n\n\nleast_squares_loss &lt;- function(par, X, y) {\n  \n  linear_parameters &lt;- X %*% par\n  \n  mu &lt;- linear_parameters   \n  \n  loss &lt;- crossprod(y - mu)\n}\n\n\nquantile_loss &lt;- function(par, X, y, tau) {\n  \n  linear_parameters &lt;- X %*% par\n  \n  residual &lt;- y - linear_parameters\n  \n  loss &lt;- ifelse(residual &lt; 0 , \n                (-1 + tau)*residual, \n                tau*residual)\n  \n  sum(loss)\n}\n\n\n\n\ndef least_squares_loss(par, X, y):\n  linear_parameters = X.dot(par)\n  \n  mu = linear_parameters\n  \n  loss = np.dot(y - mu, y - mu)\n  \n  return loss\n\n\ndef quantile_loss(par, X, y, tau):\n  linear_parameters = X.dot(par)\n  \n  residual = y - linear_parameters\n  \n  loss = []\n\n  for i in residual:\n    if i &lt; 0: loss.append((-1 + tau)*i)\n    else: loss.append(tau*i)\n  \n  return sum(loss)\n\n\n\n\nYou’ll notice right away that we have a few differences. Our quantile loss function includes the tau argument, which will let us set our quantile of interest; naturally, it can be any value between 0 and 1. The residual is multiplied by the tau value, only if the residual is greater than 0. If the residual is negative, we need to add tau to -1. Since we need a positive value for our loss values, we will multiply our negative residuals by the negative value produced from -1 plus our tau value. After that, we just sum all of those positive loss values and do our best to minimize that summed value.\n\n\n7.1.6 Model Fitting\nNow that we have our data and our loss function, we can fit the model almost exactly like our standard linear model. Again, note the difference here with our tau value, which we’ve set to .5 to represent the median.\n\nRPython\n\n\n\noptim(\n  par = c(intercept = 0, total_reviews = 0),\n  fn  = quantile_loss,\n  X   = X,\n  y   = y,\n  tau = .5\n)$par\n\n    intercept total_reviews \n 2.702730e+00  7.258694e-05 \n\n\n\n\n\nfrom scipy.optimize import minimize\n\nminimize(\n  quantile_loss, \n  x0 = np.array([0, 0]), \n  args = (X, y, .5)\n  ).x\n\narray([2.70270449e+00, 7.26658864e-05])"
  },
  {
    "objectID": "linear_model_extensions.html#additive-models",
    "href": "linear_model_extensions.html#additive-models",
    "title": "7  Beyond the Basics",
    "section": "7.2 Additive Models",
    "text": "7.2 Additive Models\n\nWiggle, wiggle, wiggle, yeah! – LMFAO\n\n\n7.2.1 Why Should You Care?\nNot every relationship is linear and not every relationship is monotonic. Sometimes, you need to be able to model a relationship that has a fair amount of nonlinearity – they can appear as slight curves, waves, and any other type of wiggle that you can imagine. Additive models will give you the ability to model those nonlinear relationships between your features and target.\n\n\n7.2.2 When Straight Lines Aren’t Enough\nFitting a line through your data is always going to be useful, regardless of whether you are using the median or the mean. Those lines give us a wonderful ability to say important things about the relationships between variables and how one variable might influence another. What if we just want to dispense with the notion that we need to fit a straight line through some mass of the data? What if we relax the idea that we need a straight line and think in terms of fitting something curvy through the data.\nIn other words, we can go from the straight line in Figure 7.1:\n\n\n\n\n\nFigure 7.1: A standard linear model\n\n\n\n\nTo the curve seen in Figure 7.2:\n\n\n\n\n\nFigure 7.2: A generalized additive model\n\n\n\n\nThat curved line in Figure 7.2 is called a spline. Oddly enough, we can still use a linear model to fit this spline through the data. While this might not give us the same tidy explanation that a typical line would offer, we will certainly get better prediction.\nThese models belong to a broad group of generalized additive models, often shortened to GAMs. When we fit a quantile regression, we made a slight tweak to the y; to fit a GAM, we are going to tweak our predictors. How are we going to tweak them, you might ask? We are essentially going to let them be an additive function of features. We will have a model that looks like this:\n\\[\ny = f(x) + \\epsilon\n\\]\nThese additive features have no concern about linearity with the outcome and will capture nonlinearities in our data very nicely. At this point, you might be asking yourself, “Couldn’t I just use some type of polynomial regression or even a nonlinear regression?” Of course you could, but both have pretty serious limitations. The typical polynomial regression likely doesn’t fit beyond the data that you currently have and you are forcing curves to fit through the data. To use a nonlinear model, you need to know what the underlying nonlinear form actually looks like before you can even specify the model. A GAM will do away with both of these issues; it will produce a curve that will best fit through the data, without the need to know the underlying linear form.\n\n\n7.2.3 Standard Functions\nNow that you have some background, let’s use the mgcv package in R and the pygam package in Python to fit a GAM.\n\nRPython\n\n\n\nlibrary(mgcv)\n\ngam_model &lt;- gam(rating ~ s(total_reviews_sc, bs = \"cr\"), data = reviews)\n\nsummary(gam_model)\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\nrating ~ s(total_reviews_sc, bs = \"cr\")\n\nParametric coefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.05140    0.01862   163.9   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n                      edf Ref.df    F p-value    \ns(total_reviews_sc) 8.672  8.966 16.2  &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =  0.124   Deviance explained = 13.1%\nGCV = 0.34994  Scale est. = 0.34655   n = 1000\n\n\n\n\n\nfrom pygam import LinearGAM, s\n\ny = reviews['rating']\n\nX = pd.DataFrame(\n  {'intercept': 1, \n  'total_reviews': reviews['total_reviews']}\n)\n\ngam_model = LinearGAM(s(0)).fit(X, y)\n\ngam_model.summary()\n\nLinearGAM                                                                                                 \n=============================================== ==========================================================\nDistribution:                        NormalDist Effective DoF:                                         1.0\nLink Function:                     IdentityLink Log Likelihood:                                 -1254.3233\nNumber of Samples:                         1000 AIC:                                             2512.6465\n                                                AICc:                                            2512.6585\n                                                GCV:                                                0.3962\n                                                Scale:                                              0.3955\n                                                Pseudo R-Squared:                                      0.0\n==========================================================================================================\nFeature Function                  Lambda               Rank         EDoF         P &gt; x        Sig. Code   \n================================= ==================== ============ ============ ============ ============\ns(0)                              [0.6]                20           1.0          1.11e-16     ***         \nintercept                                              1            0.0          1.11e-16     ***         \n==========================================================================================================\nSignificance codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nWARNING: Fitting splines and a linear function to a feature introduces a model identifiability problem\n         which can cause p-values to appear significant when they are not.\n\nWARNING: p-values calculated in this manner behave correctly for un-penalized models or models with\n         known smoothing parameters, but when smoothing parameters have been estimated, the p-values\n         are typically lower than they should be, meaning that the tests reject the null too readily.\n\n\n\n\n\nFor the results of our gam model, one of the best places to look first is the edf column. The edf column is the effective degrees of freedom. You can think of edf as a measure of wiggle in the relationship between the predictor and the target. The higher the edf, the more wiggle you have. If you have a value close to 1, then you have a linear relationship. With an edf of 8.672, we can be pretty confident that a nonlinear relationship gives a better idea about the relationship between total_reviews and rating than a linear relationship.\nWe also get information about our Adjusted \\(R^2\\) (R-sq.(adj)) and Deviance explained, which is an analog to the unadjusted \\(R^2\\) value for a Gaussian model. We also have GCV – the generalized cross validation score. It is an estimate of the mean square prediction error based on a leave-one-out cross validation estimation process. Naturally, the lower the GCV, the better the model.\n\n\n7.2.4 Splines\nNow that you’ve gotten a taste of a standard way of specifying a GAM, let’s roll our own. We are going to need to generate several functions to make this work. The first will be to produce the cubic spline. Do take note that there are many different types of splines that could be used.\n\nRPython\n\n\n\ncubic_spline &lt;- function(x, z) {\n  ((z - 0.5)^2 - 1/12) * ((x - 0.5)^2 - 1/12)/4 -\n    ((abs(x - z) - 0.5)^4 - (abs(x - z) - 0.5)^2 / 2 + 7/240) / 24\n}\n\n\n\n\nimport numpy as np\n\ndef cubic_spline(x,z):\n    return (((z - 0.5)**2 - 1/12) * ((x - 0.5)**2 - 1/12)/4 -\n            ((np.abs(x - z) - 0.5)**4 - (np.abs(x - z) - 0.5)**2 / 2 + 7/240) / 24)\n\n\n\n\n\n\n7.2.5 Model Matrix Function\nThen we a function to produce the model matrix:\n\nRPython\n\n\n\nsplX &lt;- function(x, knots) {\n  q &lt;- length(knots) + 2        # number of parameters\n  n &lt;- length(x)                # number of observations\n  X &lt;- matrix(1, n, q)          # initialized model matrix\n  X[ ,2] &lt;- x                   # set second column to x\n  X[ ,3:q] &lt;- outer(x, knots, FUN = cubic_spline) \n  X\n}\n\n\n\n\ndef splX(x, knots):\n    q = len(knots) + 2\n    n = len(x)\n    X = np.ones((n, q))\n    X[:,1] = x\n    for i in range(2, q):\n        X[:,i] = cubic_spline(x, knots[i-2])\n    return X\n\nThis model matrix will help us to produce an unpenalized spline.\n\n\n\n\n\n7.2.6 Model Matrix\nWe can create a model with 4 knots – you can think of knots as places where individual regression lines will get joined together. You can always experiment with more or less knots. Once we have our knots ready, we can create the model matrix.\nAs soon as you create your X object, you should take a look at it. It will be a matrix with 4 columns. The first column will be all 1s, the second column will be the scaled user_age, and the last two columns will be the cubic splines.\n\nRPython\n\n\n\nknots &lt;- 1:4/5\n\n\nrating &lt;- reviews$rating\n\nx &lt;- reviews$total_reviews\n\nX &lt;- splX(x, knots)            \n\n\nhead(X)\n\n     [,1] [,2]          [,3]          [,4]          [,5]          [,6]\n[1,]    1 2574 -1.827050e+12 -1.826482e+12 -1.825914e+12 -1.825346e+12\n[2,]    1 7590 -1.382279e+14 -1.382133e+14 -1.381987e+14 -1.381842e+14\n[3,]    1 1618 -2.850697e+11 -2.849287e+11 -2.847878e+11 -2.846469e+11\n[4,]    1 6251 -6.359049e+13 -6.358236e+13 -6.357422e+13 -6.356608e+13\n[5,]    1 6040 -5.542876e+13 -5.542142e+13 -5.541408e+13 -5.540674e+13\n[6,]    1 7130 -1.076407e+14 -1.076286e+14 -1.076165e+14 -1.076044e+14\n\n\n\n\n\nknots = np.arange(1, 5) / 5\n\n\nx = reviews['total_reviews']\n\nrating = reviews['rating']\n\nX = splX(x, knots)\n\n\nX[:5,:]\n\narray([[ 1.00000000e+00,  2.57400000e+03, -1.82704987e+12,\n        -1.82648207e+12, -1.82591427e+12, -1.82534646e+12],\n       [ 1.00000000e+00,  7.59000000e+03, -1.38227877e+14,\n        -1.38213307e+14, -1.38198738e+14, -1.38184169e+14],\n       [ 1.00000000e+00,  1.61800000e+03, -2.85069671e+11,\n        -2.84928740e+11, -2.84787808e+11, -2.84646876e+11],\n       [ 1.00000000e+00,  6.25100000e+03, -6.35904948e+13,\n        -6.35823568e+13, -6.35742188e+13, -6.35660807e+13],\n       [ 1.00000000e+00,  6.04000000e+03, -5.54287604e+13,\n        -5.54214191e+13, -5.54140778e+13, -5.54067364e+13]])\n\n\n\n\n\n\n\n7.2.7 Model Fitting\nNow that we have a model matrix, X, we can fit the model. All of the hardwork was done in creating the model matrix and we can just use lm or OLS to fit the model.\n\nRPython\n\n\n\nfit_lm &lt;- lm(rating ~ X)\n\nfit_lm\n\n\nCall:\nlm(formula = rating ~ X)\n\nCoefficients:\n(Intercept)           X1           X2           X3           X4           X5           X6  \n  2.826e+00           NA   -8.055e-06   -9.670e-11    9.670e-11           NA           NA  \n\n\n\n\n\nimport statsmodels.api as sm\n\nfit_lm = sm.OLS(rating, X).fit()\n\nfit_lm.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nrating\nR-squared:\n-1.346\n\n\nModel:\nOLS\nAdj. R-squared:\n-1.351\n\n\nMethod:\nLeast Squares\nF-statistic:\n-286.0\n\n\nDate:\nSat, 09 Dec 2023\nProb (F-statistic):\n1.00\n\n\nTime:\n17:54:38\nLog-Likelihood:\n-1381.0\n\n\nNo. Observations:\n1000\nAIC:\n2768.\n\n\nDf Residuals:\n997\nBIC:\n2783.\n\n\nDf Model:\n2\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nconst\n5.439e-07\n1.07e-08\n51.027\n0.000\n5.23e-07\n5.65e-07\n\n\nx1\n0.0012\n2.42e-05\n51.027\n0.000\n0.001\n0.001\n\n\nx2\n1.186e-05\n2.32e-07\n51.025\n0.000\n1.14e-05\n1.23e-05\n\n\nx3\n-1.089e-05\n2.13e-07\n-51.027\n0.000\n-1.13e-05\n-1.05e-05\n\n\nx4\n-1.381e-05\n2.71e-07\n-51.026\n0.000\n-1.43e-05\n-1.33e-05\n\n\nx5\n1.284e-05\n2.52e-07\n51.028\n0.000\n1.23e-05\n1.33e-05\n\n\n\n\n\n\nOmnibus:\n64.602\nDurbin-Watson:\n1.992\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n75.828\n\n\nSkew:\n0.644\nProb(JB):\n3.42e-17\n\n\nKurtosis:\n3.404\nCond. No.\n1.27e+16\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 1.27e+16. This might indicate that there arestrong multicollinearity or other numerical problems.\n\n\n\n\n\n\n\n7.2.8 Prediction\nWe can set some prediction values for this model:\n\nRPython\n\n\n\nxp &lt;- seq(0, 1, by = .01)\nXp &lt;- splX(xp, knots)  \n\n\n\n\nxp = np.arange(0, 1, 0.01)\nXp = splX(xp, knots)\n\n\n\n\nWhile creating those predictions is nice, using them to visualize the model is far more helpful.\n\n\n\n\n\nFigure 7.3: Visualizing cubic regression spline\n\n\n\n\nIn Figure 7.3, we can see that the relationship starts a bit flat, increases, and then flattens out again. This is a pretty good example of a nonlinear relationship.\n\n\n7.2.9 Penalized Cubic Spline\nRecall that this is an unpenalized cubic spline. If we want to have a finer degree of control over that wiggly line, we can include a lambda penalty.\nWe’ll need to change up our spline function just a bit.\n\nRPython\n\n\n\nsplS &lt;- function(knots) {\n  q &lt;- length(knots) + 2\n  S &lt;- matrix(0, q, q) \n  S[3:q, 3:q] &lt;- outer(knots, knots, FUN = cubic_spline)\n  S\n}\n\n\n\n\ndef splS(knots):\n    q = len(knots) + 2\n    S = np.zeros((q, q))\n    S[2:, 2:] = cubic_spline(knots, knots[:,None])\n    return S\n\n\n\n\nWe also need to be able to take the square root of our entire matrix. This is a bit more complicated than it sounds. We need to take the eigenvalue decomposition of the matrix, take the square root of the eigenvalues, and then recombine the matrix.\n\nRPython\n\n\n\nmat_sqrt &lt;- function(S) {\n  d &lt;- eigen(S, symmetric = TRUE)\n  rS &lt;- d$vectors %*% diag(d$values^.5) %*% t(d$vectors)\n  rS\n}\n\n\n\n\ndef mat_sqrt(S):\n    w, v = np.linalg.eig(S)\n    rS = v @ np.diag(w**.5) @ v.T\n    return rS\n\n\n\n\n\n\n7.2.10 Penalized Model Fitting Function\nWith those functions in hand, we can create the function to fit the entire model.\n\nRPython\n\n\n\nprs_fit &lt;- function(y, x, knots, lambda) {\n  q  = length(knots) + 2    # dimension of basis\n  n  = length(x)            # number of observations\n  Xa = rbind(splX(x, knots), mat_sqrt(splS(knots))*sqrt(lambda)) # augmented model matrix\n  y[(n + 1):(n+q)] = 0      # augment the data vector\n  \n  lm(y ~ Xa - 1) # fit and return penalized regression spline\n}\n\n\n\n\ndef prs_fit(y, x, knots, lamba):\n    q = len(knots) + 2\n    n = len(x)\n    Xa = np.vstack(\n      (splX(x, knots), mat_sqrt(splS(knots))*np.sqrt(lamba))\n      )\n    y_add = np.zeros(q)\n    y = y.to_numpy()\n    y = np.concatenate((y, y_add), axis = 0)\n    return sm.OLS(y, Xa).fit()\n\n\n\n\nNotice again that magic happens in the model matrix, but that we are still just using lm or OLS to fit the model.\n\n\n7.2.11 Penalized Model Fitting\nLet’s stick with 4 knots and see what happens when we set our lambda to .1:\n\nRPython\n\n\n\nknots &lt;- 1:4/5\n\nfit_penalized &lt;- prs_fit(\n  y &lt;- rating,\n  x &lt;- x,\n  knots &lt;- knots,\n  lambda &lt;- .1\n) \n\nXp &lt;- splX(xp, knots) \n\n\n\n\nknots = np.arange(1, 5)/5\n\nfit_penalized = prs_fit(\n    y = y,\n    x = x,\n    knots = knots,\n    lamba = .1\n)\n\nXp = splX(xp, knots)\n\n\n\n\nAs shown in Figure 7.4, there is definitely some wiggle to that line, but it is not as extreme as what we saw with our unpenalized cubic spline.\n\n\n\n\n\nFigure 7.4: GAM model with lambda set to .1\n\n\n\n\nWe can test out what happens at different lambda values:\n\n\n\n\n\nFigure 7.5: GAM model with different lambda values\n\n\n\n\nWhat can we take from Figure 7.5? As lambda values get closer to 1, we see lines that look very similar to a standard linear model. If you recall the our function to fit the model, we multiplied the square root of the matrix by the square root of the lambda value; since the square root of 1 is 1, we wouldn’t see anything too interesting happen. As our lambda value gets lower, we see an increasing amount of wiggle happen.\nNaturally, this is a great time to think about how these models would work on new data. As lambda gets smaller, we are fitting our in-sample data much better. How do you think this would fare with unseen data? If you’d say that we would do well with training and horrible on testing, we’d likely agree with you."
  },
  {
    "objectID": "linear_model_extensions.html#mixed-models",
    "href": "linear_model_extensions.html#mixed-models",
    "title": "7  Beyond the Basics",
    "section": "7.3 Mixed Models",
    "text": "7.3 Mixed Models\n\n7.3.1 Why Should You Care?\nStructures within your data are important and paying attention to how data might be grouped in some way will let you generate more insights about how observations within and between groups might behave. After all, people working in the same department are likely to have a more similar experience than people working in a different department. Mixed models will let us handle nested and grouped data, all while getting the benefits of a standard linear model.\n\n\n7.3.2 Knowing Your Data\nAs much fun as modeling is, knowing your data is far more important. You can throw any model you want at your data, from simple to fancy, but you can count on disappointment if you don’t fundamentally know the structures that live within your data. Let’s take a quick look at the following visualizations:\n\n\n\n\n\nFigure 7.6: Linear relationship between year of movie release and rating.\n\n\n\n\nIn Figure 7.6, we see a pretty solid positive relationship between the total number of reviews and ratings. We could probably just stop there, but we might be ignoring something substantial within our data: genre. We might want to ask a question, “Does this relationship work the same way across the different genre?”\n\n\n\n\n\nFigure 7.7: Linear relationship between year of movie release and rating, with genre.\n\n\n\n\nA very quick examination of Figure 7.7 might suggest that the relationship between user age and rating varies significantly over the different genres. Most genres show a positive relationship, while one (other) shows a negative relationship, and all with various levels of strength.\nJust for fun, try to group by genre and summarize it by the mean rating. You’ll find that the averages are very different across the genre! Then, subtract the overal mean rating from those values. Keep them handy!\nClearly, genre is offering some type of additional information to the model, but how can we incorporate that information into our model? An interaction might come to find at first, but that becomes a tricky interpretation endeavour. Instead, the mixed model can be used to incorporate that information into our model, without much hassle and with a huge boost in explanability.\nThe term mixed model is as vanilla as we can possibly make it,but you might have heard of different flavors of them before. If you’ve come from the Social Sciences, you might have heard of Hierarchical Linear Models. You might have even heard the words multilevel models or mixed-effects models tossed around before. Maybe you’ve even be exposed to ideas like random effects or random slopes. No matter what word you say, they are all instances of a mixed model.\nWhat makes a model a mixed model? The mixed model is characterized by the idea that a model can have fixed and random effects. Fortunately, you’ve already encountered fixed effects – those are the features that we have been using in all of our models so far! The random effect, though, is typically some type of grouping-based variable that contributes to unique variance in the outcome and we are going to let them vary from the rest of the model. Grouping variables can be actual groups, with the classic example being students in a classroom. Those groups can also be nested within larger groups – students nested within classrooms, nested within schools, nested within districts. These groups can also capture the notion of a repeated measure for an individual, like repeated test scores.\nWhile we aren’t rejecting the idea of a mean with these mixed-models, we are implying that each group (whether it is a group, nested group, or repeated observations from a person) has its own unique mean that can be useful for modeling the target. Formally, we might specify something like this:\n\\[\n\\textrm{rating} = b_{\\textrm{0\\_genre}} + b_\\textrm{year}*\\textrm{year}\n\\]\nWe are explicitly saying that genre has its own unique slope for this model, where \\(b_{\\text{0\\_genre}} ~ \\eta(b_{intercept}, \\tau)\\), meaning that the random intercepts will be normally distributed and the overall intercept is just the mean of those random intercepts.\nBefore we go to modeling our reviews, let’s consider an example training program to increase vertical jump, with average vertical increases of 2 inches. That really doesn’t sound all that impressive; however, that increase came across 5 distinct groups: NBA players, NFL players, NHL players, MLB players, and data analysts. We can be pretty certain that each group has a very different vertical jump distribution coming into this training program. Given the amount of jumping that NBA players do, this program is unlikely to produce dramatic increases in vertical jump for them. We would probably expect modest gains in the NFL and even greater gains within NHL and MLB players. Where we are going to see the best gains, though, is from data analysts – they might want to jump to conclusions, but don’t need to jump over their computers very often. Now that we know the additional information, can we just look at the average increase and be satisfied? Probably not. Instead, we need to look at the group that we might be in and judge accordingly. A mixed-model is going to let us to model all of this without too much work.\n\n\n7.3.3 Standard Functions\nLet’s fit some mixed models with lme4 or statsmodels. We will also create null models (i.e., models with an intercept only), since we will need some infomration from them later.\n\nRPython\n\n\n\nlibrary(lme4)\n\nfit_mer = lmer(rating ~ total_reviews_sc + (1 | genre), \n               reviews, \n               REML = FALSE)\n\nsummary(fit_mer)\n\nLinear mixed model fit by maximum likelihood  ['lmerMod']\nFormula: rating ~ total_reviews_sc + (1 | genre)\n   Data: reviews\n\n     AIC      BIC   logLik deviance df.resid \n  1506.6   1526.2   -749.3   1498.6      996 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.0992 -0.6840  0.0465  0.7187  3.1890 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n genre    (Intercept) 0.08309  0.2882  \n Residual             0.25482  0.5048  \nNumber of obs: 1000, groups:  genre, 8\n\nFixed effects:\n                 Estimate Std. Error t value\n(Intercept)        2.9782     0.1038   28.69\ntotal_reviews_sc   0.2479     0.0164   15.12\n\nCorrelation of Fixed Effects:\n            (Intr)\nttl_rvws_sc -0.005\n\nnull_model &lt;- lmer(rating ~ 1 + (1 | genre), \n                   reviews, \n                   REML = FALSE)\n\nsummary(null_model)                  \n\nLinear mixed model fit by maximum likelihood  ['lmerMod']\nFormula: rating ~ 1 + (1 | genre)\n   Data: reviews\n\n     AIC      BIC   logLik deviance df.resid \n  1710.2   1724.9   -852.1   1704.2      997 \n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-3.08192 -0.76091 -0.04675  0.66740  2.98841 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n genre    (Intercept) 0.07557  0.2749  \n Residual             0.31371  0.5601  \nNumber of obs: 1000, groups:  genre, 8\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  2.98593    0.09962   29.98\n\n\n\n\n\nimport statsmodels.api as sm\n\nfit_mer = sm.MixedLM.from_formula(\"rating ~ total_reviews_sc\", reviews, \n                                  groups=reviews[\"genre\"])\n\nfit_mer = fit_mer.fit()\n\nfit_mer.summary()\n\n\n\n\nModel:\nMixedLM\nDependent Variable:\nrating\n\n\nNo. Observations:\n1000\nMethod:\nREML\n\n\nNo. Groups:\n8\nScale:\n0.2551\n\n\nMin. group size:\n49\nLog-Likelihood:\n-753.8114\n\n\nMax. group size:\n300\nConverged:\nYes\n\n\nMean group size:\n125.0\n\n\n\n\n\n\n\n\n\nCoef.\nStd.Err.\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n2.978\n0.111\n26.863\n0.000\n2.761\n3.195\n\n\ntotal_reviews_sc\n0.248\n0.016\n15.114\n0.000\n0.216\n0.280\n\n\nGroup Var\n0.095\n0.104\n\n\n\n\n\n\n\n\n\n\n\nnull_model = sm.MixedLM.from_formula(\"rating ~ 1\", reviews, \n                                     groups=reviews[\"genre\"])\n\n\n\n\nWhat can we take from these summaries and what are we getting beyond the linear model?\nFor starters, we should notice a change in our standard errors – by integrating information about the groups, we are getting a better sense of how much uncertainty our model contains at the global average level.\nWe also see some additional information for our random effects. The standard deviation is telling us how much the rating moves around based upon genre after getting the information from our fixed effects (i.e., the rating can move around nearly .3 points from genre alone).\nWe can use information for the random effects components to get some additional information. For instance, we can calculate the proportion of variance in scores that is accounted for by the genre alone:\n\\[\\frac{\\text{intercept variance}}{\\text{intercept variance} + \\text{residual variance}}\\]\nWith our values, we have:\n\\[\n.08309 / (.08309 + .25482) = 0.2458939\n\\]\nThis is what is known as the intraclass correlation (ICC). It ranges from 0 (no variance between clusters) to 1 (variance between clusters). With an ICC of .25, we can say that 25% of the variance in scores is accounted for by the genre alone.\nWe can also use various bits of information in our output to create different R^2 values.\nFor the first level of the model, we can calculate the \\(R^2\\) as:\n\\[R^2_11 - \\frac{\\sigma^2_{M1} + \\tau^2_{M1}}{\\sigma^2_{M0} + \\tau^2_{M0}}\\]\nWe can pull that information from our two model outputs:\n\nm1Sigma = .08309 # full model random effect intercept\n\nm1Tau = .25482 # full model random effect residual\n\nm0Sigma = .07557 # null model random effect intercept\n\nm0Tau = .31371 # null model random effect residual\n\n1 - ((m1Sigma + m1Tau) / (m0Sigma + m0Tau))\n\n[1] 0.1319616\n\n\nThe fixed effect of number of reviews accounts for nearly 13% of the variation within the reviews.\nThe second level can be calculated as:\n\\[R^2_2 = 1 - \\frac{\\sigma^2_{M1} / B + \\tau^2_{M1}}{\\sigma^2_{M0} / B + \\tau^2_{M0}}\\]\nWe see that we added a B into the mix here and it is just the average size of the level 2 units (1000 observations / 8 genres).\n\nlevel2Mean = 1000 / 8\n\nr2Numerator = m1Sigma / level2Mean + m1Tau\n\nr2Denominator = m0Sigma / level2Mean + m0Tau\n\n1 - (r2Numerator / r2Denominator)\n\n[1] 0.1871687\n\n\nWhich gives us .18 or 18% of the variation in scores is accounted for by the genre alone.\nYou can also get these values in R like this:\n\nperformance::r2(fit_mer)\n\n# R2 for Mixed Models\n\n  Conditional R2: 0.362\n     Marginal R2: 0.154\n\nMuMIn::r.squaredGLMM(fit_mer)\n\n           R2m       R2c\n[1,] 0.1539124 0.3619545\n\n\nHere we have two values: the marginal R2 (R2m) and the conditional R2 (R2c). You can think of the marginal values as the standard type of R2 – it is the variability explained by the fixed effects part of the model (it is what we have already done above). The conditional R2 is using both fixed and random effects, so you can think of it as the toal variability explained. Clearly, we can subtract the two to get a better understanding of the effect of the random effects. With \\(.362 - .154 = .208\\), we can say that the random effects account for 20.8% of the variation in ratings.\nNotice that those values are a bit different than what we produced by hand. Packages are now using a revised method proposed by Nakagawa, Johnson, and Schielzeth (2017) that is a bit more accurate than the previous method.\nWe can also get a good sense of the random effect estimates:\n\n\n\n\n\nFigure 7.8: Random effects estimates\n\n\n\n\nFor Figure 7.8, the easiest way to think about it is that the values are effects for each individual random effect (i.e., each genre’s random intercept). Since we are just dealing with intercepts right now, they are the deviations from the fixed intercept. The intercept for the model is ~2.6 and the random effect for comedy is .48. If we wanted to predict scores for a comedy movie, we would take 2.6 + .48 for the intercept portion of the model (the same would go for any random slopes in the model). In the end, it is showing how much the intercept shifts from genre to genre, and some genres have a positive effect beyond the average and others have a negative effect (sci-fi, for instance, is well below the global average).\nRemember your group-by and summarize task earlier? Each point is the difference between a genre’s average and the overall average – values in blue are higher than the average and values in red are lower than the average. With that, the average rating for comedy is much better than the global average rating, while sci-fi is much worse than the global average rating.\n\n\n7.3.4 Model Matrix\nLet’s start our homebrewing adventures by creating a model matrix. We are going to use the model.matrix() function to create our model matrix. We are going to use the user_age variable as our fixed effect and genre as our random effect. We are going to use the factor() function to make sure that genre is treated as a categorical variable. We are also going to use the -1 to remove the intercept from the model matrix.\n\nRPython\n\n\n\nX &lt;- model.matrix(~total_reviews_sc, reviews)\nZ &lt;- model.matrix(~factor(reviews$genre) - 1)\n\ncolnames(Z) &lt;- paste0(\"released_\", sort(unique(reviews$genre)))\n\ny &lt;- reviews$rating\n\n\n\n\nX = reviews[['total_reviews_sc']]\nX = sm.add_constant(X)\nX = X.to_numpy()\n\nZ = pd.get_dummies(reviews['genre'], drop_first=True)\nZ = Z.to_numpy()\n\ny = reviews['rating']\ny = y.to_numpy()\n\n\n\n\n\n\n7.3.5 Likelihood Function\n\nRPython\n\n\n\nmixed_log_likelihood &lt;- function(y, X, Z, theta) {\n  tau   = exp(theta[1])\n  sigma = exp(theta[2])\n  n = length(y)\n  \n  # evaluate covariance matrix for y\n  e  = tcrossprod(Z)*tau^2 + diag(n)*sigma^2\n  b  = coef(lm.fit(X, y))\n  mu = X %*% b\n\n  ll = mvtnorm::dmvnorm(y, mu, e, log = TRUE)\n  -ll\n}\n\n\n\n\nfrom scipy import stats\n\ndef mixed_log_likelihood(theta, y, X, Z):\n    tau = np.exp(theta[0])\n    sigma = np.exp(theta[1])\n    n = len(y)\n    \n    e = (Z.dot(Z.T) * tau**2) + (np.eye(n) * sigma**2)\n    b = np.linalg.lstsq(X, y, rcond=None)[0]\n    mu = X.dot(b) \n    \n    ll = stats.multivariate_normal.logpdf(y, mu, e)\n    return -ll\n\n\n\n\n\n\n7.3.6 Model Fitting\n\nRPython\n\n\n\nparam_init &lt;- c(0, 0)\n\nnames(param_init) &lt;- c('tau', 'sigma')\n\nfit &lt;- optim(\n  fn  = mixed_log_likelihood,\n  X   = X,\n  y   = y,\n  Z   = Z,\n  par = param_init,\n  control = list(reltol = 1e-10)\n)\n\nfit$par\n\n       tau      sigma \n-1.2251586 -0.6802856 \n\n\n\n\n\nfrom scipy import optimize as opt\n\ntheta = np.array([0, 0])\n\nmixed_log_likelihood(theta, y, X, Z)\n\n1072.3634783404586\n\nfit = opt.minimize(\n    fun=mixed_log_likelihood,\n    x0=theta,\n    args=(y, X, Z),\n    tol=1e-10\n)\n\nfit.x\n\narray([-1.21383464, -0.64168973])"
  },
  {
    "objectID": "linear_model_extensions.html#performance-comparisons",
    "href": "linear_model_extensions.html#performance-comparisons",
    "title": "7  Beyond the Basics",
    "section": "7.4 Performance Comparisons",
    "text": "7.4 Performance Comparisons\nJust for giggles, we should see how all of our models perform:\n\n\n\n\n\n\n\n\n\nmodel\nrmse\n\n\n\n\nstandard\n0.5937297\n\n\nmedian\n0.5937922\n\n\ngam\n0.5858336\n\n\nmixed\n0.5028455\n\n\n\n\n\nFigure 7.9: Comparing model performance with RMSE\n\n\n\nLet’s check out the results in Figure 7.9. Unsurprisingly, the standard linear model and the median regression were pretty close to each other. GAM offered a small bump in performance, but our best model came from the mixed model. This finding may or may not surprise you – as you spend more time with models, you often encounter situations where simple models outperform more complex models, or are on par with them. Here, we are seeing that the mixed model is offering us a better fit to the data than the other models. However, that doesn’t mean that you can just go right to the mixed model. You need to know your data and know what you are trying to accomplish."
  },
  {
    "objectID": "linear_model_extensions.html#wrapping-up",
    "href": "linear_model_extensions.html#wrapping-up",
    "title": "7  Beyond the Basics",
    "section": "7.5 Wrapping Up",
    "text": "7.5 Wrapping Up\nThe standard linear model is useful across many different data situations. It does, unfortunately, have some issues when data becomes a little bit more “real”. When you have extreme scores or relationships that a standard model might miss, you don’t need to abandon your linear model in favor of something more exotic. Instead, you might just need to think about how you are actually fitting the line through your data."
  },
  {
    "objectID": "linear_model_extensions.html#additional-resources",
    "href": "linear_model_extensions.html#additional-resources",
    "title": "7  Beyond the Basics",
    "section": "7.6 Additional Resources",
    "text": "7.6 Additional Resources\nNo matter how much we cover in this book, there is always more to learn. Here are some additional resources that you might find helpful.\nIf you want absolute depth on quantile regression, we will happily point you to the OG of quantile regression, Roger Koenker. His book, Quantile Regression is a must read for anyone wanting to dive deeper into quantile regression. If you don’t want to spring for the book, you might want to check out his 2005 article, Galton, Edgeworth, Frisch, and prospects for quantile regression in econometrics. You can find it at https://www.econometricsociety.org/publications/econometrica/2005/03/01/galton-edgeworth-frisch-and-prospects-quantile-regression.\nIf you want to dive more into the GAM world, we would recommend that you start with the Moving Beyond Linearity chapter in An Introduction to Statistical Learning (James, Witten, Hastie, & Tibshirani). Not only do they have versions for both R and Python, but both have been made available online at https://www.statlearning.com/. If you are wanting more after that, you can’t beat Simon Wood’s book, Generalized Additive Models: An Introduction with R.\nThere is no shortage of great references for mixed effects models. If you are looking for a great introduction to mixed models, we would recommend to start with the tutorial by one of your fearless authors! Michael Clark’s Mixed Models with R is a great introduction to mixed models and is available at https://m-clark.github.io/mixed-models-with-R/. If you want to dig just a little deeper, the lme4 vignette for Fitting Linear Mixed-Effects Models Using lme4 is a great resource. You can find it at https://cran.r-project.org/web/packages/lme4/vignettes/lmer.pdf."
  },
  {
    "objectID": "causal.html#key-ideas",
    "href": "causal.html#key-ideas",
    "title": "12  Causal Modeling",
    "section": "12.1 Key ideas",
    "text": "12.1 Key ideas\n\nNo model can tell you whether a relationship is causal or not. Causality is inferred, not proven, based on the available evidence.\nThe exact same models would be used for similar data settings to answer a causal question, or a predictive question. The difference is in the interpretation of the results.\nExperimental design, such as randomized control trials, are the gold standard for causal inference. But in this case, the gold standard is often not practical, and not without its shortcomings even when it is, and never perfectly implemented. More like a silver standard?\nCausal inference is often done with observational data, which is often the only option, and that’s okay.\nSeveral models exist which are typically employed to answer a more causal-oriented question. These include structural equation models, graphical models, uplift modeling, and more.\nInteractions are the norm, if not the reality. Causal inference generally regards a single effect. If the normal setting is that such an effect would always vary depending on other features, you should question why you want to aggregate your results to a single ‘effect’, since that effect would be potentially misleading."
  },
  {
    "objectID": "causal.html#why-it-matters",
    "href": "causal.html#why-it-matters",
    "title": "12  Causal Modeling",
    "section": "12.2 Why it matters",
    "text": "12.2 Why it matters\nOften we need a precise statement about the feature-target relationship, not just whether there is some relationship. For example, we might want to know whether a drug works well, or whether showing an advertisement results in a certain amount of new sales. Whether or not random assignment was used, we generally need to know whether the effect is real, and the size of the effect, and often, the uncertainty in that estimate. Causal modeling is, like machine learning, more of an approach than a specific model, and that approach may involve the design or implementing models we’ve already seen in a different way to answer the key question. Without more precision in our understanding, we could miss the effect, or overstate it, and make bad decisions as a result.\n\n12.2.1 Good to know\nHonestly this section is pretty high level, and we are not going to go into much detail here so even just some understanding of correlation and modeling would likely be enough."
  },
  {
    "objectID": "causal.html#classic-experimental-design",
    "href": "causal.html#classic-experimental-design",
    "title": "12  Causal Modeling",
    "section": "12.3 Classic Experimental Design",
    "text": "12.3 Classic Experimental Design\nMany of those who have taken a statistics course have been exposed to the simple t-test to determine whether two groups are different. While it can be applied to any binary group setting, for our purposes here we can assume the two groups result from some sort of treatment that is applied to one group, and not the other. The ‘treatment’ could regard a new drug, demographic groups, a marketing campaign, a new app’s feature, or anything else.\nThis is a very simple example of an experimental design, and it is a very powerful one. Ideally, we would randomly assign our observational units to the two groups, one which gets the treatment and one which doesn’t.Then we’d measure the difference between the two groups, using some metric to conclude that the two groups are different. This is the basic idea behind the t-test, which would compare the target means of the two groups.\nThe t-test tells us whether the difference in means between the two groups is statistically significant. It definitely does not tell us whether the treatment itself caused the difference, whether the effect is large, nor whether the effect is real, or even if the treatment is a good idea to do in the first place. It just tells us whether the two groups are statistically different.\nTurns out, a t-test is just a linear regression. It’s a special case of linear regression where there is only one independent variable, and it is a categorical variable with two levels. The coefficient from the linear regression would tell you the mean difference, i.e. as you go from one group to the other, how much does the target mean change? The t-test is just a special case of this, but under the same conditions, the t-statistic from the linear regression and t-test, and corresponding p-value would be identical.\nAnalysis of variance, or **ANOVA, allows the t-test to be extended to more than two groups, and multiple features, and is also commonly employed to analyze the results of experimental design settings. But ANOVA is still just a linear regression. Even when we get into more complicated design settings such as repeated measures and mixed design, it’s still just a linear regression, we’d just be using mixed models. TODO: LINK TO MIXED MODELS\nIf using a linear regression didn’t suggest any notion of causality to you before, it certainly shouldn’t now either. The model is identical whether there was an experimental design with random assignment or not. The only difference is that the data was collected in a different way, and the theoretical assumptions and motivations are different. Experimental design can give us more confidence in the causal explanation of model results, whatever model is used, and this is why we like to use random assignment when we can. It gives us control for the unobserved factors that might otherwise be influencing the results. If we can be fairly certain the observations are essentially the same except for the treatment, then we can be more confident that the treatment is the cause of the difference, and we can be more confident in the causal interpretation of the results. But it doesn’t change the model itself, and the results of a model do not in any way prove a causal relationship.\n\n\n\n\n\n\nA/B testing is just marketing-speak for a two-group setting where one could employ the same mindset they would if they were doing a t-test. It implies randomized assignment, but you’d have to know the context to know if that is actually the case."
  },
  {
    "objectID": "causal.html#natural-experiments",
    "href": "causal.html#natural-experiments",
    "title": "12  Causal Modeling",
    "section": "12.4 Natural Experiments",
    "text": "12.4 Natural Experiments\nAs we noted, random assignment or a formal experiment is not always possible or practical to implement. But sometimes we get to do it anyway, or at least close enough! Sometimes, the world gives us a natural experiment, where the assignment to the groups is essentially random, or where there is clear break before and after some event occurs, such that we examine the change as we would in pre-post design.\nFor example, a certain recent pandemic allowed us to examine vaccination effects, policy effects, remote work, and more. This was not a tightly controlled experiment, but it’s something we can treat very similar to an experiment, and we can compare the differences in various outcomes before and after the pandemic to see what changes took place."
  },
  {
    "objectID": "causal.html#causal-inference",
    "href": "causal.html#causal-inference",
    "title": "12  Causal Modeling",
    "section": "12.5 Causal Inference",
    "text": "12.5 Causal Inference\nReasoning about causality is a very old topic, philosophically dating back millenia, and more formally hundreds of years. Random assignment is a relatively new idea, say 150 years old, but was even posited before Wright, Fisher, and Neyman Pearson and the 20th century rise of statistics. But with stats and random assignment we had a way to start using models to help us reason about causal relationships. Pearl and others came along to provide a perspective from computer science, and things have been progressing along. We were actually using programming approaches to do causal inference before back in the 1970s even! Economists eventually got into the game too (e.g., Heckman), though largely reinventing the wheel.\nNow we can use recently developed modeling approaches to help us reason about causal relationships, which can be both a blessing and a curse. Our models can be more complex, and we can use more data, which can potentially give us more confidence in our conclusions. But we can still be easily fooled by our models, as well as by ourselves. So we’ll need to be careful in how we go about things, but let’s see what some of our options are!"
  },
  {
    "objectID": "causal.html#models-for-causal-inference",
    "href": "causal.html#models-for-causal-inference",
    "title": "12  Causal Modeling",
    "section": "12.6 Models for Causal Inference",
    "text": "12.6 Models for Causal Inference\n\n12.6.1 Linear Regression\nYep, linear regression. Your old #1 is quite possibly the mostly widely used model for causal inference, historically speaking. We’ve even already seen linear regression as a graphical model Figure 3.2, and in that sense can serve as the starting point for structural equation models and related, or be used as a baseline for other approaches. Linear regression also tells us for any particular effect, what that effect is, accounting for all the other features constant, which kind of already gets into a causal mindset.\nHowever, your standard linear model doesn’t care where the data came from and will tell you about group differences whether they come from a randomized experiment or not. And if you don’t include features that would have a say in the treatment, you’ll potentially get a biased estimate of the effect. As such, linear regression by itself cannot save us from the difficulties of causal inference. But it can be used in conjunction with other approaches, and it can be used to help us reason about causal relationships. For example, we can use it to help us understand the effect of a treatment, or to help us understand the effect of a feature on the target, accounting for other features.\n\n\n12.6.2 Structural Equation Models\nTODO: LINK LATENT VARIABLES\nStructural Equation Models are basically multivariate models, as in multiple targets, used for regression and classification. They are widely employed in the social sciences, and are often used to model both observed and latent variables, with either serving as features or targets. They are also used to model causal relationships, to the point that historically they were called causal graphical models or causal structural models, and are a special case of graphical models more generally speaking. They have one of the longest histories of formal statistical modeling dating back over a century1. Economists later reinvented the approach under various guises, and computer scientists joined the party after that.\nUnfortunately for those looking for causal effects, the basic input for SEM is a correlation matrix, and the basic output is a correlation matrix. Insert your favorite modeling quote here - you know which one. Also, a linear regression and even deep-learning models like autoencoders can be depicted as graphical models, as we have seen. The point is that SEM, like linear regression, can no more tell you whether a relationship is causal than the linear regression, or for that matter, the t-test, could.\n\n\n12.6.3 Counterfactual Thinking\nWhen we think about causality, we really out to think about counterfactuals. What would have happened if I had done something different? What would have happened if I had not done something? What would have happened if I had done something sooner rather than later? What would have happened if I had done nothing at all? These questions are all examples of counterfactual thinking. And this is one of the best ideas to take aways from this…\n\nthe question is not whether there is a difference between A and B but whether there would still be a difference if A was B and B was A.\n\nThis is the essence of counterfactual thinking. It’s not about whether there is a difference between two groups, but whether there would still be a difference if those in one group had actually been treated differently. In this sense, we are concerned with the potential outcomes of the treatment, however defined.\nHere is a more concrete example:\n\nRoy is shown ad A, and buys the product.\nPris is shown ad B, and does not buy the product.\n\nWhat are we to make of this? Which ad is better? A seems to be, but maybe Pris wouldn’t have bought the product if shown that ad either, and maybe Roy would have bought the product if shown ad B too! With counterfactual thinking, we are concerned with the potential outcomes of the treatment, which in this case is whether or not to show the ad.\nLet’s say ad A is the new one, i.e., our treatment group, and B is the status quo ad, our control group. Our real question can’t be answered by a simple test of whether means or predictions are different among the two groups, as this estimate would be biased if the groups are already different in the first place. The real effect is whether, for those who saw ad A, what the difference in the target would be if they hadn’t seen it.\nFrom a prediction stand point, we can get an estimate straightforwardly. For those in the treatment, we can just plug in their feature values with treatment set to ad A. Then we just make a prediction with treatment set to ad B.\n\nPythonR\n\n\n\nmodel.predict(X.assign(treatment = 'A')) - \n    model.predict(X.assign(treatment = 'B'))\n\n\n\n\npredict(model, X |&gt; mutate(treatment = 'A')) - \n    predict(model, X |&gt; mutate(treatment = 'B'))\n\n\n\n\nWith counterfactual thinking explicitly in mind, we can see that the difference in predictions is the difference in the potential outcomes of the treatment.\n\n\n12.6.4 Uplift Modeling\nThe counterfactual prediction we just did can be called the uplift or gain from the treatment. Uplift modeling is a general term applied to models where counterfactual thinking is at the forefront, especially in a marketing context. Uplift modeling is not a model, but any model that is used to answer a question about the potential outcomes of a treatment. The key question is what is the gain, or uplift, in applying a treatment vs. not? Typically any statistical model can be used to answer this question, and often the model is a classification model, whether Roy both the product or not.\nSome in uplift modeling distinguish:\n\nSure things: those who would buy the product whether or not shown the ad.\nLost causes: those who would not buy the product whether or not shown the ad.\nSleeping dogs: those who would buy the product if not shown the ad, but not if shown the ad. ‘Do not disturb’!\nPersuadables: those who would buy the product if shown the ad, but not if not shown the ad.\n\nOne of additional goals in uplift modeling is to identify persuadables for additional marketing efforts, and to avoid wasting money on the lost causes. But to get there, we have to think causally first!\n\n\n\n\n\n\nThere appear to be more widely used tools for uplift modeling and meta-learners in Python than in R, but there are some options in R as well. In Python you can check out causalml and sci-kit uplift for some nice tutorials and documentation.\n\n\n\n\n\n12.6.5 Meta-Learning\nMeta-learners are used in uplift modeling and other contexts to assess potentially causal relationships between some treatment and outcome.\n\nS-learner - single model for both groups; predict the difference as when all observations are treated vs when all are not, similar to our code demo above.\nT-learner - two models, one for each treatment group; predict the difference as when all observations are treated vs when all are not for both models, and take the difference\nX-learner - a more complicated modification to the T-learner also using a multi-step approach.\nMisc-learner - other meta-learners that are not as popular, but might be applicable for your problem.\nTransformed outcome: transform your uplift modeling into a regression problem in which the prediction is the difference in the potential outcomes. This simplifies the problem to a single model, and can be quite effective.\n\n\n\n\n\n\n\nMeta-learners are not to be confused with meta-analysis, which is also related to understanding causal effects. Meta-analysis attempts to combine the results of multiple studies to get a better estimate of the true effect. The studies are typically conducted by different researchers and in different settings. Meta-learning has also been used to refer to what is more commonly called ensemble learning.\n\n\n\n\n\n12.6.6 Others approaches\nNote that there are many model approaches that would fall under the umbrella of causal inference, and several that are discipline specific but really only a special application of some of the ones we’ve mentioned here. A few you might come across:\n\nMarginal structural models\nInstrumental variables and two-stage least squares\nPropensity score matching/weighting\nMeta-analysis\nBayesian networks"
  },
  {
    "objectID": "causal.html#commentary",
    "href": "causal.html#commentary",
    "title": "12  Causal Modeling",
    "section": "12.7 Commentary",
    "text": "12.7 Commentary\nYou will often hear people speak very strongly about causality in the context of modeling, and those who assume that employing an experimental design solves every problem. But anyone who has actually conducted experiments knows that the implementation is never perfect, often not even close, especially when humans are involved as participants or as experimenters. Experimental design is hard, and if done well, can be very potent, but by itself does not prove anything regarding causality. You will also hear people say that you cannot infer causality from observational data, but it’s done all the time, and it’s often the only option.\nIn the end, the main thing is that when we want to make causal statements, we’ll make do with what data setting we have, and be careful that we rule out some of the other obvious explanations and issues. The better we can control the setting, or the better we can do things from a model standpoint, the more confident we can be in making causal claims. Causal modeling is an exercise in reasoning, which makes it such an interesting endeavor."
  },
  {
    "objectID": "causal.html#where-to-go-from-here",
    "href": "causal.html#where-to-go-from-here",
    "title": "12  Causal Modeling",
    "section": "12.8 Where to go from here",
    "text": "12.8 Where to go from here\nWe have only scratched the surface here, and there is a lot more to learn. Here are some resources to get you started:\n\nBarrett, McGowan, and Gerke (2023)\nCunningham (2023)\nFacure Alves (2022)\n\n\n\n\n\nBarrett, Malcolm, Lucy D’Agostino McGowan, and Travis Gerke. 2023. Causal Inference in R. https://www.r-causal.org/.\n\n\nCunningham, Scott. 2023. Causal Inference The Mixtape. https://mixtape.scunning.com/.\n\n\nFacure Alves, Matheus. 2022. “Causal Inference for The Brave and True — Causal Inference for the Brave and True.” https://matheusfacure.github.io/python-causality-handbook/landing-page.html."
  },
  {
    "objectID": "misc_models.html",
    "href": "misc_models.html",
    "title": "13  Misc Models",
    "section": "",
    "text": "PLACEHOLDER"
  }
]