[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Models",
    "section": "",
    "text": "Preface\nHi there, this is our book!"
  },
  {
    "objectID": "index.html#acknowledgments",
    "href": "index.html#acknowledgments",
    "title": "Models",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nA lot of people helped when writing the book, like Gordon Ramsey, zefrank1, and the guy who invented the wheel."
  },
  {
    "objectID": "introduction.html#what-is-this-book",
    "href": "introduction.html#what-is-this-book",
    "title": "1  Introduction",
    "section": "1.1 What Is This Book?",
    "text": "1.1 What Is This Book?\nThis book is a practical resource, something we hope you can refer to for a quick overview of a specific modeling technique, a reminder of something you’ve seen before, or perhaps a sneak peak into some modeling details. The text is focused on a few statistical and machine learning approaches that are widely employed, and specifically those which form the basis for most other models in use. Believe it or not, whether a lowly t-test or a complex neural network, there is a tie that binds. We hope to help you understand some of the core modeling principles, and how the simpler models can be extended and applied to a wide variety of data scenarios.\nOur approach here is first and foremost a practical one, as models themselves are just tools to help us reach a goal. If a model doesn’t work in the world, it’s not very useful. But modeling is often a delicate balance of interpretation and prediction, and each data situation is unique in some way, requiring a bespoke approach. What works well in one setting may be poor in another, and what may be the state of the art may only be marginally better than a notably simpler approach that is far more interpretable. In addition, complexities arise even in an otherwise deceptively simple application. However, if you have the core understanding of the techniques that lie at the heart of many models, you’ll automatically have many more tools at your disposal to tackle the problems you face, and be more comfortable with choosing the best for your needs."
  },
  {
    "objectID": "introduction.html#who-should-use-this-book",
    "href": "introduction.html#who-should-use-this-book",
    "title": "1  Introduction",
    "section": "1.2 Who Should Use This Book?",
    "text": "1.2 Who Should Use This Book?\nThis book is intended for every type of data dabbler, no matter what part of the data world you call home. If you consider yourself a data scientist, a business analyst, or a statistical hobbyist, you already know that the best part of a good dive into data is the modeling. But whatever your data persuasion, models give us the possibility to answer questions, make predictions, and understand what we’re interested in a little bit better. And no matter who you are, it isn’t always easy to understand how the models work. Even when you do get a good grasp of a modeling approach, it can still get complicated, and there are a lot of details to keep track of. In other cases, maybe you just have other things going on in your life and have forgotten a few things. We find that it’s always good to remind yourself of the basics! So if you’re just interested in data and hoping to understand it a little better, then it’s likely you’ll find something useful in this book!\nYour humble authors have struggled mightily themselves throughout the course of their data history, and still do! We were initially taught by those that weren’t exactly experts, and often found it difficult to get a good grasp of statistical modeling and machine learning. We’ve had to learn how to use the tools, how to interpret the results, and possibly the most difficult, how to explain what we’re doing to others! We’ve forgotten a lot, confused ourselves, and made some happy accidents in the process. That’s okay! Our goal here is to help you avoid some of those pitfalls, help you understand the basics of how models work, and get a sense of how most modeling endeavors have a lot of things in common.\nWhether you enthusiastically pour over formulas and code, or prefer to skip over them, we promise that you don’t need to memorize a formula to get a good understanding of modeling and related issues. We are the first to admit that we have long dumped the ability to pull formulas out of our brain folds1; however, knowing how those individual pieces work together only helps to deepen your understanding of the model. Typically using code puts the formula into more concrete terms that you can then use in different ways to solidify and expand your knowledge. Sometimes you just need a reminder or want to see what function you’d use. And often, the visualization will reveal even more about what’s going than the formula or the code. In short, there are a lot of tools at your disposal to help learn modeling in a way that works for you. We hope that anyone that would be interested in the book will find a way to learn things in a manner that suits them best.\nThere is bit of a caveat. We aren’t going to teach you basic statistics or how to program in R or Python. Although there is a good chance you will learn some of it here, you’ll have an easier time if you have a very basic understanding of statistics and some familiarity with coding. We will provide some resources for you to learn more about these topics, but we won’t be covering them in detail. The (appendix?) will provide some more information about prerequisites or just stuff that would be good to know. However, we really aren’t assuming a lot of conceptual knowledge, and are, if anything, assuming that whatever knowledge you have may be a bit loose or fuzzy. That’s okay!"
  },
  {
    "objectID": "introduction.html#what-can-you-expect",
    "href": "introduction.html#what-can-you-expect",
    "title": "1  Introduction",
    "section": "1.3 What Can You Expect?",
    "text": "1.3 What Can You Expect?\nFor each model that we cover, you can expect the following in terms of content:\n\nOverview\n\nWhy it’s useful\nConceptual example and interpretation\nWhere the model lies in the grand scheme of topics that we will cover\n\nKey ideas and concepts\n\nBrief summary and definition list of concepts or terms\n\nDemonstration with data, code, results, and visualizations\n\nThe data will often be simulations as that opens doors for further understanding, or a dataset that hopefully is a little more interesting than mtcars or iris.\nThe demonstrations will provide you the opportunity to get your hands as dirty as you wish. We will present the code in two ways:\n\nstandard functions (e.g., lm in R, ols in statsmodels for Python)\nthe steps to recreate the estimation process on your own (or at least something a little more hands-on)\n\n\nCommentary, cautions, and where to explore next\n\nWe are taking this approach for one reason: so that you can go as deep as you wish. If you are looking for a quick tutorial on helpful models, then you might not find yourself going any deeper than the standard functions (or even getting into the code at all). If you want to really dive into these models, then you might find yourself working through the complete steps. Another approach is to allow yourself some time between the standard functions and complete steps. You could work through the standard functions of every chapter, give it some time to marinate, and then work back through the complete steps. While we certainly recommend working through the chapters in order, we want to give you the flexibility to choose your own depth within each.\nWe hope that this book can serve as a “choose your own adventure” statistical reference. Whether you want a surface-level understanding, a deeper dive, or just want to be able to understand what the analysts in your organization are talking about, you will find value in this book. While we assume that you have a basic familiarity with coding, that doesn’t mean that you need to work through every line of code to understand the fundamental principles and use cases of every model."
  },
  {
    "objectID": "introduction.html#which-language",
    "href": "introduction.html#which-language",
    "title": "1  Introduction",
    "section": "1.4 Which Language?",
    "text": "1.4 Which Language?\nYou’ve probably noticed most books, blogs, and courses on data science choose R or Python. While many individuals often take an opinionated approach towards teaching and using one over the other, we eschew dogmatic approaches and language flame wars. R and Python are both great languages (and equally flawed in unique ways), and it is advantageous to be knowledgeable of both, even if you focus on one specifically, as they are the most popular languages for statistical modeling and machine learning. We use both extensively in our own work for teaching, personal use, and production level code, and have found both are up to whatever task you have in mind. Throughout this book, we will be presenting demonstrations in both R and Python, and you can use both or take your pick, but we want to leave that choice up to you. Our goal isn’t to obscure the ideas behind packages and specialty functions or tell you why one languages is superior to the other (they aren’t), but to show you the most basic functions behind big model ideas.\nWhile we want to provide you choice, we truly hope that displaying both languages can help people to “convert” from one to the other. We have spent countless hours, slumped over our computers, debugging errors and figuring things out. If we can take away one source of pain for you, that would be great! We’d like to consider this as a resource for the R user, who knows exactly what they want to do in R, but could use a little help translating their R knowledge to Python; we’d also like this book to be a resource for the Python user, who sees the value in R’s statistical modeling abilities."
  },
  {
    "objectID": "introduction.html#choose-your-own-adventure",
    "href": "introduction.html#choose-your-own-adventure",
    "title": "1  Introduction",
    "section": "1.5 Choose Your Own Adventure",
    "text": "1.5 Choose Your Own Adventure\nAs an example of how things will go, let’s look at the different ways we might express a relationship between two variables. If you want just a little bit of background, you can read through the Overview and Key Ideas. If you want to see the easy way to use the model, you can work through the Standard Function section. If you want to see the method from start to finish, feel free to work through the Roll Your Own section. If all of that sounds like a lot of work and you’d rather look at the pretty pictures, that’s fine too2, just jump to the Visualization section.\n\n1.5.1 Overview\nCorrelations provide a means of understanding how, and if, two or more things are related. For any two variables or features, we can estimate a single value that signals the strength and direction of the relationship between them. Despite limitations, correlations can be a great way to get a quick understanding of the relationship between two features. One variable could be temperature while another variable is the number of ice cream cones sold, or one is the number of hours spent studying and another variable is the grade on a test, maybe the number of hours spent watching TV versus the number of hours spent exercising. The correlation value will give us information that is similarly interpretable in each case.\n\n\n1.5.2 Key Ideas\nHere are key ideas to consider for understanding correlation.\n\nVariance: Two variables must vary if they are to co-vary\nCovariance: Joint variability of two variables, i.e. how they vary together\nInterpretation: Covariance is hard to interpret because the variables are typically on different scales\nCorrelation: Correlation is a standardized covariance, so it is easier to interpret\n\nAnother way to think about correlation is through the lens of variance covariance. Covariance is a measure of the joint variability of two variables, i.e. how they vary together. If two variables are highly correlated, then they have a high degree of covariance, i.e. they vary together in a meaningful way.\n\n\n1.5.3 Demonstration\nTo demonstrate correlation, we could start by formally defining it. Here is a formula for the Pearson-Product-Moment correlation coefficient. What everyone typically just calls “correlation” or Pearson’s r is actually the Pearson Product-Moment Correlation – that is a lot of words, though, so we will just go with correlation. As we mentioned earlier, we want to give you the choice to dive in as deep as you want.\n\\[\n\\rho = \\frac{\\Sigma(x_i-\\bar{x})(y_i-\\bar{y})}{\\sqrt{\\Sigma(x_i-\\bar{x})^2\\Sigma(y_i-\\bar{y})^2}}\n\\tag{1.1}\\]\nIn words, starting with the numerator, we are going to subtract the mean of x from every observation of x, do the same for y, multiply them together, and then sum them to produce a single value. If you’ve taken a statistics class before, you might have seen this part of the formula when talking about covariance – the joint variability of two variables. In fact, if you just divide the numerator by n-1 you’d get the covariance value. Covariance values, though, are not standardized; therefore, they are a bit tough to interpret. You will get a sign indicating relationship (just like a correlation) and the magnitude tells you the strength of the relationship, but it is hard to get your head around how much stronger a covariance of 4.00 is compared to a covariance of 0.28.\nThis is where the denominator comes into play. To put it into real terms, we will subtract the mean of x from every observation of x (i.e. get the deviations of x), square those values, and sum those values to produce a single value. We will do the same for y, multiply those results, and then take the square root of that product. The denominator might also look familiar to some. The individual pieces- \\(\\Sigma(x_i-\\bar{x})^2\\) and \\(\\Sigma(y_i-\\bar{y})^2\\) - are computing the variance of x and y. Taking the square root of variance gives us the standard deviation. Basically, we’re getting a combined standard deviation to scale the covariance. Using the standard deviation will mitigate the effects of x and y potentially being on different scales; in other words, we want to know how much two variables move together, even if the means and typical movement (deviation) might be very different.\nHere is a simplification, depicting correlation as a standardized covariance.\n\\[\n\\rho = \\frac{cov(x,y)}{\\sigma_x\\sigma_y}\n\\tag{1.2}\\]\nwhere \\(cov(x,y)\\) is the covariance between \\(x\\) and \\(y\\), and \\(\\sigma_x\\) and \\(\\sigma_y\\) are the standard deviations of \\(x\\) and \\(y\\), respectively. So putting it all together, we are simply looking at the ratio of raw covariance (i.e., how much two variables “move” together) and standard deviation (i.e., the amount of dispersion within a variable). Just as we mentioned earlier, simpler concepts often tie models together! What might look like a tricky formula, really just starts with means and deviations, variance and covariance!\nAs a reminder, this correlation will give us an idea about the linear relationship between two continuous variables; we will get a value between -1 and 1, with values closer to 0 indicating that there is no linear relationship between the two variables. As values get closer to 1, we have a positive correlation – as values for one variable increase, values for the other variable tend to increase along with it. As the correlation gets closer to -1, we have a negative correlation – as values for one variable increase, values for the other variable typically decrease. Correlations can be useful for quickly exploring linear relationships, but let’s not get too excited about it – they aren’t going to help you answer any big questions! We also don’t want to get too carried away with “statistical significance” yet – once samples get large, even small correlations become “significant”. Instead, just use correlations to explore the patterns within your data, start getting ideas about interesting relationships that you might find, and leave worries about significance for people with more time on their hands.\n\n1.5.3.1 Code\nIt’s often easier to understand a concept by seeing it in action. So let’s start by creating some data. We’ll create a variable x, then make a y that will have a linear relationship with it, but also have some random noise. We’ll then plot the two variables to see what they look like. To help your understanding, fiddle with the knobs noted.\n\nRPython\n\n\n\nset.seed(seed = 1001)\nN &lt;- 500\nx &lt;- rnorm(n = N, mean = 0, sd = 1)\n\n# Fiddle with the .5 and .75.\n# The first can also be negative if you like!\ny &lt;- .5 * x + rnorm(n = N, mean = 0, sd = .75)\n\n\n\n\nimport numpy as np\n\nnp.random.seed(seed = 1001)\nN = 500\nx = np.random.normal(loc = 0, scale = 1, size = N)\n\n# Fiddle with the .5 and .75. \n# The first can also be negative if you like!\ny = .5*x + np.random.normal(loc = 0, scale = .75, size = N) \n\n\n\n\nNow check out the plot of those two values:\n\n\n\n\n\nScatterplot of two variables.\n\n\n\n\nRemember that correlation is testing for the presence of a linear relationship, with -1 indicating a perfect negative relationship, 1 indicating a perfect positive relationship, and 0 indicating no relationship. Before we see the actual correlation value for these two variables, take a guess as to what value we are going to get!\nBefore we create our own function, we can use R’s cor function or numpy’s corrcoef function. You should get something around 0.6.\n\n# Results for R and Python will be slightly\n# different due to different random number generators\ncor(x, y)\nnp.corrcoef(x, y)\n\n\n\n[1] 0.557\n\n\nWhen you guessed the value, were you close? If so, congrats! If not, try fiddling with those knobs noted until things get a little clearer. But now that we already know the answer, let’s make sure that we can get the same answer by working through the formula via code. The following takes that initial formula approach and turns it into a function that we can use to compute the correlation between any two variables. We’ll then use that function to compute the correlation between our x and y variables.\n\nRPython\n\n\n\nmy_cor = function(x, y) {\n    # First, we need to compute the averages for x and y.\n    # The rest follows the formula.\n    x_bar = mean(x)\n    y_bar = mean(y)\n\n    numerator = sum((x - x_bar) * (y - y_bar))\n\n    denominator = sqrt(\n        sum((x - x_bar)^2) * sum((y - y_bar)^2)\n    )\n\n    numerator / denominator\n}\n\n# using the builtin functions\nmy_cor2 = function(x, y) {\n    cov(x, y) / (sd(x) * sd(y))\n}\n\nmy_cor(x, y)\n\n[1] 0.557\n\n\n\n\n\ndef my_cor(x, y):\n  # First, we need to compute the averages for x and y.\n  # The rest follows the formula.\n    x_bar = np.mean(x)\n    y_bar = np.mean(y)\n    \n    numerator = np.sum((x - x_bar) * (y - y_bar))\n    \n    denominator = np.sqrt(\n      np.sum((x - x_bar)**2) * np.sum((y - y_bar)**2)\n    )\n\n    # We will finish by dividing the numerator by the \n    # denominator.\n    # This will ensure that we have a value between -1 and 1.\n    return(numerator / denominator)\n\nmy_cor(x, y)\n\n0.5390318454354402\n\n\n\n\n\nIt doesn’t matter which language we use, the steps are largely the same when we break it down into the individual pieces!\n\n\n\n1.5.4 Visualization\nA long time ago, in a land far away, the authors of this book worked together to help clients traverse the forests of data to reach their modeling goals. While there were many great learning opportunities along the way, working with clients showed us the kinds of help that people really needed in adventuring with data and models. Even so, there were many requests that made us grimace, and one stood atop Mount Ridiculous: to produce a correlation matrix with 115 variables and export that matrix to a spreadsheet. We still don’t recommend such shenanigans, but there are ways to try and understand correlation matrices. Since we were in the business of helping people do their work better, one way we often did so was via a corrplot.\nWe’ll start with a something manageable. We create a data set with six variables of two sets: a, b , c, and x, y, z, and then we can take a quick look at the correlation matrix.\n\n\n\n\nCorrelation matrix\n  \n    \n    \n      feature\n      a\n      b\n      c\n      x\n      y\n      z\n    \n  \n  \n    a\n1.00\n0.47\n0.45\n−0.24\n−0.17\n−0.13\n    b\n0.47\n1.00\n0.51\n−0.18\n−0.16\n−0.07\n    c\n0.45\n0.51\n1.00\n−0.25\n−0.20\n−0.14\n    x\n−0.24\n−0.18\n−0.25\n1.00\n0.51\n0.45\n    y\n−0.17\n−0.16\n−0.20\n0.51\n1.00\n0.49\n    z\n−0.13\n−0.07\n−0.14\n0.45\n0.49\n1.00\n  \n  \n  \n\n\n\n\nNow we have the pairwise correlations between all six of our variables, with 1’s on the diagonals (naturally, a variable has a perfect correlation with itself). You can check out the lower diagonal or the upper diagonal, because they contain the exact same information. Quickly, though, find the interesting pattern in that matrix!\nProducing the correlations between just 6 variables gives us 15 correlation coefficients to examine! You can see that you’ll need to spend more than a few seconds on finding the interesting patterns within the data (or if there are any patterns at all). Our brains are oriented towards vision, so we can use preattentive processing elements, like hue, saturation, and size, to make finding interesting patterns easier.\nSince we already have a correlation matrix, we can use various means to find those patterns, which include visualizing the matrix itself, network graphs and others. Here is one way to do it.\n\n\n\n\n\n\nNote\n\n\n\nSome recommended R packages for visualizing a correlation matrix include corrr, ggcor, and corrplot. For python, one has options for seaborn, pandas, biokit, and others.\n\n\n\n\n\n\n\nCorrelation matrix visualization\n\n\n\n\nLet’s break down what we’re seeing just a little bit. The lower triangle has the correlation values. It adds information, though, by changing the hue by correlation value – red for negative values and blue for positive values – and increasing the saturation as the correlation value becomes stronger. The upper triangle contains the same information, but the size of the circle is tied directly to the strength of the correlation. You’ll also notice that the weaker correlations are more hidden in the visualization, allowing us to focus only on those interesting relationships.\nWhat do you think? Was it easier to spot the points of interest? It looks like there is an a-b-c group and x-y-z group that are similarly correlated within their respective groups. We can also see that the a-b-c group is negatively correlated with the x-y-z group. Visualizing the correlation matrix can usually make it easier to find those interesting patterns.\n\n\n1.5.5 Commentary\nThe correlation is a starting point for understanding linear models, which serve as the foundation for modeling in general. It is very limited by only assessing linear relationships between variables, as well as only pairwise relationships. Other metrics can overcome these, but they have their own limitations. The basic Pearson-Product-Moment correlation coefficient is still the most widely used and a typical starting point in many data adventures.\nThings to explore further next:\n\nRank correlation (e.g., Spearman’s rho, Kendall’s tau)\nDistance metrics (e.g., euclidean, manhattan, cosine)\nNon-linear relationships and interactions (e.g., distance correlation, polynomial, splines)\nMultivariate relationships (e.g., partial correlations, r-squared)"
  },
  {
    "objectID": "introduction.html#moving-towards-an-excellent-adventure",
    "href": "introduction.html#moving-towards-an-excellent-adventure",
    "title": "1  Introduction",
    "section": "1.6 Moving Towards An Excellent Adventure",
    "text": "1.6 Moving Towards An Excellent Adventure\nRemember the point we made about “choosing your own adventure”? Statistical modeling and programming is an adventure, even if you never leave your desk! Every situation calls for choices to be made and every choice you make will lead you down a different path. You will run into errors, dead ends, and you might even find that you’ve spent considerable time to conclude that nothing interesting is happening in your data. This, no doubt, is part of the fun and all of those struggles make success that much sweeter. Like every adventure, things might not be immediately clear and you might find yourself in perilous situations! If you find that something isn’t making sense upon your first read, that is okay. Both authors have spent considerable time mulling over models and foggy ideas during our assorted (mis)adventures; nobody should expect to master complex concepts on a single read through! In any arena where you strive to develop skills, distributed practice and repetition are essential. When concepts get tough, step away from the book, and come back with a fresh mind.\nThanks for coming on this adventure with us and welcome to our Book of Models.\nThis is a test reference (Hastie, Tibshirani, and Friedman 2009).\n\n\n\n\nHastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2009. The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition. 2nd ed. 2009. Corr. 3rd printing 5th Printing. Springer."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Hastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2009. The\nElements of Statistical Learning: Data Mining, Inference, and\nPrediction, Second Edition. 2nd ed. 2009. Corr. 3rd printing 5th\nPrinting. Springer."
  }
]