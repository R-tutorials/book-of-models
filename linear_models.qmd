# The Foundation

> It is the chief characteristic of data science that it works.
â€• Isaac Asimov (paraphrased)

```{r}
#| label: setup-lm
#| include: false
source("load_packages.R")
source("setup.R")
source("functions/utils.R")

reticulate::use_condaenv("book-of-models")
```



Packages needed or useful for this chapter include:

:::{.panel-tabset}

##### R

```{r}
#| echo: true
#| eval: false
#| label: packages

library(tidyverse)
```

##### Python

```{python}
#| label: modules
#| 
import numpy as np
import pandas as pd
import statsmodels.formula.api as smf
```

:::


```{r}
#| echo: false
#| label: r-display-setup

options(digits = 4) # number of digits of precision for floating point output
```


```{python}
#| echo: false
#| label: py-display-setup

np.set_printoptions(precision = 4, suppress=True) # suppress is for whether to use scientific notation for otherwise rounded to zero numbers
pd.set_option('display.precision', 4) # number of digits of precision for floating point output
```


## Introducing the Greatest Of All Time

Now that you have some idea of what you're getting into, it's time to dive in! We'll start things off by covering the building block of all modeling, and a solid understanding here will provide you the basis for just about anything that comes after, no matter how complex it gets. The **linear model** is our starting point. At first glance, it may seem like a very simple model, but it's actually quite powerful and flexible, able to take in different types of inputs, handle nonlinear relationships, temporal and spatial relations, clustering, and more. Linear models have a long history, with even the formal and scientific idea behind correlation and linear regression being well over a century old[^corbypeirce]! And in that time, the linear model is far and away the most used model out there. But before we start talking about the *linear* model, we need to talk about what a **model** is in general.


[^corbypeirce]: Peirce & Bowditch were well ahead of Pearson and Galton [@rovine2004peirce].


### What is a Model?

At its core, a model is just an **idea**. It's a way of thinking about the world, about how things work, how things change over time, how things are different from each other, and how they are similar. The underlying thread is that **a model expresses relationships** about things in the world around us. One can also think of a **model as a tool**, one that allows us to take information, in the form of data, and act on it in some way. Just like other ideas (and tools), models have consequences in the real world, and they can be used wisely or foolishly.

On a practical level, a model is expressed through a particular language, math, but don't let that worry you if you're not so inclined. As it's still just an idea at its core, the idea is the most important thing to understand about a model. The **math is just a formal way of expressing the idea** in a manner that can be communicated and understood by others in a standard way, and math can help make the idea precise. But in everyday terms, we're trying to understand things like how the amount of sleep relates to cognitive functioning, how the weather affects the number of people who visit a park, how much money to spend on advertising to increase sales, how to detect fraud, and so on.  Any of these  could form the basis of a model, as they stem from scientifically testable ideas, and they all express relationships between things we are interested in, possibly even with an implication of causal relations. 

If you wanted to run a linear model to understand the relationship between sleep and cognitive functioning, you might express it in code as:

:::{.panel-tabset}

##### R

```{r}
#| eval: false
#| label: lm-sleep-cog-func
lm(cognitive_functioning ~ sleep)
```

##### Python

```{python}
#| eval: false
#| label: ols-sleep-cog-func
from statsmodels.formula.api import ols

model = ols('cognitive_functioning ~ sleep', data=df).fit()
```

:::


Very easy! But that's all it takes to express a straightforward idea.  In this case, we're saying that cognitive functioning is a linear (function) of sleep.  By the end of this chapter you'll also know why R's function is `lm` (linear model) and the [statsmodels]{.pack} function is `ols`, but both are doing the same thing. 


## Key ideas

We can pose a few concepts key to understanding models. This is not an exhaustive list, but it's a good start.  We'll cover each of these as we go along.

- What a model is: The model as an idea
- Features, targets, and input-output mappings: how do we get from input to output?
- Prediction: how do we use a model?
- Interpretation: what does a model tell us?
    - Prediction underlies all interpretation
    - We can interpret a model at the feature level and as a whole

As we go along and cover these concepts, be sure that you feel you have the 'gist' of what we're talking about.  Almost everything of what comes after linear models builds on these ideas, so it's important to have a firm grasp before climbing to new heights.

Chapter goals:

- Understand what a model is conceptually
- Understand what a linear model is and how features are mapped to the target
- Be able to get predictions from our model
- Be able to understand the results of a model at a basic level
- Get a sense of complexity and other issues

## What goes into a model? 

### Features and Targets

In the context of a model, how we specify the nature of the relationship depends on the context. In the interest of generality, we'll refer to the **target** as what we want to explain, and **features** as those aspects of the data we will use to explain it. Because people come at data from a variety of contexts, they often use different terminology to mean the same thing. The table below shows some of the common terms used to refer to features and targets. 

```{r tbl-feat-target, cache=FALSE}
#| echo: false
#| label: tbl-feature-target-names
#| tbl-cap: Common Terms for Features and Targets


tbl_feat_targ = tibble(
    Feature = c("independent variable", "predictor variable", "explanatory variable", "covariate", "x", "input", "right-hand side"),
    Target  = c("dependent variable", "response", "outcome", "label", "y", "output", "left-hand side"),
) |>
    gt() |>
    rm_caption() # does nothing

tbl_feat_targ
```

Some of these actually suggest a particular type of relationship (e.g., a causal relationship, an experimental setting), but we'll typically avoid those terms if we can. In the end, we may use many of these words to describe things so that you are comfortable with the terminology, but typically we'll stick with **features** and **targets** for the most part. In our opinion, this terminology has the least hidden assumptions/implications.


### Expressing Relationships

As noted, a model is a way of expressing a relationship between a set of features and a target, and one way of thinking about this is in terms of **inputs** and **outputs**. But how can we go from input to output?  Well to begin, we assume that the features and target are **correlated**, i.e. that there is some relationship between the x and y.  If so, then we can ultimately use the features to **predict** the target. In the simplest setting a correlation implies a  relationship where x and y typically move up and down together (left plot) or they move in opposite directions where x goes up and y goes down (right plot).

```{r corr-plot, cache.rebuild=TRUE}
#| echo: false
#| label: fig-corr-plot
#| fig-cap: Correlation

p_dat = tibble(
    x = rnorm(50),
    y = .75 * x + rnorm(50, sd = .5),
    yneg = -.75 * x + rnorm(50, sd = .5)
)

p1 = ggplot(p_dat, aes(x, y)) +
    geom_point() +
    geom_smooth(method = "lm", se = FALSE) +
    labs(subtitle = "Positive Correlation")

p2 = ggplot(p_dat, aes(x, yneg)) +
    geom_point() +
    geom_smooth(method = "lm", se = FALSE) +
    labs(
        y = 'y'
    ) +
    labs(subtitle = "Negative Correlation")

p1 + p2
```

In addition, the typical correlation suggests a linear relationship.  There are many types of correlation metrics, but the most common one, the **Pearson correlation**, is explicitly a measure of the linear relationship between two variables.  It's expressed as a number between -1 and 1, where 0 means there is no linear relationship. As we move closer to a 1.0 correlation value, we would see a tighter scatterplot like the one on the left, until it became a straight line. The same happens for the negative relationship as we get closer to a value of -1.  If we have only one feature and target, the Pearson correlation reflects the exact result of the linear model we'd conduct. But even with multiple features, we often use a version of the Pearson R to help us understand how the features account for the target's variability.


```{r}
#| echo: false
#| eval: false
#| label: anim-corr-plot

# https://github.com/quarto-dev/quarto-cli/discussions/3551
library(gganimate)

# create data that expresses different correlation strengths between x and y for each 'set'

set.seed(1234)
n_sets = 10
n_obs = 500
# x = rnorm(n_sets * n_obs)
# y = rnorm(n_sets * n_obs)
corrs = seq(-1, 1, length.out = n_sets)

p_dat_corrs = map2_df(
    corrs,
    1:n_sets,
    \(corr, set)
    as_tibble(mvtnorm::rmvnorm(n_obs, c(0, 0), matrix(c(1, corr, corr, 1), nrow = 2))) |>
        rename(
            x = 1,
            y = 2
        ) |>
        mutate(
            corr = round(corr, 2),
            set = set
        )
)

# use gganimate to create a plot for each correlation value

# anim = p_dat_corrs |>
#     ggplot(aes(x, y)) +
#     geom_point() +
#     geom_smooth(method = "lm", se = FALSE) +
#     labs(
#         x = "x",
#         y = "y",
#         title = "Correlation: {closest_state}",
#     ) +
#     transition_states(corr, transition_length = 2, state_length = 1) +
#     ease_aes('linear') +
#     theme(
#         plot.title = element_text(hjust = 0.5),
#         plot.subtitle = element_text(hjust = 0.5)
#     )
# gganimate::anim_save(
#     "img/corr_anim.gif", anim, fps = 1, duration = 10, width = 600, height = 600
#     )

p_dat_corrs |>
    ggplot(aes(x, y)) +
    geom_point() +
    geom_smooth(method = "lm", se = FALSE) +
    labs(
        x = "x",
        y = "y",
        title = "Correlation: {closest_state}",
    ) +
    # transition_states(corr, transition_length = 2, state_length = 1) +
    # ease_aes('linear') +
    facet_wrap(~corr, ncol = 5) +
    theme_void() +
    theme(
        plot.title = element_text(hjust = 0.5),
        plot.subtitle = element_text(hjust = 0.5)
    )

```

<!-- ![](img/linear_model/corr_anim.gif) BRING BACK FOR HTML --> 



## *THE* Linear Model

The linear model is perhaps the simplest *functional* model we can use to express a relationship between features and targets. And because of that, it's possibly still the most common model used in practice, and it is the basis for many types of other models. Why don't we run one now?

The following dataset has individual [movie reviews](#app-data-review) and contains the rating (1-5 stars scale), along with features pertaining to the review (e.g., word count, etc.), those that regard the reviewer (e.g., age) and features about the movie (e.g., genre, release year).  We'll use the linear model to predict the rating from the length of the review in terms of word count.

```{r}
#| echo: false
#| eval: false
#| cache: false
#| label: import-review
library(tidyverse)

df_reviews = read_csv("data/movie_reviews.csv")

skimr::skim(df_reviews)
```

For our first linear model, we'll keep things simple.  Let's predict the rating from the length of the review in terms of word count.  We'll use the `lm()` function in R and the `ols()` function in Python[^smfolsR] to fit the model.  Both functions take a formula as the first argument, which is a way of expressing the relationship between the features and target.  The formula is expressed as `y ~ x1 + x2 + ...`, where `y` is the target name and `x` are the feature names. We also need to specify what the data object is, typically a data frame. 

[^smfolsR]: We actually are using the `smf.ols` approach because it is modeled on the R approach.

:::{.panel-tabset}

##### R

```{r my-first-model}
#| label: r-my-first-model
df_reviews = read_csv("data/movie_reviews.csv")

model_reviews = lm(rating ~ word_count, data = df_reviews)

summary(model_reviews)
```

```{r}
#| echo: false
#| eval: false
#| label: save-r-model_reviews
# only run as needed

save(model_reviews, file = "data/model_reviews.RData")
```

##### Python

```{python}
#| label: py-my-first-model
#| 
df_reviews = pd.read_csv('data/movie_reviews.csv')

model_reviews = smf.ols('rating ~ word_count', data = df_reviews).fit()

model_reviews.summary(slim = True)
```

```{python}
#| echo: false
#| eval: false
#| label: save-py-model_reviews

# only run as needed
model_reviews.save("linear_models/data/model_reviews.pkl")
```

:::

```{r}
#| echo: false
#| label: my-first-model-coefs-etc
intercept  = round(coef(model_reviews)[1], 2)
wc_coef    = round(coef(model_reviews)[2], 2)
sd_y       = round(sd(df_reviews$rating), 2)
sd_x       = round(sd(df_reviews$word_count), 2)
rl_ci      = round(confint(model_reviews)[2, ], 3)
n_words    = round(mean(df_reviews$word_count))
model_rmse = round(performance::performance_rmse(model_reviews), 2)
model_rsq  = round(performance::r2(model_reviews)$R2, 2)
```

For such a simple model, we certainly have a lot to unpack here! Don't worry, you'll eventually come to know what it all means. But it's nice to know how easy it is to get the results!

We'll start with the fact that the linear model posits a **linear combination** of the features. A linear combination is just a sum of the features, each of which has been multiplied by some specific value. That value is often called a **coefficient**, or possibly **weight**, depending on the context.  The linear model is expressed as (math incoming!):

$$
y = b_0 + b_1x_1 + b_2x_2 + ... + b_nx_n
$$  {#eq-lm-basic}

- $y$ is the target.
- $x_1, x_2, ... x_n$ are the features.
- $b_0$ is the intercept, which is kind of like a baseline value or offset. If we had no features at all it would just be the mean of the target.
- $b_1, b_2, ... b_n$ are the coefficients or weights for each feature.

But lets start with something simpler, let's say you want to take a sum of several features. In math you would write it as:

$$
x_1 + x_2 + ... + x_n
$$

In the previous equation, x is the feature and n is the number identifier for the features, so $x_1$ is the first feature, $x_2$ the second, and so on.  $x$ is an arbitrary designation, you could use any letter, symbol you want, or even better, would be the actual feature name. Now look at the linear model.

$$
y = x_1 + x_2 + ... + x_n
$$


In this case, the function is *just a sum*, something so simple we do it all the time. In the linear model sense though, we're actually saying a bit more. Another way to understand that equation is that *y is a function of x*. We don't show any coefficients, i.e. the bs in our initial depiction, but technically it's as if each coefficient was a value of 1.  In other words, for this simple linear model, we're saying that each feature contributes in an identical fashion to the target. 

In practice, features will not contribute in the same ways, because they correlate with the target differently or are on different scales. So if we want to relate some feature, x1, and some other feature, x2, to target y, we probably would not assume that they both contribute in the same way from the beginning.  We might give relatively more weight to x1 than x2. In the linear model, we express this by multiplying each feature by a different coefficient. So the linear model is really just a sum of the features multiplied by their coefficients, i.e. a *weighted sum*. In fact, we're saying that each feature contributes to the target in proportion to the coefficient. So if we have a feature x1 and a coefficient b1, then the contribution of x1 to the target is b1\*x1. If we have a feature x2 and a coefficient b2, then the contribution of x2 to the target is b2\*x2. And so on.  So the linear model is really just a sum of the features multiplied by their coefficients.  

For our model, here is the mathematical representation:

$$
\textrm{rating} = b_0 + b_1 \cdot \textrm{word\_count}
$$

And with the actual results of our model:


```{r}
#| echo: false
#| results: 'asis'
#| label: word-count-linear-model

init <- "
$$
\\textrm{rating} = "

end <- " \\cdot \\textrm{word\\_count}
$$"

glue("{init}{intercept} + {wc_coef}{end}")
```



Not too complicated we hope!  But let's make sure we see what's going on here just a little bit more. 

- Our *idea* is that the length of the review is in some way related to the eventual rating given to the movie. 
- Our *target* is rating, and the *feature* is the word count
- We *map the feature to the target* via the linear model, which provides an initial understanding of how the feature is related to the target.  In this case, we start with a baseline of `r intercept`.  This value makes sense only in the case of a rating with no review, but we have reviews for every observation, so it's not very meaningful as is. We'll talk about ways to get a more meaningful intercept later, but for now, that is our starting point. Moving on, if we add a single word to the review, we expect the rating to go down by `r wc_coef` stars.  So if we had a review that was `r n_words` words long, i.e., the mean word count, we would predict a rating of `r paste(intercept)` + `r glue("{n_words}*{wc_coef}")` = `r round(predict(model_reviews, newdata = data.frame(word_count = n_words)), 1)` stars.


### Models as a Graph

We can also express the linear model as a graph, which can be a very useful way to think about models in a visual fashion, and as we see others, can help us literally see how different models relate to one another.  In the following, we have three features predicting a single target, so we have three nodes for the features, and a single node for the target.  The feature nodes are connected to the target node by an edge, which is labeled with the coefficient. The graph below shows a basic linear model.

```{dot}
//| echo: false
//| cache: false
//| label: graph-lm
//| fig-cap: Linear Model as a Graph
//| fig-width: 4

// file: linear_models/misc/graphical_lm.dot

digraph DAG {

    graph [rankdir = LR bgcolor=transparent]

    node [shape = circle, fontcolor=gray25 color=gray80]

    node [fontname="Helvetica" fontsize=8 fixedsize=true width=.3]
    x1 [label=<x<sub>1</sub>>]; x2 [label=<x<sub>2</sub>>]; x3 [label=<x<sub>3</sub>>]; 

    node [fillcolor=gray90 style=filled fixedsize=true width=.4]
    y; b0[label=<b<sub>0</sub>> width=.3];

    edge [color=gray50 style=filled arrowsize=.5 fontsize = 5 fontcolor=gray25]
    x1 -> y [label=<b<sub>1</sub>>]; x2 -> y [label=<b<sub>2</sub>>]; x3 -> y [label=<b<sub>3</sub>>];

    b0 -> y;
}
```

```{mermaid}
%%| echo: false
%%| eval: false

%%  graph LR
    n1((X<sub>1</sub>)):::feat --> |b<sub>1</sub>| n4((y))
    n2((X<sub>2</sub>)):::feat --> |b<sub>2</sub>| n4((y))
    n3((X<sub>3</sub>)):::feat --> |b<sub>3</sub>| n4((y))
    n5((b<sub>0</sub>))     --> n4((y))

    style n1 fill:#56B4E9,stroke:#fff,color:#fff
    style n2 fill:#56B4E9,stroke:#fff,color:#fff
    style n3 fill:#56B4E9,stroke:#fff,color:#fff
    style n4 fill:#E69F00,stroke:#333,stroke-width:1px,color:#fff,height:150px
    style n5 fill:#404040,stroke:#333,color:#fff
    classDef e3 font-weight:800,height:50px,font-size:20px,width:50px,vertical-align:top
    %% size doesn't work with circles and not well with squares
    %% unbelievable https://github.com/mermaid-js/mermaid/issues/3183
```




So at this point you have the basics of what a linear model is and how it works. But there is a lot more to it than that. Just getting the model is easy enough, but we need to be able to use it and understand the details better, so we'll get into that now!



## What do we do with a model? 


Once we have a working model, there are two primary ways we can use it.  One way to use a model is to help us understand the relationships between the features and our outcome of interest. In this way the focus can be said to be on **explanation**, or interpreting the model results. The other way to use a model is to use it to make estimates about the outcome for specific observations, often ones we haven't seen in our data. In this way the focus is on **prediction**. In practice, we often do both, but the focus is usually on one or the other. We'll cover both in detail here, starting with prediction.

### Prediction

It may not seem like much at first, but a model is of no use if it can't be used to make predictions about what we can expect in the world around us. Once our model has been *fit* to the data, we can obtain our predictions by plugging in values for the features that we are interested in, and, using the corresponding weights and other parameters that have been estimated, come to a guess about a specific observation. Let's go back to our results, starting with a simpler depiction.

```{r}
#| echo: false
#| label: my-first-model-output-2

broom::tidy(model_reviews, conf.int = TRUE) |>
    janitor::clean_names() |>
    rename(feature = term) |>
    mutate(feature = ifelse(feature == "(Intercept)", "intercept", feature)) |>
    gt()
```

The table shows the **coefficient** for each feature including the intercept, which is our starting point.  In this case, the coefficient for word count is `r wc_coef`, which means that for every additional word in the review, the rating goes `r ifelse(sign(wc_coef) == 1, 'up', 'down')` by `r wc_coef` stars.  So if we had a review that was `r n_words` words long, we would *predict* a rating of `r paste(intercept)` + `r glue("{n_words}*{wc_coef}")` = `r round(predict(model_reviews, newdata = data.frame(word_count = n_words)), 1)` stars.  


When we're talking about predictions for a linear model, we usually will see this as the following mathematically:

$$
\hat{y} = b_0 + b_1x_1 + b_2x_2 + ... + b_nx_n
$$

What is $\hat{y}$? The hat over the $y$ just means that it's a predicted value of the model, rather than the one we actually observe. In fact, we were missing something in our previous depictions of the linear model.  We need to add what is usually referred to as an **error term**, $\epsilon$, to account for the fact that our predictions will not be perfect[^perfect_prediction].  So the full linear model is:

[^perfect_prediction]: In most circumstances, if you ever have perfect prediction, or even near perfect prediction, the usual issues are that you have either asked a rather obvious/easy question of your data (e.g., predicting whether an image is of a human or a car), or have accidentally included the target in your features (or a combination of them) in some way.

$$
y = b_0 + b_1x_1 + b_2x_2 + ... + b_nx_n + \epsilon
$$

The error term is a random variable that represents the difference between the actual value and the predicted value.  We can't know what the error term is, but we can estimate it.  We'll talk more about that in the section on [estimation][#estimation].


### What kinds of predictions can we get?

What predictions we get depends on the type of model we are using.  For the linear model, we can get predictions for the target, which is a **continuous variable**.  Very commonly, we also can get predictions for a **categorical target**, such as whether the rating is 'good' or 'bad'. This simple breakdown pretty much covers everything, as we typically would be predicting a continuous variable or a categorical variable, or more of them, like multiple continuous variables, or a target with multiple categories, or sequences of categories (e.g. words). In our case, we can get predictions for the rating, which is a number between 1 and 5. Had our target been a binary good vs. bad rating, our predictions would still be numeric, and usually expressed as a probability between 0 and 1. We then would convert that probability to a class of good or bad depending on a chosen probability cutoff. We'll talk about how to get predictions for categorical targets later.

We saw a prediction for a single observation, but we can also get predictions for multiple observations at once.  In fact, we can get predictions for all observations in our dataset. Besides that, we can also get predictions for observations that we don't have data for. The following shows how we can get predictions for all data, and for a single observation with a word count of 5.

::: {.panel-tabset}

##### R

```{r}
#| label: my-first-model-predictions-r

all_predictions = predict(model_reviews)
single_prediction = predict(model_reviews, newdata = data.frame(word_count = 5))
```

##### Python

```{python}
#| label: my-first-model-predictions-py

all_predictions   = model_reviews.predict()
single_prediction = model_reviews.predict(pd.DataFrame({'word_count': [5]}))
```

:::



Here is a plot of our predictions versus the actual ratings[^jitter]. The reference line is where the points would fall if we had perfect prediction. We can see that the predictions are definitely not perfect, but they are not completely off base either. We'll talk about how to assess the quality of our predictions later, but we can at least get a sense that we have a correspondence relationship between our predictions and target, which is definitely better than not having a relationship at all!

[^jitter]: Word count is **discrete**- it can only take whole numbers like 3 or 20, and it is our only feature.  Because of this, we can only make very limited predicted rating values, while the observed rating can take on many other values. Because of this, the true plot would show a more banded result with many points overlapping, so we use a technique called **jittering** to move the points around a little bit so we can see them all.  The points are still roughly in the same place, but they are moved around a little bit so we can see them all.  

```{r}
#| echo: false
#| label: my-first-model-predictions-plot
#| fig-cap: Predictions vs. Actual Ratings\

df_reviews |>
    mutate(pred = all_predictions) |>
    ggplot(aes(pred, rating)) +
    # geom_point() +
    geom_point(position = position_jitter()) +
    geom_abline(intercept = 0, slope = 1, color = okabe_ito[2], size = 2) +
    labs(
        x = "Predicted Rating",
        y = "Actual Rating",
        title = "Predictions vs. Actual Ratings",
        caption = "Points have been jittered for better visibility."
    ) +
    theme(
        plot.caption = element_text(vjust = -2)
    )
```

Now let's look at what our prediction looks like for a single observation, and we'll add in a few more- one for 10 words, and one for a 50 word review, which is beyond the length of any review in this dataset, and one for 12.3 words, which isn't even possible for this data.

```{r}
#| echo: false
#| label: tbl-predictions
#| tbl-cap: Predictions for Specific Observations

tibble(
    word_count = c(5, 10, 12.3, 50),
    pred = predict(model_reviews, newdata = data.frame(word_count = word_count))
) |>
    rename("Word Count" = word_count, "Predicted Rating" = pred) |>
    gt() |>
    fmt_number(decimals = 1)
```

The values reflect the `r word_sign(wc_coef, c('positive', 'negative'))` coefficient from our model, reflecting a `r word_sign(wc_coef, c('increasing', 'decreasing'))` relationship. Further more, we see the power of the model's ability to make predictions for what we can't see. Maybe we limited our data review size, but we know there are 50 word reviews out there, and we can still make a guess as to what the rating would be for such a review.  Maybe in another case, we know a group of people who have on average 12.3 word reviews, and we can make a guess as to what the average rating would be for that group. Our model doesn't know anything about the context of the data, but we can use our knowledge to make predictions about the world around us. This is a very powerful capability, and it's one of the main reasons we use models in the first place.  


### Prediction Error

As we have seen, predictions are not perfect, and an essential part of the modeling endeavor is to better understand these errors and why they occur. In addition, error assessment is the fundamental way in which we assess a model's performance, and, by extension, compare that performance to other models. In general, prediction error is the difference between the actual value and the predicted value or some function of it, and in statistical models, is also often called the **residual**.  We can look at these individually, or we can look at them in aggregate with a single metric.


Let's start with looking at the residuals visually. Often the modeling package you use will have this as a default plotting method when doing a standard linear regression, so it's wise to take advantage of it. We plot both the distribution of raw error scores and the cumulative distribution of absolute prediction error. Here we see a couple things. First, the distribution is roughly normal, which is a good thing, since statistical linear regression assumes our prediction error is normally distributed. Second, we see that the mean of the errors is zero, which is a consequence of linear regression, and the reason we look at the mean squared error rather than the mean error when assessing model performance. We can also see that most of our predictions are within 1 star rating. 

```{r}
#| echo: false
#| label: my-first-model-error-plot
#| fig-cap: Distribution of Prediction Errors
p_hist <- tibble(
    error = resid(model_reviews)
) |>
    ggplot(aes(error)) +
    geom_histogram() +
    geom_vline(xintercept = 0, color = okabe_ito[2], size = 2) +
    labs(
        x = "Prediction Error",
        y = "Count",
        title = "Distribution of Prediction Errors",
    ) +
    theme(
        plot.caption = element_text(vjust = -2)
    )

p_ecdf <- tibble(res = abs(resid(model_reviews))) |>
    arrange(res) |>
    mutate(prop = 1 - row_number() / n()) |>
    ggplot(aes(x = res)) +
    geom_vline(xintercept = 1, color = okabe_ito[2]) +
    geom_vline(xintercept = mean(abs(resid(model_reviews))), color = okabe_ito[2]) +
    stat_ecdf(size = 1, color = okabe_ito[1]) +
    stat_ecdf(aes(ymin = 0, ymax = ..y..), size = 1, fill = okabe_ito[1], geom = "ribbon", alpha = .1) +
    scale_y_continuous(breaks = seq(10, 100, 10) / 100, labels = scales::percent) +
    labs(
        x = "Absolute Prediction Error",
        y = "Cumulative Proportion",
        # title = "Prediction Errors",
        caption = "Vertical lines show mean absolute error and 1 star error."
    ) +
    theme(
        plot.caption = element_text(vjust = -2)
    )

p_hist + p_ecdf

```

Of more practical concern however, is that we don't see extreme values or clustering that might indicate a failure on the part of the model to pick up certain segments of the data.  It still is good to look at the extremes just in case we can pick up on some aspect of the data that we could potentially incorporate into the model. 

```{r}
#| echo: false
#| label: get_worst_prediction
#|
worst_prediction = df_reviews |>
    mutate(prediction = predict(model_reviews)) |>
    slice(which.max(abs(resid(model_reviews)))) |>
    select(rating, prediction, word_count)
```


Looking at our worst prediction in absolute terms, we see the observation has a typical word count, and so our simple model will just predict a fairly typical rating. But the actual rating is `r worst_prediction$rating`, which is `r abs(round(worst_prediction$rating - worst_prediction$prediction, 1))` away from our prediction, a very noticeable difference.


```{r}
#| echo: false
#| label: tbl-worst-prediction
#| tbl-cap: Worst Prediction

worst_prediction |>
    gt() |>
    fmt_number(c(rating, prediction), decimals = 1) |>
    fmt_number(word_count, decimals = 0)
```

We can also get an overall assessment of the prediction error. In the case of the linear model we've been looking at, we can express this in a single metric as the sum or mean of our (squared) errors, the latter of which is a very commonly used modeling metric- **MSE** or **mean squared error**, or also, its square root - **RMSE** or **root mean squared error**. 

If we look back at our results, we can see this expressed as the part of the output or as an attribute of the model[^sigmanotrmse]. The RMSE is more interpretable, and it gives us a sense that we can expect an average prediction error for rating of `r model_rmse`.  Given that the rating is on a 1-5 scale, this maybe isn't bad, but we could definitely hope to do better than get within roughly half a point on this scale.  We'll talk about ways to improve this later.


[^sigmanotrmse]: The actual divisor for linear regression output depends on the complexity of the model, and in this case the sum of the squared errors is divided by N-2 (due to estimating the intercept and coefficient) instead of N. This is a technical detail that would only matter for data too small to make much of in the first place, and not important for our purposes here.

:::{.panel-tabset}

##### R

```{r}
#| label: my-first-model-mse-r
summary(model_reviews) # 'Residual standard error' is approx RMSE
```

##### Python


```{python}
#| label: my-first-model-mse-py
np.sqrt(model_reviews.scale)   # RMSE
```

:::


At this point you have the gist of prediction and prediction error, but there is a lot more to it. More detail can be found in the [estimation chapter](#estimation), since we often estimate the parameters of our model by picking those that will reduce the prediction error the most.    For now, let's move on to the other main use of models, which is to help us understand the relationships between the features and the target, or **explanation**.


## How do we interpret the model?

When it comes to interpreting the results of our model, there are a lot of tools at our disposal, though many of the tools we can ultimately use depend on the specifics of the model we have employed. In general though, we can group our understanding approach to that of the **feature level** and the **model level**. A feature level understanding regards the relationship between a single feature and the target. Beyond that, we also attempt comparisons of feature contributions to prediction, i.e. relative importance. Model level interpretation is focused on assessments of how well the model 'fits' the data, or more generally, predictive performance. We'll start with the feature level, and then move on to the model level.

### Feature Level 

As mentioned, at the feature level, we are primarily concerned with the relationship between a single feature and the target. More specifically, we are interested in the direction and magnitude of the relationship, but in general, it all boils down to how a feature induces change in the target. For numeric features, we are curious about the change in the target given some amount of change in the feature. For categorical features it's the same, but often we like to express the change in terms of group mean differences or something similar, since the order of categories is not usually meaningful.  Key to the feature level interpretation is the specific predictions made at key feature values.

#### Basics

Let's start with the basics by looking again at our coefficient table from the model output.

```{r}
#| echo: false
#| label: my-first-model-output

broom::tidy(model_reviews, conf.int = TRUE) |>
    janitor::clean_names() |>
    rename(feature = term) |>
    mutate(feature = ifelse(feature == "(Intercept)", "intercept", feature)) |>
    gt()
```

Here, the main thing to look at are the actual feature coefficients and the direction of their relationship, positive or negative.  We saw before that the coefficient for word count is `r wc_coef`, and this means that for every additional word in the review, the rating goes `r ifelse(sign(wc_coef) == 1, 'up', 'down')` by `r wc_coef`.  So if we had a review that was `r n_words` words long, we would predict a rating of `r paste(intercept)` + `r glue("{n_words}*{wc_coef}")` = `r round(predict(model_reviews, newdata = data.frame(word_count = n_words)), 1)` stars.

This interpretation gives us directional information, but how can we interpret the magnitude of the coefficient? Let's try and use some context to help us. The value for the coefficient is `r round(wc_coef, 2)`, and the standard deviation of the target, i.e. how much it moves around naturally on its own, is `r sd_y`.  So the coefficient is about `r round(abs(wc_coef) / sd_y * 100, 0)`% of the standard deviation of the target.  In other words, the addition of a single word to a review results in an expected `r word_sign(wc_coef, c('increase', 'decrease'))` of `r round(abs(wc_coef) / sd_y * 100, 0)`% of what the review would normally bounce around in value. We probably wouldn't consider this negligible, but also, a single word change isn't much. What would be a significant change in word count? Let's consider the standard deviation of the feature. In this case it's `r sd_x` for word count.  So if we increase the word count by one standard deviation, we expect the rating to `r word_sign(wc_coef, c('increase', 'decrease'))` by `r glue('{wc_coef} * {sd_x} = {round(wc_coef * sd_x, 2)}')`, and this translates in to a change of `r round(wc_coef * sd_x, 2)`/`r sd_y` = `r  round(sd_x/sd_y * wc_coef, 2)` standard deviation units of the target. Without additional context, many would think that's a significant change [CITATION], or at the very least, that the coefficient is not negligible, and that the feature is indeed related to the target.  But we can also see that the coefficient is not so large that it's not believable.

::: {.callout-tip title='Standardized Coefficients'}
The calculation we just did results in what's often called a 'standardized' or 'normalized' coefficient. In the case of the simplest model with only one feature like this, it is identical to the Pearson r correlation metric, which we invite you to check and confirm on your own, which should roughly equal our calculation using rounded values. In the case of multiple features, it represents a (partial) correlation between the target and the feature, after adjusting for the other features. But before you start thinking of it as a measure of *importance*, it is not. It provides some measure of the feature-target linear relationship, but that doesn't not entail *practical* importance, nor is it useful in the presence of nonlinear relationships, interactions, and a host of other interesting things that are typical to data and models.
:::


After assessing the coefficients, next up in our table is the **standard error**. The standard error is a measure of how much the coefficient varies from sample to sample.  If we collected the data multiple times, even under identical circumstances, we wouldn't get the same value each time- it would bounce around a bit, and the standard error is an estimate of how much it would bounce around. In other words, it's a measure of **uncertainty**, and along with the coefficients, it's used to calculate everything else in the table.  The statistic, here a t-statistic from the student t distribution[^coefstats], is the ratio of the coefficient to the standard error. This gives us a sense of the effect relative to its variability, but the statistic's primary use is to calculate the **p-value** related to its distribution[^pvaluet], which is the probability of seeing a coefficient as large as the one we have, *if* we assume from the outset that the true value of the coefficient is zero. In this case, the p-value is `r scales::label_scientific()(summary(model_reviews)$coefficients[2, 4])`, which is very small. We can conclude that the coefficient is statistically different from zero, and that the feature is related to the target, at least statistically speaking.


[^coefstats]: Most statistical tables of this sort will use a t (student t distribution), Z (normal distribution), or F (F distribution) statistic.  It doesn't really matter for your purposes which is used by default, they provide the p-value of interest to claim statistical significance. 

[^pvaluet]: You can calculate this as `pt(stat, df = model degrees of freedom, lower=FALSE)*2` in R or `stats.t.cdf` in Python.  The model degrees of freedom are provided in the summary output (a.k.a. residual degrees of freedom)  `lower=FALSE` and `*2` are to get the two-sided p-value, which is what we want in this case. When it comes to t and Z statistics, anything over 2 is statistically significant by the common standard of a p-value of .05 or less. Note also that even though output will round it to zero, the true p-value can never be zero.

Aside from the coefficients, the most important output is the **confidence interval** (CI). The CI is a range of values that encapsulates the uncertainty we have in our guess about the coefficients.  While our best guess for the effect of word count on rating is `r round(wc_coef, 2)`, we know it's not exactly that, and the CI gives us a range of reasonable values we might expect the effect to be based on the data at hand and the model we've employed. In this case, the default is a 95% confidence interval, and we can think of the confidence interval like [throwing horseshoes](https://en.wikipedia.org/wiki/Horseshoes_(game)). If we kept collecting data and running models, 95% of our CIs would capture the true value, and this is one of them. That's the technical definition, which is a bit abstract[^whatisaci], but we can also think of it more simply as a range of values that are good guesses for the true value.  In this case, the CI is `r paste(rl_ci[1])` to `r paste(rl_ci[2])`, and we can be 95% confident that a good range for the coefficient is between those values. We can also see that the CI is relatively narrow, which is good, as it implies that we have a good idea of what the coefficient is.  If it was very wide, we would have a lot of uncertainty about the coefficient, and we would not likely not want to base important decisions regarding it.

[^whatisaci]: The interpretation regarding the CI is even more nuanced than this, but we'll leave that for another time. For now, we'll just say that the CI is a range of values that are good guesses for the true value. Your authors have used frequentist and Bayesian statistics for many years, we are fine with both of them, because they both work well enough in the real world. Despite where this ranged estimate comes from, the vast majority use CIs in the same way, and they are a useful tool for understanding the uncertainty in our estimates.



```{r}
#| echo: false
#| label: fig-ci-plot
#| fig-cap: Confidence Intervals as Horseshoes
# from https://en.wikipedia.org/wiki/Horseshoes_(game)
# knitr::include_graphics("img/horseshoes.jpeg")
```

![](img/horseshoes.jpeg){width='50%'}


Keep in mind that your model has a lot to say about what you'll be able to say at the feature level. As an example, as we get into machine learning models, you won't have as easy a time with coefficients and their confidence intervals. For now we'll stop here, but there is a lot more to the story when it comes to feature level interpretation, and we'll continue to return to the topic. But first, let's take a look at interpreting things in another way.


### Is it a Good Model?

```{r}
#| echo: false
#| label: model-metrics
# need to update later usage to use mr_metrics object or named with mr_prefix
mr_metrics = performance::performance(model_reviews)
rsq = round(mr_metrics$R2, 2)
rsq_perc = 100 * rsq
mse = round(mr_metrics$RMSE^2, 2)
rmse = round(mr_metrics$RMSE, 2)
mae = round(performance::performance_mae(model_reviews), 2)
aic = round(mr_metrics$AIC, 2)
```

Thus far, we've focused on interpretation at the feature level. But knowing the interpretation of a feature doesn't do you much good if the model itself is poor! In that case, we also need to assess the model as a whole, and as with the feature level, we can do this in a few ways. Before getting too carried away with asking whether your model is any good or not, you always need to ask your self *relative to what*?  Many model claim top performance, but are statistically indistinguishable from many other models. So we need to be careful about how we assess our model, and what we compare it to.  

First, we can start with the predictions of our model. As noted previously, how well the predictions and target line up is a measure of how well the model fits the data. Most model-level interpretation involves assessing and comparing model fit and variations on this theme. One of the better ways to assess model fit is visually, so let's look at our predictions vs. the target.

::: {.panel-tabset}

##### R

```{r}
#| echo: true
#| eval: false
#| label: prep-pp-plot-r
predictions = predict(model_reviews)
y = df_reviews$rating
```

##### Python

```{python}
#| echo: true
#| eval: false
#| label: prep-pp-plot-py

predictions = model_reviews.predict()
y = df_reviews.rating
```

:::


```{r}
#| echo: false
#| label: fig-pp-scatter
#| fig-cap: Predictions vs. Observed Ratings


p1 <- df_reviews |>
    mutate(pred = predict(model_reviews)) |>
    ggplot(aes(pred, rating)) +
    geom_point() +
    # geom_point(position = position_jitter()) +
    geom_abline() +
    labs(
        x = "Predicted Rating",
        y = "Observed Rating",
        subtitle = "Raw values"
    )

p2 <- df_reviews |>
    mutate(pred = predict(model_reviews)) |>
    ggplot(aes(pred, rating)) +
    geom_point(position = position_jitter()) +
    geom_abline() +
    labs(
        x = "Predicted Rating",
        y = "Observed Rating",
        subtitle = "Jittered values"
    )

p1 + p2

```

The one on the left is using the raw target and predictions- very stripey! The reason is that our ratings are only at the single decimal place precision, and our word count is at the integer level precision, so we have a lot of ties. The right side jitters the data randomly a bit so we can see a better pattern, but is otherwise the same.  In general, the closer to a line this plot becomes the better, so we can tell already there is still a lot of noise left to explain beyond our model.



#### Model Metrics

We've already discussed mean-squared error[^msermse], but there are other metrics we can use to assess **model fit**. As we noted, (R)MSE is a very popular measure for continuous targets, telling us the standard deviation of errors, or how much they bounce around on average. In our case, the value was `r rmse`.  Another metric we can use in this particular situation is the mean absolute error, which is similar to the mean squared error, but instead of squaring the errors, we just take the absolute value. Conceptually it attempts to get at the same idea, how much our predictions miss on average, and here the value is `r mae`, which we actually showed in our initial residual plot \@fig-residuals-plot. With either metric, the closer to zero the better, since as we get closer, we are reducing error.

[^msermse]: Any time we're talking about MSE, we're also talking about RMSE, but MSE is one less word/letter so will usually be our default. 

We can also look at the **R-squared** (R^2^)value of the model. R^2^ is possibly the most popular measure of model performance with linear regression and linear models in general. Before squaring, it's just the correlation of the values that we saw in the previous plot (@fig-pp-scatter). When we square it,  we can interpret it as a measure of how much of the variance in the target is explained by the model. In this case, our model shows the R^2^ is `r rsq`, which is pretty good for a single feature model. We interpret it that `r rsq_perc`% of the target is explained by our model. In addition, we can also interpret R^2^ as 1 - the prorportion of error varaince in the target, which we can calculate as $1 - \frac{\textrm{MSE}}{var(y)}$.  In other words the complement of R^2^ is the proportion of the variance in the target that is not explained by the model. Either way, our result suggests there is plenty of work left to do!

Note also, that with R^2^ we get a sense of the variance shared between *all* features in the model and the target, however complex the model gets. As long as we use it descriptively as a simple correspondence assessment of our predictions and target, it's a fine metric. For various reasons, it's not a great metric for comparing models to each other, but again, as long as you don't get carried away, it's fine.



### Prediction vs. Explanation

In your humble authors' views, one can't stress enough the importance of a model's ability to predict the target. It can be a poor model, maybe because the data is not great, or we're exploring a new area, but we'll always be interested in how well a model **fits** the observed data, and predicts new data.



<!---
 They are used to explain how the features are related to the target, and we may use terms like saying a feature 'strongly' correlates, or 'negatively' correlates to the target.  In our case, we can say that for every additional word in the review, we *expect* the rating to go up by `r wc_coef` stars.  We might even use our background knowledge and context to say this is a 'strong' or 'weak' relationship.   
--->
<!---
 
Additionally, the extra information in the table shows that word count is **statistically significant**, which is a very loaded description. For our purposes right now, we'll interpret it as follows: if we expected the coefficient to be zero, the probability of seeing a coefficient as large as or larger than the one in our model is very small. In fact, the probability is so small, we might conclude that the coefficient is not zero, and that the feature is related to the target. Unfortunately, statistical significance is affected by other things besides the size of the coefficient, and without an understanding of the context of the features (like how long typical reviews are, what their range is, what variability of ratings is, etc.), the information it provides is extremely limited, and many would argue, not even useful at all. 
--->

Even to this day, **statistical significance** is focused on a great deal, even to the point that a much hullabaloo is made about models that have no predictive power at all. As strange as it may sound, you can read whole journal articles, news articles, and business reports in many fields with hardly any mention of prediction. The focus is almost entirely on the **explanation** of the model, and usually the statistical significance of the features. In those settings, statistical significance is often used as a proxy for importance, which it never should be. Unfortunately, it is affected by other things besides the size of the coefficient, and without an understanding of the context of the features (like how long typical reviews are, what their range is, what variability of ratings is, etc.), the information it provides is extremely limited, and many would argue, not even useful at all. If we are very interested in the coefficient, it is better to focus on the range of possible values, which is provided by the **confidence interval**.  While a confidence interval is also a loaded description of a feature's relationship to the target, we can use it in a very practical way as a range of possible values for that weight, and more importantly, think of possibilities rather than certainties.

Suffice it to say at this point that how much one focuses on prediction vs. explanation depends on the context and goals of the data endeavor. There are cases where predictive capability is of utmost importance, and we care less about about explanatory details, but not to the point of ignoring it. For example, even with deep learning models for image classification, where the inputs are just RGB values, we'd still like to know what the (notably complex) model is picking up on, otherwise we may be classifying images based on background nonsense. In some business settings, we are very or even mostly interested in the coefficients/weights, which might indicate how to allocate resources in some fashion, but if they come from a model with no predictive power, this may be a fruitless endeavor.  

In the end we'll need to balance our efforts to suit the task at hand. Prediction and explanation are both fundamental to the modeling endeavor.



## Adding Complexity

We've seen how to fit a model with a single feature and interpret the results, and that helps us to get oriented to the process. However, we'll always have more than one feature for a model except under some very specific circumstances, such as exploratory data analysis. So let's see how we can do that with a model that makes more sense. 




### Multiple Features

We can add more features to our model very simply. Using the standard functions we've already demonstrated, we just add them to the formula (both R and statsmodels) as follows.

```{python}
#| eval: false
#| label: lm-extra-features-formula
'y ~ feature_1 + feature_2 + feature_3'
```

In other cases, additional features will just be the additional input columns. We might have a lot of features, and even for linear models this could be dozens in some scenarios. A compact depiction of our model uses the matrix representation, which we'll show in the callout below, and you can find more detail in the [matrix section](#matrix) overview. For our purposes, all you really need to know is that this:

$$
y = X\beta\qquad  \textrm{or}\qquad y = \alpha + X\beta
$$ {#eq-lm-XB}

is the same as this:

$$
y = \alpha + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 \dots
$$

where $X$ is a matrix of features[^interceptones], and $\beta$ is a vector of coefficients. Matrix multiplication allows us an efficient way to get our expected value/prediction. 

[^interceptones]: For linear regression as we estimate it here, there is actually an additional column at the beginning of the matrix that is all ones, which is a way to incorporate the intercept. However, most models that use a matrix as input will not have the intercept column, as it's not part of the model estimation or is estimated separately.




::: {.callout-note collapse="true" appearance="minimal"}
## Matrix Representation of a Linear Model

Here we'll show the matrix representation form of the linear model, for the typical case where we have more than one feature in the model. In the following, y is a vector of all target observations, and likewise each x is a (row) vector of all observations for that feature.  The b vector is the vector of coefficients.  The 1 serves as a means to incorporate the intercept. It's just a feature that always has a value of 1.  The matrix multiplication is just a compact way of expressing the sum of the features multiplied by their coefficients.  We can even do it more

Here is y as a vector of observations, n x 1.

$$
\textbf{y} = \begin{bmatrix}
y_1 \\
y_2 \\
\vdots \\
y_n
\end{bmatrix}
$$ {#eq-lm-mat-y}

Here is the vector for x, including the intercept:

$$
\textbf{X} = \begin{bmatrix}
1 & x_{11} & x_{12} & \dots & x_{1p} \\
1 & x_{21} & x_{22} & \dots & x_{2p} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
1 & x_{n1} & x_{n2} & \dots & x_{np}
\end{bmatrix}
$$ {#eq-lm-mat-x}

And finally, here is the vector of coefficients:

$$
\textbf{b} = \begin{bmatrix}
b_0 \\
b_1 \\
\vdots \\
b_p
\end{bmatrix}
$$ {#eq-lm-mat-b}


Putting it all together, we get the linear model in matrix form:

$$
\textbf{y = Xb }
$$ {#eq-lm-mat-mult}

:::

With that in mind, let's get to our model! In what follows, we keep the word count, but now we add some aspects of the reviewer, such as age and the number of children in the household, features related to the movie- the release year, the length of the movie in minutes, and the total reviews received. We'll also add another review level feature- the year the review was written.  We'll use the same approach as before, and literally just add them as we depicted in our linear model formula (@eq-lm-basic).

:::{.panel-tabset}

##### R

```{r}
#| label: lm-extra-features-r
model_reviews_extra = lm(
    rating ~
        word_count
        + age
        + review_year
        + release_year
        + length_minutes
        + children_in_home
        + total_reviews,
    data = df_reviews
)

summary(model_reviews_extra)
```

```{r}
#| echo: false
#| eval: false
#| label: python-condition-number
ev = eigen(vcov(model_reviews_extra))
sqrt(max(ev$values) / min(ev$values))
car::vif(model_reviews_extra)
```

##### Python

```{python}
#| label: lm-extra-features-py
model_reviews_extra = smf.ols(
    formula = 'rating ~ word_count + age + review_year + release_year + length_minutes + children_in_home + total_reviews',
    data = df_reviews
).fit()

model_reviews_extra.summary(slim = True)
```

:::


```{r}
#| echo: false
#| label: get-mre-output
mre_table = broom::tidy(model_reviews_extra, conf.int = TRUE) |>
    janitor::clean_names() |>
    rename(feature = term) |>
    mutate(feature = ifelse(feature == "(Intercept)", "intercept", feature)) |>
    mutate(
        estimate  = round(estimate, 4),
        std_error = round(std_error, 2),
        statistic = round(statistic, 2),
        p_value   = round(p_value, 4),
        conf_low  = round(conf_low, 2),
        conf_high = round(conf_high, 2)
    )

mre_intercept = mre_table |>
    filter(feature == "intercept") |>
    pull(estimate)

mre_coef = mre_table |>
    filter(feature != "intercept") |>
    select(feature, estimate) |>
    pivot_wider(names_from = feature, values_from = estimate) |>
    map_df(\(x) ifelse(round(x, 2) == 0, round(x, 4), round(x, 2)))

mre_metrics = round(performance::performance(model_reviews_extra), 2)
mre_mae = round(performance::performance_mae(model_reviews_extra), 2)
```

There is definitely more to unpack here but it's important to note that it's just *more* stuff, not different stuff.  The model-level components are the same in that we still see R^2^ etc., although they are all 'better' (higher R^2^, lower error) because we have a more predictive model. Our coefficents look the same also, and the fact is we'd interpret the in the same way.  Starting with word count, we see that it's still statistically significant, but it has been reduced just slightly from our previous model where it was the only feature (`r wc_coef` vs. `r mre_coef['word_count']`).  Why? This suggests that word count has some non-zero correlation, sometimes called **collinearity**, with other features that are also explaining the target to some extent. Our linear model shows the effect of each feature *controlling for other features*, or, *holding other features constant*[^controllingfor]. Conceptually this means that the effect of word count is the effect of word count *after* we've accounted for the other features in the model. In this case, an increase of a single word results in a `r round(mre_coef['word_count'], 2)` `r word_sign(mre_coef['word_count'], c('bump', 'drop'))`. Looking at another feature, the addition of a child to the home is associated with `r mre_coef['children_in_home']` `r word_sign(mre_coef['children_in_home'], c('bump', 'drop'))` in rating, accounting for the other features.

Thinking about prediction, how would we get a prediction for a movie rating with a review that is 12 words long, written in 2020, by a 30 year old with one child, for a movie that is 100 minutes long, released in 2015, with 10000 total reviews?  Conceptually we just plug in the values for each feature into the model equation exactly as we did for a single feature. But since that'd be tedious, we'll just use the `predict` function, which is a bit easier and what we'd normally do.

:::{.panel-tabset}

##### R

```{r}
#| label: predict-mre-r
predict_observation = tibble(
    word_count = 12,
    age = 30,
    children_in_home = 1,
    review_year = 2020,
    release_year = 2015,
    length_minutes = 100,
    total_reviews = 10000
)

predict(
    model_reviews_extra,
    newdata = predict_observation
)
```

##### Python

```{python}
#| label: predict-mre-py
predict_observation = pd.DataFrame(
    {
        'word_count': 12,
        'age': 30,
        'children_in_home': 1,
        'review_year': 2020,
        'release_year': 2015,
        'length_minutes': 100,
        'total_reviews': 10000
    },
    index = ['new_observation']
)

model_reviews_extra.predict(predict_observation)
```

:::

In our example we're just getting a single prediction, but don't let that hold you back! You can predict an entire data set if you want, and use any values for the features you want.  We'll do this explicitly in the [machine learning chapter](#machine-learning), but for now, try getting a prediction for a different set of values.


[^controllingfor]: A lot of statisticians and causal modeling folks get very hung up on the terminology here, but we'll leave that to them as we'd like to get on with things. For our purposes, we'll just say that we're interested in the effect of a feature *after* we've accounted for the other features in the model. 




### More Interpretation Tools

#### SHAP Values

Some models are more complicated than can be explained by a simple coefficient, e.g. nonlinear effects in generalized additive models, or may not even have feature-specific coefficients, like gradient boosting models, or may have many parameters associated with a feature, as in deep learning. Such models typically won't come with statistical output like standard errors and confidence intervals either. But we'll still have some tricks up our sleeve to help us figure things out!

A very common interpretation tool is called a **SHAP value**. SHAP stands for **SHapley Additive exPlanations**, and it provides a means to understand how much each feature contributes to a specific prediction. It's based on a concept from game theory called the **Shapley value**, which is a way to understand how much each player contributes to the outcome of a game. The reason we bring it up here is that it is has a nice intuition in the linear model case, and demonstrating now is a good way to get a sense of how it works. While the actual computations can be tedious, the basic idea is relatively straightforward- for a given prediction at a specific observation with set feature values, we can calculate the difference between the prediction at that observation and the average prediction. This is the **local effect** of the feature.  However, we must also consider doing this for all possible values of other features that might be in a model, as well as considering whether other features are present for the prediction or not. The initial Shapley approach is to average the local effects over all possible combinations of features, which is computationally intractable for all but the simplest models. The SHAP approach offers more computationally feasible methods for estimation which, while still computationally intensive, is doable for many models. The SHAP approach also has the benefit of being able to be applied to *any* model, and it's the approach we'll use here and return to with some of our other models.

Let's look at the SHAP values for our model.  We'll start with a single feature value/observation, using our multifeature model. Here we'll use the first observation where there are 12 words for word count, age of reviewer is 30, a movie length of 100 minutes etc. To aid our understanding, we calculate the shap value related to word count at that observation by hand, and using a package.

:::{.panel-tabset}

##### R

```{r}
#| eval: true
#| label: shap-values-r

# first we need to get the average prediction
avg_pred = mean(predict(model_reviews_extra))


# then we need to get the prediction for the feature value of interest
# for all observations, and average them
pred_observation = predict(
    model_reviews_extra,
    newdata = df_reviews |> mutate(word_count = 12)
)

# then we can calculate the shap value
shap_value = mean(pred_observation) - avg_pred

# we can also use the DALEX package to do this for us
explainer = DALEX::explain(model_reviews_extra, verbose = FALSE)

# observation of interest we want shap values for
obs_of_interest = tibble(
    word_count = 12,
    age = 30,
    children_in_home = 1,
    length_minutes = 100,
    total_reviews = 10000,
    release_year = 2015,
    review_year = 2020,
)

shap_value_package = DALEX::predict_parts(
    explainer,
    obs_of_interest,
    type = "shap"
)
```


##### Python

```{python}
#| eval: false
#| label: shap-values-py

# first we need to get the average prediction
avg_pred = model_reviews_extra.predict(df_reviews).mean()

# then we need to get the prediction for the feature value of interest
pred_observation = model_reviews_extra.predict(df_reviews.assign(word_count = 12))

# then we can calculate the shap value
shap_value = pred_observation.mean() - avg_pred


# now use the shap package for this; it does not work with statsmodels though,
# and single feature models are a bit cumbersome, but we still get there in the end!
import shap
from sklearn.linear_model import LinearRegression

# set data up for shap and sklearn
fnames = ['word_count', 'age', 'review_year', 'release_year', 'length_minutes', 'children_in_home', 'total_reviews']

X = df_reviews[fnames]
y = df_reviews['rating']

# use a linear model that works with shap
model_reviews = LinearRegression().fit(X, y)

# 1000 instances for use as the 'background distribution'
X_sample = shap.maskers.Independent(data = X, max_samples = 1000)  

# # compute the SHAP values for the linear model
explainer_linear = shap.Explainer(
    model_reviews.predict, 
    X_sample   
)

# find an index where word_count is 12
obs_of_interest = pd.DataFrame({
    'word_count': 12,
    'age': 30,
    'children_in_home': 1,
    'review_year': 2020,
    'release_year': 2015,
    'length_minutes': 100,
    'total_reviews': 10000
}, index = ['new_observation'])

shap_values_linear = explainer_linear(obs_of_interest)

shap_value = shap_values_linear.values[0, 0]
```

:::


```{r}
#| echo: false
#| label: tbl-shap-values-comparison
tibble(
    ours = shap_value,
    dalex = as_tibble(shap_value_package['word_count', ])[["contribution"]]
) |>
    gt(decimals = 3)
```

So we see the contribution to a prediction for a single feature, but the shap-related packages provide them for all features, and thus get a sense of each feature's contribution to the predicion. The following shows this as a visualization, either a force plot and waterfall plot. Smaller contributions are aggregated to one effect to simplify the plot. The dotted line represents the average prediction from our model (E[f(x)]) and the prediction we have for the observation (f(x)). We see that total reviews and length in minutes contribute most to the prediction at this observation, followed by release year.  We can also see that the effect of movie length is negative.

```{r}
#| echo: false
#| label: shap-viz-r
#| fig-cap: SHAP Visualizations
#| fig-height: 8
#| fig-width: 6
pp <- DALEX::predict_parts(
    explainer,
    obs_of_interest,
    type = "shap",
    # B = 1000 #  b won't matter since linreg
)

fc = okabe_ito[c(5,6)]

library(shapviz)

plot_dat_shap = shapviz(pp)

layout = '
    AAAAA
    #BBB#
'

bd = sv_force(
    plot_dat_shap, 
    fill_colors = fc, 
    max_display = 4,
    colour = "white", 
    contrast = FALSE
) +
    labs(
        x = "Breakdown",
        title = "SHAP Force"
    )

wf = sv_waterfall(
    plot_dat_shap,
    max_display = 4,
    fill_colors = fc,
    colour = "white",
    contrast = FALSE
) +
    labs(
        x = "Contribution to Prediction",
        title = "SHAP Waterfall"
    )

bd / wf +
    plot_layout(nrow = 2, widths = c(.25, 1), design=layout ) 

# sv_importance(plot_dat_shap) +
#     xlab("Importance")
```

Pretty neat huh? So for any observation we want to, and more importantly, for any model we might use, we can get a sense of how features contribute to that prediction.  We also can get a sense of how much each feature contributes to the model as a whole by aggregating these values across all observations in our data, and this provides a measure of **feature importance**, but we'll come back to that in a bit.

If we are concerned with a single feature's relationship with the target, we can also look at the **partial dependence plot** (PDP). The PDP shows the relationship between a feature and the target, but averaged over all other features. In other words, it shows the effect of a feature on the target, but averaged over all other features. For the linear case, it has a direct correspondence to the shap value. The SHAP value is the value the difference between the average prediction and the point on the PDP for a feature at a specific feature value. 

We can also look at the **individual conditional expectation** (ICE) plot, which is a PDP for a single observation.  The PDP is a very common plot for understanding the relationship between a feature and the target, and we'll see it again with other models.  The ICE plot is less common, but it's a nice way to see how a feature affects the target for a single observation.  

In addition, there are other plots that are similar to the PDP and ICE, such as the **accumulated local effect** (ALE) plot, which is a bit more robust to correlated features than the PDP plo. Where the PDP and ICE plots show the average effect of a feature on the target, the ALE plot focuses on average differences in predictions for the feature at a specific value versus predictions at feature values nearby, and centers the result so that the average difference is zero. We'll show all three here.

https://christophm.github.io/interpretable-ml-book/ (good reference for all plots)

```{r}
#| echo: false
#| label: pdp-ice-r
#| fig-cap: Partial Dependence and Individual Conditional Expectation Plots
#| fig-height: 10
# elements of this come from initial shap calculation demo

library(ggplot2)
library(iml)

layout = '
    AAAA
    AAAA
    AAAA
    BBCC
    BBCC
'

p_init = Predictor$new(model_reviews_extra, df_reviews)

pdp = FeatureEffect$new(p_init, 'word_count', method = "pdp")$plot() + 
    geom_hline(yintercept = mean(df_reviews$rating), linetype = "dashed") +
    geom_vline(xintercept = obs_of_interest$word_count, linetype = "dashed") +
    geom_segment(
        data = obs_of_interest, 
        aes(x = word_count, xend = word_count, y = mean(pred_observation), yend= mean(df_reviews$rating)), 
        color = okabe_ito[6],
        linewidth = 2
    ) +
    geom_point(
        data = obs_of_interest, aes(x = word_count, y =  mean(pred_observation) ), 
        color =okabe_ito[6], 
        alpha = 1,
        size = 4
    ) +
    annotate(
        "text", 
        x = obs_of_interest$word_count + 2, 
        y = mean(df_reviews$rating) + - .02, 
        label = "SHAP Value"
    ) +
    annotate(
        "text", 
        x = 3, 
        y = mean(df_reviews$rating), 
        size = 3,
        vjust = -1,
        label = 'Average Prediction'
    ) +
    annotate(
        "text", 
        x = 12, 
        y = 2.8, 
        size = 3,
        vjust = -1,
        angle = 90,
        label = "Word Count = 12"
    ) +
    labs(subtitle = 'PDP') 

ice = FeatureEffect$new(p_init, 'word_count', method = "pdp+ice")$plot() +
    # scale_color_manual(values = okabe_ito[5]) + 
    labs(subtitle = 'ICE')
ale = FeatureEffect$new(p_init, 'word_count', grid.size=100)$plot() + labs(subtitle = 'ALE')

pdp + ice + ale +
    plot_layout(design = layout) &
    theme(
        plot.subtitle = element_text(size = 14)
    ) 
```

Kinda cool but maybe not so interesting in that they all kind of tell us the same thing about our negative relationship between word count and rating, which we already knew from our coefficient value. The real power will come in later when we use interactions, nonlinear effects, and other models. But it's good to note now that the PDP, ICE, and ALE plots are a nice way to get a sense of the relationship between a feature and the target, and we'll see them again with other models.



```{r}
#| echo: false
#| eval: false
#| label: shap-demo-no-show

# just a quick demo to get all the shap values for the observation of interest
map2_df(
    .x = obs_of_interest,
    .y = colnames(obs_of_interest),
    \(x, y)
    mean(
        predict(model_reviews_extra, newdata = insight::get_data(model_reviews_extra) |> mutate(!!y := x)) - mean(predict(model_reviews_extra))
    )
) |>
    dplyr::select(sort(colnames(obs_of_interest)))

```

```{r}
#| echo: false
# saving for later
# explainer <- DALEX::explain(
#     model_reviews_extra,
#     data = insight::get_data(model_reviews_extra) |> sample_n(1000) |> dplyr::select(-rating),
#     y = df_reviews$rating
#     )

# pp <- DALEX::model_parts(
#     explainer,
#     observation_of_interest,
#     loss_function = DALEX::loss_default(explainer$model_info$type),
#     B = 100
# )
# pp |> plot()
```


### Feature Importance


How important is a feature? It's a common question, and one that is often asked of models, but the answer ranges from 'it depends' and 'it doesn't matter'. Let's start with some hard facts:

- There is no single definition of importance.
- There is no single metric for *any* model that will definitively tell you how important a feature is relative to others in all data/model contexts.
- There are many metrics for a given model that are equally valid, but may come to different conclusions.
- Any non-zero feature contribution is potentially 'imortant', however small.
- Many metrics of importance fail to adequately capture interactions and deal with correlated features.
- All measures of importance are measured with uncertainty, and the uncertainty can be large.
- Relative to... what? A poor model will still have relatively 'important' features, but they may not be useful.
- It rarely makes sense to drop features based on importance alone, and will typically drop performance to do so.
- In the end, what will you do with the information? 



To show just how difficult measuring feature importance is, we only have to stick with our simple linear regression. Think again about R^2^: it tells us the proportion of the target explained by our features. An ideal measure of importance would be able to tell us how much each feature contributes to that proportion, or in other words, decomposes R^2^ into the relative contributions of each feature. One of the most common measures of importance in linear models is the standardized coefficient we demonstrated earlier. You know what it doesn't do? Decompose R^2^. The easiest situation we could hope for with regard to feature importance is the basic linear model we've been using.  Everything is linear, with no interactions, or other things going on. And yet there are many logical ways to determine feature importance, and some even break down R^2^, but they won't necessarily agree with each other in ranking or relative differences. If you can get a measure of statistical difference between whatever metric you choose, it's often the case that 'top' features will not be statistically different from other features.  So what do we do? We'll show a few methods here, but the main point is that there is no single answer, and it's important to understand what you're trying to do with the information.

Let's start things off by returning to our SHAP value. If we take the average absolute shap for each feature, we get a sense of the typical contribution size for the features. We can then rank order them as accordingly. Here we see that the most important features here are the number of reviews and the length of the movie. Note that we can't speak to direction here, only magnitude.  We can also see that word count is relatively less important. 

```{r}
#| echo: false
#| label: shap-importance-bar
#| fig-cap: SHAP Importance


p_dat_shap = sv_importance(plot_dat_shap)$data

p_dat_shap |> 
    rename(contribution = value) |>
    mutate(
        feature = fct_reorder(feature, abs(contribution), .fun = mean),
        # feature = fct_rev(feature)
    ) |>
    ggplot(aes(contribution, feature)) +
    geom_col(
        aes(
            color = feature == 'word_count',
            fill  = feature == 'word_count'
        ),
        width = .05,
        show.legend = FALSE
    ) +
    scale_color_manual(
        values = c(okabe_ito[2], okabe_ito[1]),
        aesthetics = c("color", "fill")
    ) +
    ggnewscale::new_scale_color() +
    geom_point(
        aes(
            color = feature == 'word_count'
        ),
        alpha = 1,
        size = 5,
        show.legend = FALSE
    ) +
    scale_color_manual(
        values = c(okabe_ito[1], okabe_ito[2]),
    ) +
    labs(
        x = "Average Absolute SHAP Value/Contribution to Rating Prediction",
        y = "",
        title = "Feature Importance"
    ) +
    theme(
        axis.ticks.y = element_blank(),
        plot.caption = element_text(vjust = -2)
    )
```

Now here are some additional methods, some more reasonable than others, some which decompose R^2^ and those that do not. Aside from SHAP, the value represents the proportion of the R^2^ value that is attributable to the feature. The ones that truly decompose R^2^ are in agreement for the most part and seem to think highly of word count. The latter seem to be more varied, and only SHAP devalues word count, but possibly for good reason. Which is best? Which is correct? None. But we can get a sense at least that total reviews and length in minutes are likely useful features to our model.

```{r}
#| echo: false
#| eval: true
#| label: fig-importance
#| fig-cap: Feature Importance by Various Methods
#| fig-height: 8

# library(relaimpo) # do not call directly, imports MASS ffs
metrics = c("lmg", "last", "first", "betasq", "pratt", "genizi", "car")
init_rela_false = relaimpo::calc.relimp(
    model_reviews_extra,
    type = metrics,
    rela = FALSE
)

init_rela_true = relaimpo::calc.relimp(
    model_reviews_extra,
    type = metrics,
    rela = TRUE
)

df_rela = map_df(metrics, \(x) slot(init_rela_false, x)) |>
    t()
colnames(df_rela) = metrics

decomposer2 = names(which(round(colSums(df_rela), 3) == round(summary(model_reviews_extra)$r.squared, 3)))

pdat_rela = df_rela |>
    as_tibble(rownames = "feature") |>
    pivot_longer(-feature, names_to = "metric", values_to = "value") |>
    mutate(
        metric = fct_inorder(metric),
        true_decompose = ifelse(metric %in% decomposer2, "Decomposes R-squared", "Does not")
    ) |> 
    bind_rows(
        p_dat_shap |> mutate(metric = 'shap', true_decompose = 'Does not')
    )

pdat_rela |>
    arrange(true_decompose, metric, value) |>
    ggplot(aes(value, fct_inorder(feature))) +
    geom_col(
        aes(
            alpha = I(ifelse(true_decompose == "Decomposes R-squared", 1, .25)),
            color = feature == "word_count",
            fill  = feature == "word_count",
            width = I(ifelse(feature == "word_count", .5, .1)) # says it ignores this, but doesn't
        ),
        show.legend = FALSE
    ) +
    scale_color_manual(
        values = c(okabe_ito[2], okabe_ito[1]),
        aesthetics = c("color", "fill")
    ) +
    ggnewscale::new_scale_color() +
    geom_point(
        aes(
            # alpha = I(ifelse(true_decompose == 'Decomposes R-squared', 1, .25)),
            color = feature == "word_count",
            size = I(ifelse(feature == "word_count", 5, 4))
        ),
        alpha = 1,
        show.legend = FALSE
    ) +
    scale_color_manual(
        values = c(okabe_ito[1], okabe_ito[2]),
        # aesthetics = c("color", "fill")
    ) +
    # guides(color = 'none', fill = 'none', alpha = 'none') +
    labs(
        x = "Contribution to R-squared",
        y = "",
        title = "Relative Importance"
    ) +
    facet_wrap(vars(true_decompose, metric), scales = "free", ncol = 2, dir = "v") +
    theme(        plot.caption = element_text(vjust = -2)
    )

ggsave("img/fig-rsq-decomp.png", width = 8, height = 6)
```



### Model Level Interpretation

As before we can move beyond feature level interpretation, but we are still going to be concerned with the same sorts of questions - how well does the model fit? Have we impoved the fit significantly? What do our predictions look like, and so on.

As an example, we see that our R^2^ has gone up to `r mre_metrics$R2`, but this comes with an important caveat - adding any feature would increase our R^2^, even one that was pure noise! So while it is informative, we want to look at MSE or MAE to determine whether the model has truly improved. In both cases they've been reduced. For example, RMSE is now `r mre_metrics$RMSE` a reduction of `r scales::label_percent()(1 - mre_metrics$RMSE/mr_metrics$RMSE)`, so we can feel confident that our model has improved. While there is more a lot more to unpack, we will save doing so for the Model Criticism chapter. At least at this point, we have an idea of how to assess our this added complexity at the model level.

:::{.callout-note title='Adjusted R^2^'}
Part of the output contains an 'adjusted' R^2^.  This is a version of R^2^ that penalizes the addition of features to the model as a way to account for the fact that adding features will always increase R^2^, even if they are not useful. This is why we can't use R^2^ alone to determine whether a model has improved, and why we suggest only considering it as a descriptive statistic. But the adjusted version is kind of a hack, and can even be negative for very poor models. If you want to compare models, use MSE, MAE, and similar metrics.
:::


### Other Complexity

#### Categorical Features

Categorical features can be added to a model just like any other feature.  The main issue is that they have to be represented numerically, because models only work on numerically coded features and targets. The simplest and most common encoding is called a **one-hot encoding** scheme, which creates a new feature for each category, and assigns a 1 if the observation is in that category, and a 0 otherwise. This is also called a **dummy coding** when used for statistical models. Here is an example of what the coding looks like for the season feature. This is really all there is to it.

```{r}
#| label: one-hot-r
#| eval: true
#| echo: false

df_reviews |> 
    mutate(value = 1, idx = row_number()) |> 
    select(idx, rating, season, value) |> 
    tidyr::pivot_wider(
        id_cols = c(idx, rating),
        names_from = season,
        values_from = value
    ) |> 
    mutate(
        across(everything(), \(x) coalesce(x, 0)),
        season = df_reviews$season
    ) |> 
    select(rating, season, everything(), -idx)  |> 
    head(10) |>
    gt() |> 
    fmt_number(
        columns = vars(-rating, -season),
        decimals = 0
    )

```

When using statistical models we don't have to do this ourselves. Even other tools for machine learning models will typically have a way to identify and appropriately handle categorical features, even in very complex ways when it comes to deep learning models.  What is important is to be aware that they require special handling, but often this is done behind the scenes.  Now let's do a quick example using a categorical feature with our data, and we'll keep a numeric feature as well just for consistency.

:::{.panel-tabset}

##### R

```{r}
#| eval: true
#| label: cat-feature-r

model_cat = lm(
    rating ~ word_count + season,
    data = df_reviews
)

summary(model_cat)
```

##### Python

```{python}
#| eval: true
#| label: cat-feature-py

model_cat = smf.ols(
    formula = "rating ~ word_count + season",
    data = df_reviews
).fit()

model_cat.summary()
```

:::

We now see the usual output. There is word count again, with its slightly negative assocation with rating. And we have an effect for each season as well... except, wait a second, where is the fall effect? The coefficients are interepreted the same way - as we move one unit on x, we see a corresponding change in y. But moving from one category to another requires starting at some category in the first place! So one is chosen arbitrarily, but you would have control over this.  In our model, fall is chosen because its first alphabetically.  So if we look at say, the effect of summer, we see an increase in the rating of `r round(coef(model_cat)[['seasonSummer']], 2)` relative to fall. 

A better approach to understanding categorical features for standard linear models is through what are called **marginal effects**, which can provide a kind of average prediction for each category while accounting for the other features in the model. Better still is to visualize these, and we can use something like our PDP approach from before to do so[^mepy]. It's actually tricky to define 'average' when there are multiple features and interactions involved, so be careful, but we'd interpret the result similarly. In this case, we expect higher ratings for summer releases.

```{r}
#| echo: false
#| label: cat-feature-viz-r
#| fig-cap: Marginal Effects of Season on Rating
init = marginaleffects::marginal_means(model_cat, 'season')

broom::tidy(init) |> 
    ggplot(aes(x = value, y = estimate)) +
    geom_linerange(
        aes(ymin = conf.low, ymax = conf.high),
        color = okabe_ito[1],
        size = 0,
        linewidth = 2
    ) + 
    geom_point(
        color = okabe_ito[2],
        size = 5,
        alpha = 1
    ) +
    labs(
        x = "",
        y = "Rating",
        title = "Marginal Means"
    ) +
    theme(
        panel.grid.major.y = element_line(color = 'gray92'),
    )
```


[^mepy]: At the time of this writing, there seems to be very little for this sort of thing in Python. statsmodels provides limited functionality, but only for logistic regression models. In R you have various tools like `marginaleffects`, `emmeans`, `ggeffects` and more.



#### Interactions (a preview)

We'll demonstrate this more in the [extensions chapter](#extensions), but a common way to add complexity in linear models is through interactions.  This is where we allow the effect of a feature to vary depending on the values of another feature, or even itself! As a conceptual example, we might expect that the effect of the number of children in the home on rating is different for movies from different genres (much higher for kids movies, maybe lower for horror movies), or that genre and season work together in some way to affect rating (e.g. action movies get higher ratings in summer).  We might also consider that the length of a movie might plateau or even have a negative effect on rating after a certain point, i.e. having a curvilinear effect. All of these are types of interactions we can explore. Interactions allow us to incorporate nonlinear relationships into the model, and so greatly extend the linear model's capabilities - we basically get to use a linear model in a nonlinear way!  




## Assumptions and More

MOVE BULK OF THIS TO MODEL CRITICISM??

Every model you use has underlying assumptions which, if not met, could potentially result in incorrect inferences. The standard linear regression model we've shown is no different, and it has a number of assumptions that must be met for it to be *statistically* valid. Briefly they are:

- That your model is not grossly misspecified (e.g., you've included the right features and not left out important ones)
- The data your modeling reflects the population you want to make generalizations about
- The model is linear in the parameters (i.e. no $\beta_1*e^\beta_2$ type stuff)
- The features are not correlated with the error (prediction errors)
- Your data observations are independent of each other
- The prediction errors are homoscedastic (don't have large errors with certain predictions vs low with others)
- Normality of the errors (i.e. your prediction errors). Another way to put it is that your target variable is normally distributed conditional on the features.

Some of these are more important than others depending on what you're trying to do with the model, so 

Things it does not assume:

- That the features are normally distributed
    - For example, using categorical features is fine
- That the relationship between the features and target is linear
    - Interactions, polynomial terms, etc. are all fine
- That the features are not correlated with each other
    - They usually are
- That all linear models have these assumptions

If you don't meet these assumptions, it doesn't mean:

- That your model will have poor predictions 
- That your conclusions will necessarily be incorrect

If you do meet those assumptions, your coefficient estimates are unbiased[^unbiased], and in general, your statistical inferences are correct ones. If you don't meet them, there are alternative versions of the linear model you could use that would get around the problem. For example, data that runs over a sequence of time (**time series** data) violates the independence assumption, but we would use a **time series** or similar model instead. If normality is difficult to meet, you could assume a different data generating distribution.  We'll discuss some of these in the [extensions chapter](#extensions), but it's also important to note that not meeting the assumptions may only mean you'll prefer a different type of linear or other model to use for the data.  Our opinion is that not meeting the assumptions often is the result of a poor model, e.g. using poor features in an **underspecified** way (e.g. not including interactions).

[^unbiased]: This means they are correct on average, not the true value. And if they were biased, this is statistical bias, and has nothing to do with the moral or ethical implications of the data, or whether the features themselves are biased in measurement. Culturally biased data is a different problem than statistical/prediction bias or measurement error, though they are not mutually exclusive. The latter can more readily be tested, while the former is usually more difficult to assess.  If our moview reviews only came from a website with a paywall, they would be biased if we wanted to use them to refer to general public opinion. Our model results are unaffected by this scenario, as long as they are used to generalize only to that population of people who pay for the website.  If we wanted to generalize to the general public, we would need to account for this bias in some way, or use a different data source.  MOVE THIS TO SOME OTHER CHAPTER (DATA?)

On top of not meeting the assumptions, we may in fact introduce bias to get better prediction!  For example, we might use a **penalized regression** model to reduce the variance in our predictions, at the cost of introducing bias in the coefficients. We'll talk more of this in the [machine learning](#machine-learning) chapter, but suffice it to say for now, if you are more interested in prediction, you may be less interested in the statistical assumptions of the basic linear model.

### More Complex Models

Let's say your running some XGBoost or Deep Linear Model X and getting outsanding predictions. Assumption smumptions you say! And you might even be right! But if you want to talk confidently about feature contributions, or know something about the uncertainty in the predictions (which you're assessing right?) well, maybe you might want to know if you're meeting your assumptions. Some of them are:

- You have enough data to make the model generalizable
- Your data isn't biased (e.g., you don't have 90% of your data from one region when you want to talk about a whole area)
- You adequately sampled the hyperparameter space (e.g. you didn't just use the defaults or a small grid search)
- Your observations are independent or at least **exchangeable** and don't have **data leakage**, or you are explicitly modeling spatio-temporal dependence
- That all the parameter settings you set are correct or at least viable (e.g. you let the model run for a long enough set of iterations, your batch size was adequate, you had enough hidden layers, etc.)

And if you want to talk about specific feature contributions:

- The features are largely uncorrelated
- The features largely do not interact (but then why are you doing a complex model that is inherently interactive), or that your understanding of feature contribution deals with the interactions


Yeah... so, sorry to say, using non-statistical models doesn't mean you don't have to worry about assumptions, you still have some of the old stuff and some new ones to boot.




Model criticism and diagnostics, but briefly note something here.

<!---
 performance::check_model(model_reviews_extra) 
--->

## Classification

We've been using a continuous target, but what about a categorical target? For example, what if we just had a binary target of whether a movie was good or bad? We will dive much more into classification models in our upcoming chapters, but it turns out that we can still formulate it as a linear model problem. The main difference is that we use a transformation of our linear combination, sometimes called a **link function**, and we'll need to use a different **objective function** rather than least squares, such as the binomial likelihood, to deal with the binary target. This also means we'll move away from R^2^ as a measure of model fit, and look at something something else, like accuracy. 

Graphically we can see it in the following way, which when compared with our linear model, doesn't look much different.  In what follows, we create our linear combination $\mu$ and put it through the sigmoid function $\sigma$, which is a common link function for binary targets[^revlogit].  The result is a probability, which we can then use to classify the observation as good or bad based on a chosen threshold.  We'll see this in more detail in the classification chapter.

[^revlogit]: The sigmoid function in this case is the inverse logistic function, and the resulting statistical model is called logistic regression. In other contexts the model would not be a logistic regression, but this is still a very commmonly used *activation function*. But many others could potentially be used e.g. using a normal instead of logistic distribution, resulting in the so-called probit model. 


```{mermaid}
%%| echo: false
%%| eval: true
%%| fig-cap: Linear Model with Transformation

graph LR
    n1((X<sub>1</sub>)) --> |b<sub>1</sub>| n4((Âµ))
    n2((X<sub>2</sub>)) --> |b<sub>2</sub>| n4((Âµ))
    n3((X<sub>3</sub>)) --> |b<sub>3</sub>| n4((Âµ))


    n4 --> n5(("Ïƒ(Âµ)"))
    n5 --> n6((Y))

    style n1 fill:#f9f,stroke:#333,stroke-width:4px
    %% style e3 font-size:50%

```

As soon as we move away from the standard linear model and use transformations of our linear predictor, simple coefficient interpretation becomes difficult, sometimes exceedingly so. What will help 


## More linear models

Before we leave our humble linear model, let's look at some others

Generalize Linear Models and related

- True GLM e.g. logistic, poisson
- Other distributions: beta regression, tweedie, t (so-called robust), truncated
- Penalized regression: ridge, lasso, elastic net
- Censored outcomes: Survival models, tobit

Multivariate/multiclass/multipart

- Multivariate regression (multiple targets)
- Multinomial/Categorical/Ordinal regression (>2 classes)
- Zero (or some number) -inflated/hurdle/altered
- Mixture models and Cluster analysis

Random Effects

- Mixed effects models (random intercepts/coefficients)
- Generalized additive models (GAMMs)
- Spatial models (CAR)
- Time series models (ARIMA)
- Factor analysis

Latent Linear Models

- PCA, Factor Analysis
- Mixture models
- Structural Equation Modeling, Graphical models generally

All of these are explicitly linear models or can be framed as such, and most are either identical in description to what you've already seen or require only a tweak or two - e.g. a different distribution, a different link function, penalizing the coefficients, etc. In other cases, we can bounce from one to the another. For example we can reshape our multivariate outcome to be amenable to a mixed model approach, and get the exact same results. We can potentially add a random effect to any model, and that random effect can be based on time, spatial or other considerations.  The important thing to know is that the linear model is a very flexible tool, and allows you to model most of the types of outcomes were interested in. As such, it's a very powerful tool.

What about nonlinear models?



## Commentary

Linear models are a very popular tool for data analysis, and for good reason.  They are relatively easy to understand, and they are very flexible.  They can be used for prediction, explanation, and inference, and they can be used for a wide variety of data types.  They are also very easy to interpret, and there are many tools at our disposal to help us understand them.  But they are not without their limitations, and we'll discuss some of those here.

- Opinions
- Limitations/Failure points
- Summary

### Choose your own adventure

Now that you've got the basics, where do you want to go?

- If you want a deeper dive into how we get the results from our model: head [Estimation](#estimation)
- If you want to know more about how to understand the model: head to  [Model Criticism](#model-criticism)
- If you want to do some more modeling: go to [Extensions](#extensions) or [Machine Learning](#machine-learning)
- Got more data questions? Go to the [Data chapter](#data)


## Quick Exercise

- Import X Data. Stick with the current data if you want, just try out other features, or maybe try the world happiness data 2018 data . You can find details about it in the appendix  ADD LINK.
- Fit a linear model, try to keep it to no more than three features.
- Get all the predictions for the data, and try at least
- Interpret the coefficients
- Assess the model fit

```{r}
#| eval: false
#| echo: false
load('data/R/world_happiness.RData')

happy |> 
    filter(year == max(year)) |> 
    select(-(democratic_quality:gini_index_world_bank_estimate)) |> 
    drop_na() |> 
    write_csv('data/world_happiness_2018.csv')


```


 # refs to add

Statistical modeling rigorous:
Econometrics books like Wooldrigde, Greene, etc.

Basic DS?

Assumptions:
https://statmodeling.stat.columbia.edu/2013/08/04/19470/