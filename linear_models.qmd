# The Foundation

> It is the chief characteristic of data science that it works.
â€• Isaac Asimov (paraphrased)

```{r, include = FALSE}
source("load_packages.R")
source("setup.R")

reticulate::use_condaenv("book-of-models")
```

Packages needed or useful for this chapter include:

```{r, include = FALSE}
library(tidyverse)
```




## Introducing the Greatest Of All Time

Now that you have some idea of what you're getting into, it's time to dive in! We'll start things off by covering the building block of all modeling, and a solid understanding here will provide you the basis for the rest of what comes after, no matter how complex it gets. The **linear model** is our starting point. At first glance, it may seem like a very simple model, but it's actually quite powerful and flexible, able to take in different types of inputs, handle nonlinear relationships, temporal and spatial relations, clustering, and more. Linear models have a long history, with even the formal and scientific idea behind correlation and linear regression being well over a century old[^corbypeirce]! And in that time, the linear model is far and away the most used model out there. But before we start talking about the *linear* model, we need to talk about what a **model** is in general.


[^corbypeirce]: Peirce & Bowditch were well ahead of Pearson and Galton [@rovine2004peirce].


### What is a Model?

At its core, a model is just an **idea**. It's a way of thinking about the world, about how things work, about how things are related to each other, about how things change over time, how things are different from each other, and how they are similar. The underlying thread here is that **a model expresses relationships** about things in the world around us.  And just like other ideas, models have consequences in the real world, and they can be used prudently or foolishly. 

On a practical level, the model is expressed through a particular language, math, but don't let that worry you if you're not so inclined. As it's still just an idea at its core, the idea is the most important thing to understand about a model. The **math is just a way of expressing the idea** in a way that can be communicated and understood by others in a standard way, as well as helping make the idea precise. But in everyday terms, we're trying to understand things like how the amount of sleep relates to congnitive functioning, how the weather affects the number of people who visit a park, how much money to spend on advertising to increase sales, how to detect fraud, and so on.  Any of these  could form the basis of a model, as they stem from scientifically testable ideas, and they all express relationships between things we are interested in, possibly even with an implication of causal relations. 

If you wanted to run a linear model to understand the relationship between sleep and cognitive functioning, you might express it as:

:::{.panel-tabset}

##### R

```{r}
#| eval: false
lm(cognitive_functioning ~ sleep)
```

##### Python

```{python}
#| eval: false
from statsmodels.formula.api import ols

model = ols('cognitive_functioning ~ sleep', data=df).fit()
```

:::


Very easy! But that's all it takes to express a simple linear relationship.  In this case, we're saying that cognitive functioning is a linear (function) of sleep.  By the end of this chapter you'll also know why R's function is `lm` and the [statsmodels]{.pack} function is `ols`. 

<!-- 
We can also express this as a simple equation:

$$ 
\textrm{cog\_func} = \beta_0 + \beta_1 \cdot \textrm{sleep}
$$

But we'll get more into that later. -->


## Key ideas

We can pose a few concepts key to understanding models. This is not an exhaustive list, but it's a good start.  We'll cover each of these in turn.

- Features, targets, and input-output mappings
- Model estimation
- Prediction
- Assumptions, probabilistic outcomes, uncertainty



### Features and Targets

In the context of a model, how we specify the nature of the relationship depends on the context. In the interest of generality, we'll refer to the **target** as what we want to explain, and **features** as those aspects of the data we will use to explain it. Because people come at data from a variety of contexts, they often use different terminology to mean the same thing. Some of these actually suggest a particular type of relationship (e.g., a causal relationship, an experimental setting), but we'll ignore that for now.  The table below shows some of the common terms used to refer to features and targets.

```{r tbl-feat-target, cache=FALSE}
#| echo: false
#| label: tbl-feature-target-names
#| tbl-cap: Common Terms for Features and Targets


tbl_feat_targ = tibble(
    Feature = c("independent variable", "predictor variable", "explanatory variable", "covariate", "x", "input", "right-hand side"),
    Target  = c("dependent variable", "response", "outcome", "label", "y", "output", "left-hand side"),
) |>
    gt() |>
    rm_caption() # does nothing

tbl_feat_targ
```

We may use any or all of these words to describe things so that you are comfortable with the terminology, but we'll stick with **features** and **targets** for the most part.  In our opinion, this terminology has the least hidden assumptions/implications.


### Expressing Relationships


As noted,  a model is a way of expressing a relationship between a set of features and a target, and one way of expressing them is as inputs and outputs. But how can we go from input to output?  Well to begin, we assume that the features and target are **correlated**, i.e. that there is some relationship between the x and y.  If so, then we can ulitmately use the features to **predict** the target.  In the simplest setting a correlation implies a  relationship where x and y typically move up and down together (left plot) or they move in opposite directions where x goes up and y goes down.

```{r corr-plot}
#| echo: false
#| label: fig-corr-plot
#| fig-cap: Correlation

p_dat = tibble(
    x = rnorm(50),
    y = .75 * x + rnorm(50, sd = .5),
    yneg = -.75 * x + rnorm(50, sd = .5)
)

p1 = ggplot(p_dat, aes(x, y)) +
    geom_point() +
    labs(subtitle = "Positive Correlation")

p2 = ggplot(p_dat, aes(x, yneg)) +
    geom_point() +
    labs(subtitle = "Negative Correlation")

p1 + p2
```

In addition, the typical correlation suggests a linear relationship.  There are many types of correlation metrics, but the most common one, the Pearson correlation, is explicitly a measure of the linear relationship between two variables.  It's expressed as a number between -1 and 1, where 0 means there is no linear relationship. As we move closer to 1, we would see a tighter scatterplot like the one on the left, until it became a straight line.  The same happens for the negative relationship as we get closer to a value of -1.  If we have only one feature and target, the Pearson correlation reflects the exact result of the linear model we'd conduct.  Even with multiple features, we often lean a version of the Pearson R to help us understand how the features account for the target's variability.

```{r fig-corr-line-plot}
#| echo: false
p1 = p1 + geom_smooth(method = "lm", se = FALSE)
p2 = p2 + geom_smooth(method = "lm", se = FALSE)
p1 + p2
```



### *THE* Linear Model




The linear model is perhaps the simplest *functional* model we can use to express a relationship between features and targets.  It's possibly still the most common model used in practice, and it is the basis for many types of models.  Why don't we run one now?

:::{.panel-tabset}
##### R

```{r my-first-model}
#| eval: false
#| label: r-my-first-model
library(tidyverse)

df = read_csv("")

my_model = lm(y ~ x, data = df)

summary(my_model)
```


##### Python

```{python}
#| eval: false
#| label: py-my-first-model
#| code-fold: true
import pandas as pd
import statsmodels.formula.api as smf

df = pd.read_csv('')
my_model = smf.ols('y ~ x', data = df).fit()

my_model.summary()
```

:::






The linear model posits a **linear combination** of the features.  A linear combination is just a sum of the features, each of which has been multiplied by some constant value.  In the case of the linear model, the constant is often called a **coefficient** or possibly **weight**.  The linear model is expressed as (math incoming!):

$$y = b_0 + b_1x_1 + b_2x_2 + ... + b_nx_n$$

- y is the target
- x1, x2, ... xn are the features
- b0 is the intercept
- b1, b2, ... bn are the coefficients



But lets start with something simpler, let's say you want to take a sum of several features. In OMG MATH you would write it as:

$$x_1 + x_2 + ... + x_n$$

In the previous equation, x is the feature and n is the number of features. `x` is an arbitrary designation, you could use any letter, symbol you want, or better would be the actual name. Now look at the linear model.


$$y = x_1 + x_2 + ... + x_n$$

In this case, the function is *just a sum*, something so simple we do it all the time. In the linear model sense though, we're actually saying a bit more. Another way to understand that equation is that *y is a function of x*.  We don't show any coefficients, but technically it's as if each coefficient was a value of `1`.  In other words, for this simple linear model, we're saying that each feature contributes equally to the target. 

In the linear model, we're saying that each feature contributes to the target, but in practice feature never contribute in the same way. If we want to relate some cool x1 and some other cool x2 to hot target y, we probably would not assume that they both contribute in the same way.  We might give more weight to cool x1 than cool x2.  In the linear model, we express this by multiplying each feature by a differnt coefficient.  So the linear model is really just a sum of the features multiplied by their coefficients.  In math, we would write it as:

$$y = b_0 + b_1x_1 + b_2x_2 + ... + b_nx_n$$

For starters, features are typically on different scales

In fact, we're saying that each feature contributes to the target in proportion to the coefficient.  So if we have a feature x1 and a coefficient b1, then the contribution of x1 to the target is b1*x1.  If we have a feature x2 and a coefficient b2, then the contribution of x2 to the target is b2*x2.  And so on.  So the linear model is really just a sum of the features multiplied by their coefficients.  In math, we would write it as:


> In chapter 0, maybe say something about science = prediction, but maybe also stay away from academic treament


Here is a matrix multiplication formula:
$$
\begin{bmatrix} 1 & x_1 & x_2 & ... & x_n \end{bmatrix} \begin{bmatrix} b_0 \\ b_1 \\ b_2 \\ ... \\ b_n \end{bmatrix}
$$ {#eq-lm-mat-mult}


Here is test reference to the previous table See @tbl-feature-target-names . But this equation reference wroks work in quarto (@eq-lm-mat-mult). does this figure @fig-corr-plot work?  How about this one @fig-corr-plot2?

```{r corr-plot2}
#| echo: false
#| label: fig-corr-plot2
#| fig-cap: Correlation


plot(rnorm(10))
```


### Prediction

Once we have a working model, we can use it to make predictions.  We can do this by plugging in values for the features.  For example, if we have a model that predicts the price of a house based on the number of bedrooms and the square footage, we can plug in the number of bedrooms and the square footage for a house we're interested in and get a predicted price.  When we're talking about predictions we usually will see this as:

$$\hat{y} = b_0 + b_1x_1 + b_2x_2 + ... + b_nx_n$$

Where $\hat{y}$ is the predicted value of the target.  The hat over the $y$ just means that it's a predicted value.  We can also express this as a matrix multiplication:

$$\hat{y} = \begin{bmatrix} 1 & x_1 & x_2 & ... & x_n \end{bmatrix} \begin{bmatrix} b_0 \\ b_1 \\ b_2 \\ ... \\ b_n \end{bmatrix}$$



### Estimation

But how do we know what the coefficients are?  We can guess, but how would we know what to guess? How would we know which guesses are better than others?  There's got to be a better way, and there is- we need to estimate them!


#### Ordinary Least Squares

We can do this in a few ways, but the most common is to use the **Ordinary Least Squares (OLS)** method.  OLS is a method of estimating the coefficients that minimizes the sum of the squared errors.  In other words, it finds the coefficients that minimize the sum of the squared differences between the predicted values and the actual values.  We can express this as:

$$\sum_{i=1}^{n} (y_i - \hat{y_i})^2$$

Where $y_i$ is the actual value of the target for observation $i$, and $\hat{y_i}$ is the predicted value of the target for observation $i$.  The hat over the $y$ means that it's a predicted value.  The sum of the squared errors is also called the **residual sum of squares** (RSS).  The OLS method finds the coefficients that minimize the RSS, or in other words, it finds the coefficients that minimize the sum of the squared differences between the predicted values and the actual values.  

#### Maximum Likelihood 

#### Estimation as 'Learning'

#### Gradient Descent?

#### Other Loss Functions Quick Demo


### Assumptions and More

## Commentary

- Opinions
- Limitations/Failure points
- Summary