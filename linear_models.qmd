## The Foundation {-}

```{r, include = FALSE}
source('load_packages.R')
source('setup.R')
```

Intro

Key ideas

- Features, targets
- Input-output mappings
- Model Estimation
- Prediction
- Assumptions/Probabilistic outcomes/Uncertainty

Estimation

- OLS/MSE
- Maximum Likelihood (including logistic/poisson)
- Estimation as ‘Learning’
- Other Loss Functions Quick Demo

Commentary

- Opinions
- Limitations/Failure points
- Summary

### Introducing the Linear Model  (SOMETHING SNAPPIER)


Now that you have some idea of what you're getting into, it's time to dive in! In this chapter we're going to cover the building block of all modeling, and a solid understanding here will provide you the basis for the rest of what comes after, no matter how complex it gets. The **linear model** is our starting point. At first glance, it may seem like a very simple model, but it's actually quite powerful and flexible, able to take in different types of inputs, handle nonlinear relationships, temporal and spatial relations and more.  But before we start talking about the linear model, we need to talk about what a **model** is.

#### What is a Model?

At its core a model is just an **idea**. It's a way of thinking about the world, about how things work, about how things are related to each other, about how things change over time, how things are different from each other, how they are the same. The underlying thread here is that **a model expresses relationships**.  On a practical level, the model is expressed through a particular language, math, but don't let that worry you if you're not so inclined. As it's still just an idea at its core, that's the most important thing to understand about a model.  The **math is just a way of expressing the idea** in a way that can be communicated and understood by others in a standard way, helping make the idea precise.  In general, we're trying to understand how ~~A~~ relates to ~~B~~, ~~inputs~~ produce ~~outputs~~. CHANGE THESE TERMS TO EVERYDAY EXAMPLES




#### Features and Targets

In the context of a model, how we specify the nature of the relationship depends on the context. In the interest of generality, we'll refer to the **target** as what we want to explain, and **features** as those aspects of the data we will use to explain it. Because people come at data from a variety of contexts, they often use different terminology to mean the same thing. Some of these actually suggest a particular type of relationship (e.g., a causal relationship, an experimental setting), but we'll ignore that for now.  The table below shows some of the common terms used to refer to features and targets.

```{r tbl-feat-target, cache=FALSE}
#| echo: false
#| tbl-cap: Common Terms for Features and Targets


tbl_feat_targ = tibble(
    Feature = c('independent variable', 'predictor variable', 'explanatory variable', 'covariate', 'x', 'input'),
    Target  = c('dependent variable', 'response', 'outcome', 'label', 'y', 'output'),
    ) |> 
    gt() |> 
    rm_caption() # does nothing

tbl_feat_targ #|> as_latex()
```

We may use any or all of these words to describe things so that you are comfortable with the terminology, but we'll stick with **features** and **targets** for the most part.


## From Input to Output

As noted,  a model is a way of expressing a relationship between a set of features and a target, and one way of expressing them is as inputs and outputs. But how can we go from input to output?  Well to begin, we assume that the features and target are **correlated**, i.e. that there is some relationship between the x and y.  If so, then we can ulitmately use the features to **predict** the target.  In the simplest setting a correlation implies a  relationship where x and y typically move up and down together (left plot) or they move in opposite directions where x goes up and y goes down.

```{r corr-plot}
#| echo: false
p_dat = tibble(
    x = rnorm(50),
    y = .75*x + rnorm(50, sd = .5),
    yneg = -.75*x + rnorm(50, sd = .5)
)

p1 = ggplot(p_dat, aes(x, y)) + geom_point() + labs(title = 'Positive Correlation')
p2 = ggplot(p_dat, aes(x, yneg)) + geom_point() + labs(title = 'Negative Correlation')

p1 + p2
```

- correlation
- simple sum (and as math equation)
- simple regression equation







