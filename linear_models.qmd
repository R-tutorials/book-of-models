# The Foundation

> It is the chief characteristic of data science that it works.
â€• Isaac Asimov (paraphrased)

```{r setup-lm, include = FALSE}
source("load_packages.R")
source("setup.R")
source("functions/utils.R")

reticulate::use_condaenv("book-of-models")
```

Packages needed or useful for this chapter include:


:::{.panel-tabset}
##### R

```{r}
```{r}
#| echo: true
#| eval: false
#| label: packages

library(tidyverse)
```

```{r}
#| echo: false
#| label: r-display-setup

options(digits = 4) # number of digits of precision for floating point output

```

##### Python


```{python}
#| echo: false
#| label: modules
import numpy as np
import pandas as pd
import statsmodels.formula.api as smf
```

```{python}
#| echo: false
#| label: py-display-setup

np.set_printoptions(precision=4, suppress=True) # suppress is for whether to use scientific notation for otherwise rounded to zero numbers
pd.set_option('display.precision', 4) # number of digits of precision for floating point output
```

:::



## Introducing the Greatest Of All Time

Now that you have some idea of what you're getting into, it's time to dive in! We'll start things off by covering the building block of all modeling, and a solid understanding here will provide you the basis for just about anything that comes after, no matter how complex it gets. The **linear model** is our starting point. At first glance, it may seem like a very simple model, but it's actually quite powerful and flexible, able to take in different types of inputs, handle nonlinear relationships, temporal and spatial relations, clustering, and more. Linear models have a long history, with even the formal and scientific idea behind correlation and linear regression being well over a century old[^corbypeirce]! And in that time, the linear model is far and away the most used model out there. But before we start talking about the *linear* model, we need to talk about what a **model** is in general.


[^corbypeirce]: Peirce & Bowditch were well ahead of Pearson and Galton [@rovine2004peirce].


### What is a Model?

At its core, a model is just an **idea**. It's a way of thinking about the world, about how things work, how things change over time, how things are different from each other, and how they are similar. The underlying thread here is that **a model expresses relationships** about things in the world around us. One can also think of a **model as a tool**, one that allows us to take information, in the form of data, and act on it in some way. Just like other ideas (and tools), models have consequences in the real world, and they can be used wisely or foolishly.  

On a practical level, a model is expressed through a particular language, math, but don't let that worry you if you're not so inclined. As it's still just an idea at its core, the idea is the most important thing to understand about a model. The **math is just a way of expressing the idea** in a way that can be communicated and understood by others in a standard way, as well as helping make the idea precise. But in everyday terms, we're trying to understand things like how the amount of sleep relates to cognitive functioning, how the weather affects the number of people who visit a park, how much money to spend on advertising to increase sales, how to detect fraud, and so on.  Any of these  could form the basis of a model, as they stem from scientifically testable ideas, and they all express relationships between things we are interested in, possibly even with an implication of causal relations. 

If you wanted to run a linear model to understand the relationship between sleep and cognitive functioning, you might express it in code as:

:::{.panel-tabset}

##### R

```{r}
#| eval: false
#| label: lm-sleep-cog-func
lm(cognitive_functioning ~ sleep)
```

##### Python

```{python}
#| eval: false
#| label: ols-sleep-cog-func
from statsmodels.formula.api import ols

model = ols('cognitive_functioning ~ sleep', data=df).fit()
```

:::


Very easy! But that's all it takes to express a simple linear relationship.  In this case, we're saying that cognitive functioning is a linear (function) of sleep.  By the end of this chapter you'll also know why R's function is `lm` (linear model) and the [statsmodels]{.pack} function is `ols`, but both are doing the same thing. 

<!-- 
We can also express this as a simple equation:

$$ 
\textrm{cog\_func} = \beta_0 + \beta_1 \cdot \textrm{sleep}
$$

But we'll get more into that later. -->


## Key ideas

We can pose a few concepts key to understanding models. This is not an exhaustive list, but it's a good start.  We'll cover each of these in turn.

- What a model is: The model as an idea
- Features, targets, and input-output mappings: how do we get from input to output?
- Model estimation: how do we find the best model?
- Prediction: how do we use a model?
- Assumptions, probabilistic outcomes, uncertainty: how do we know if we can trust a model?

As we go along and cover these concepts, be sure that you feel you have the 'gist' of what we're talking about.  Almost everything of what comes after linear models builds on these ideas, so it's important to have a firm grasp before climbing to new heights.


## What goes into a model? Features and Targets

In the context of a model, how we specify the nature of the relationship depends on the context. In the interest of generality, we'll refer to the **target** as what we want to explain, and **features** as those aspects of the data we will use to explain it. Because people come at data from a variety of contexts, they often use different terminology to mean the same thing. Some of these actually suggest a particular type of relationship (e.g., a causal relationship, an experimental setting), but we'll ignore that for now.  The table below shows some of the common terms used to refer to features and targets.

```{r tbl-feat-target, cache=FALSE}
#| echo: false
#| label: tbl-feature-target-names
#| tbl-cap: Common Terms for Features and Targets


tbl_feat_targ = tibble(
    Feature = c("independent variable", "predictor variable", "explanatory variable", "covariate", "x", "input", "right-hand side"),
    Target  = c("dependent variable", "response", "outcome", "label", "y", "output", "left-hand side"),
) |>
    gt() |>
    rm_caption() # does nothing

tbl_feat_targ
```

We may use any or all of these words to describe things so that you are comfortable with the terminology, but we'll stick with **features** and **targets** for the most part.  In our opinion, this terminology has the least hidden assumptions/implications.


### Expressing Relationships


As noted,  a model is a way of expressing a relationship between a set of features and a target, and one way of expressing them is as inputs and outputs. But how can we go from input to output?  Well to begin, we assume that the features and target are **correlated**, i.e. that there is some relationship between the x and y.  If so, then we can ulitmately use the features to **predict** the target.  In the simplest setting a correlation implies a  relationship where x and y typically move up and down together (left plot) or they move in opposite directions where x goes up and y goes down.

```{r corr-plot, cache.rebuild=TRUE}
#| echo: false
#| label: fig-corr-plot
#| fig-cap: Correlation

p_dat = tibble(
    x = rnorm(50),
    y = .75 * x + rnorm(50, sd = .5),
    yneg = -.75 * x + rnorm(50, sd = .5)
)

p1 = ggplot(p_dat, aes(x, y)) +
    geom_point() +
    labs(subtitle = "Positive Correlation")

p2 = ggplot(p_dat, aes(x, yneg)) +
    geom_point() +
    labs(subtitle = "Negative Correlation")

p1 + p2
```

In addition, the typical correlation suggests a linear relationship.  There are many types of correlation metrics, but the most common one, the Pearson correlation, is explicitly a measure of the linear relationship between two variables.  It's expressed as a number between -1 and 1, where 0 means there is no linear relationship. As we move closer to 1, we would see a tighter scatterplot like the one on the left, until it became a straight line.  The same happens for the negative relationship as we get closer to a value of -1.  If we have only one feature and target, the Pearson correlation reflects the exact result of the linear model we'd conduct.  Even with multiple features, we often lean a version of the Pearson R to help us understand how the features account for the target's variability.

```{r}
#| echo: false
#| label: fig-corr-line-plot
p1 = p1 + geom_smooth(method = "lm", se = FALSE)
p2 = p2 + geom_smooth(method = "lm", se = FALSE)
p1 + p2
```



## *THE* Linear Model


The linear model is perhaps the simplest *functional* model we can use to express a relationship between features and targets.  It's possibly still the most common model used in practice, and it is the basis for many types of models.  Why don't we run one now?

The following dataset contains movie individual [movie reviews][app-data-review] and contains the rating (1-5 stars scale) along with features pertaining to the review (e.g. gender, education) and features about the movie (e.g. genre, year).  We'll use the linear model to predict the rating from the length of the review.

```{r}
#| echo: false
#| eval: false
#| cache: false
#| label: import-review
library(tidyverse)

df_reviews = read_csv("data/review_data.csv")

skimr::skim(df_reviews)
```

For our first linear model, we'll keep things simple.  Let's predict the rating from the length of the review.  We'll use the `lm()` function in R and the `ols()` function in Python to fit the model.  The `lm()` function takes a formula as its first argument, which is a way of expressing the relationship between the features and target.  The formula is expressed as `y ~ x`, where `y` is the target and `x` is the feature.  The `ols()` function takes the same formula, but it also requires a `data` argument that specifies the data frame containing the features and target.

:::{.panel-tabset}

##### R

```{r my-first-model}
#| label: r-my-first-model
df_reviews = read_csv("data/review_data.csv") |>
    drop_na()

model_reviews = lm(rating ~ review_length, data = df_reviews)

summary(model_reviews)
```

```{r}
#| echo: false
#| eval: false
#| label: save-r-model_reviews
# only run as needed
save(model_reviews, file = "data/model_reviews.RData")

```

```{r}
#| label: r-my-first-model-output

# gt(broom::tidy(model_reviews))
```



##### Python

```{python}
#| label: py-my-first-model
#| 
df_reviews = pd.read_csv('data/review_data.csv').dropna()
model_reviews = smf.ols('rating ~ review_length', data = df_reviews).fit()

print(model_reviews.summary())
```

```{python}
#| echo: false
#| eval: false
#| label: save-py-model_reviews

# only run as needed
model_reviews.save("linear_models/data/model_reviews.pickle")
```

:::

For such a simple model, we certainly get a lot of output! Don't worry, you'll eventually come to know what it all means. But it's nice to know how easy it is to get the results!

The linear model posits a **linear combination** of the features.  A linear combination is just a sum of the features, each of which has been multiplied by some specific value.  That value is often called a **coefficient** or possibly **weight**.  The linear model is expressed as (math incoming!):

$$
y = b_0 + b_1x_1 + b_2x_2 + ... + b_nx_n
$$

- $y$ is the target.
- $x_1, x_2, ... x_n$ are the features.
- $b_0$ is the intercept, which is kind of like a baseline value or offset. If we has no inputs at all it would just be the mean of the target.
- $b_1, b_2, ... b_n$ are the coefficients or weights for each feature.

But lets start with something simpler, let's say you want to take a sum of several features. In math you would write it as:

$$
x_1 + x_2 + ... + x_n
$$

In the previous equation, x is the feature and n is the number of features. $x_1$ is the first feature, $x_2$ the second, and so on.  $x$ is an arbitrary designation, you could use any letter, symbol you want, or better would be the actual name. Now look at the linear model.


$$
y = x_1 + x_2 + ... + x_n
$$


In this case, the function is *just a sum*, something so simple we do it all the time. In the linear model sense though, we're actually saying a bit more. Another way to understand that equation is that *y is a function of x*.  We don't show any coefficients, but technically it's as if each coefficient was a value of 1.  In other words, for this simple linear model, we're saying that each feature contributes equally to the target. 

In practice, features will not contribute in the same ways. If we want to relate some feature, x1, and some other feature, x2, to target y, we probably would not assume that they both contribute in the same way from the beginning.  We might give more weight to x1 than x2.  In the linear model, we express this by multiplying each feature by a different coefficient. So the linear model is really just a sum of the features multiplied by their coefficients. In fact, we're saying that each feature contributes to the target in proportion to the coefficient. So if we have a feature x1 and a coefficient b1, then the contribution of x1 to the target is b1\*x1. If we have a feature x2 and a coefficient b2, then the contribution of x2 to the target is b2\*x2. And so on.  So the linear model is really just a sum of the features multiplied by their coefficients.  

For our model, here is the mathemtical represenation:

$$
\textrm{rating} = b_0 + b_1 \cdot \textrm{review\_length}
$$

And with the actual results of our model:

$$
\textrm{rating} = -1.2 + .09 \cdot \textrm{review\_length}
$$

Not too complicated we hope!  But let's make sure we see what's going on here just a little bit more. 

- Our *idea* is that the length of the review is in some way related to the eventual rating given to the movie. 
- Our *target* is rating, and the *feature* is the review length
- We *map the feature to the target* via the linear model, which provies an initial understanding of how the feature is related to the target.  In this case, we start with a baseline of -1.2.  This makes sense only in the case of a rating with no review, which is required for this data.  We'll talk about ways to get a more meaningful intercept later, but for now, that is our starting point.
 if we add a single character to the review, we expect the rating to increase by .09. If we add 10 characters, we expect the rating to increase by .9. If we add 20 characters, we expect the rating to increase by 1.8.


> In chapter 0, maybe say something about science = prediction, but maybe also stay away from academic treament

::: {.callout-note collapse="true" appearance="minimal"}
## Matrix Represenation of a Linear Model

Here we'll show the matrix represenation form of the linear model, for the typical case where we have more than one feature in the model. In the following, y is a vector of all target observations, and likewise each x is a (row) vector of all observations for that feature.  The b vector is the vector of coefficients.  The 1 serves as a means to incorporate the intercept. It's just a feature that always has a value of 1.  The matrix multiplication is just a compact way of expressing the sum of the features multiplied by their coefficients.  We can even do it more

Here is y as a vector of observerations, n x 1.

$$
\textbf{y} = \begin{bmatrix}
y_1 \\
y_2 \\
\vdots \\
y_n
\end{bmatrix}
$$ {#eq-lm-mat-y}

Here is the vector for x, including the intercept:

$$
\textbf{X} = \begin{bmatrix}
1 & x_{11} & x_{12} & \dots & x_{1p} \\
1 & x_{21} & x_{22} & \dots & x_{2p} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
1 & x_{n1} & x_{n2} & \dots & x_{np}
\end{bmatrix}
$$ {#eq-lm-mat-x}

And finally, here is the vector of coefficients:

$$
\textbf{b} = \begin{bmatrix}
b_0 \\
b_1 \\
\vdots \\
b_p
\end{bmatrix}
$$ {#eq-lm-mat-b}


Putting it all together, we get the linear model in matrix form:

$$
\textbf{y = Xb }
$$ {#eq-lm-mat-mult}

:::

## Intepretation

NOTE: NOW A MAIN SECTION

```{r}
#| echo: false
#| label: my-first-model-coefs-etc
intercept = round(coef(model_reviews)[1], 2)
rl_coef = round(coef(model_reviews)[2], 2)
sd_y = round(sd(df_reviews$rating), 2)
sd_x = round(sd(df_reviews$review_length), 2)
rl_ci = round(confint(model_reviews)[2, ], 2)
n_char = 50
```

Let's try and use some context to interpret the coefficient. The size for the coefficient is `r round(rl_coef, 2)`, and the standard deviation of the target, i.e. how much it moves around naturally on its own, is `r sd_y`.  So the coefficient is about `r round(rl_coef / sd_y * 100, 0)`% of the standard deviation of the target.  In other words, a single character addition to a review results in an expected increase of `r round(rl_coef / sd_y * 100, 0)`% of what the review would normally bounce around in value. Most would think this is not negligible, and by some standards maybe even large [CITATION NEEDED- COHEN?].  Why? Because that's just for a single character increase in the review length.  Well, what would be a notable increase in review length?  An easy and straightforward measure would be the standard deviation of the feature. In this case it's `r sd_x`.  So if we increase the review length by one standard deviation, we expect the rating to increase by `r round(rl_coef * sd_x, 2)`, which is `r  round(sd_x/sd_y * rl_coef, 2)` of the standard deviation of the target.  That's a pretty big jump!  So we can see that the coefficient is probably not what we'd call negligible, and that the feature is indeed related to the target.  But we can also see that the coefficient is not so large that it's not believable.

::: {.callout-tip}
The calculation we just did results in what's often called a 'standardized' coefficient.  In the case of the simplest model with one feature like this one, it is identical to the Pearson r correlation metric, which we invite you to check and confirm on your own. In the case of multiple features, it is the correlation between the target and the feature, after adjusting for the other features.  But before you start thinking of it as a measure of *importance*, it is not. It provides some measure of the feature-target linear relationship, but that doesn't not entail practical importance, nor assess nonlinear relationships, interactions, and a host of other interesting things.
:::


## What do we do with a model? Prediction

Once we have a working model, we can use it to make predictions.  We can do this by plugging in values for the features and using the corresponding weights. Let's go back to our results, starting with a simpler depiction.

```{r}
#| echo: false
#| label: my-first-model-output

broom::tidy(model_reviews, conf.int = TRUE) |>
    janitor::clean_names() |>
    rename(feature = term) |>
    mutate(feature = ifelse(feature == "(Intercept)", "intercept", feature)) |>
    gt()
```

The table shows the **coefficient** for each feature including the intercept.  In this case, the coefficient is `r rl_coef`, which means that for every additional word in the review, the rating goes up by `r rl_coef` stars.  So if we had a review that was `r n_char` characters long, we would predict a rating of `r paste(intercept)` + `r glue("{n_char}*{rl_coef}")` = `r round(predict(model_reviews, newdata = data.frame(review_length = n_char)), 1)` stars.

When we're talking about predictions we usually will see this as:

$$
\hat{y} = b_0 + b_1x_1 + b_2x_2 + ... + b_nx_n
$$

What is $\hat{y}$? The hat over the $y$ just means that it's a predicted value of the model, rather than the one we actually observe. In fact, we were missing something in our previous depictions of the linear model.  We need to add an error term, $\epsilon$, to account for the fact that our predictions will not be perfect[^perfect_prediction].  So the full linear model is:

[^perfect_prediction]: In most circumstances, if you ever have perfect prediction, or even near perfect prediction, you have either asked a rather obvious question of your data or have accidentally included the target in your features (or a combination of them).

$$
y = b_0 + b_1x_1 + b_2x_2 + ... + b_nx_n + \epsilon
$$

The error term is a random variable that represents the difference between the actual value and the predicted value.  We can't know what the error term is, but we can estimate it.  We'll talk more about that in the section on Estimation.


### Prediction vs. Explanation

In our view, one can't stress enough the importance of a model's ability to predict the target.  It can be a poor model, maybe because the data is not great, or we're exploring a new area, but we'll always be interested in how well a model **fits** the observed data, and predicts new data.

As strange as it may sound, you can read whole journal articles and business reports in many fields with hardly any mention of prediction. Instead, the focus is on the **explanation** of the model, and this is where the coefficients come in. They are used to explain how the features are related to the target, and we may use terms like saying a feature 'strongly' correlates, or 'negatively' correlates to the target.  In our case, we can say that for every additional word in the review, we *expect* the rating to go up by `r rl_coef` stars.  We might even use our background knowledge and context to say this is a 'strong' relationship.  

Additionally, the extra information in the table shows that review length is **statistically significant**, which is a very loaded description.  For our purposes right now, we'll interpret it as follows: if we expected the coefficient to be zero, the probability of seeing a coefficient as large as `r rl_coef` or larger is very small.  In fact, the probability is so small (rounded to zero), we might conclude that the coefficient is not zero, and that the feature is related to the target.  Unfortunately, statistical significance is affected by other things besides the size of the coefficient, and without an understanding of the context of the features (like how long typical reveiws are, what their range is, what variability of ratings is, etc.), the information it provides is extremely limited, and many would argue, not even useful at all.

In the past, and unfortunately even the present, statistical significance is focused on a great deal, even to the point that a papers are written about models that have no predictive power at all. In those settings, statistical significance is often used as a proxy for importance, which it never should be. If we are very interested in the coefficient, it is better to focus on the range of possible values, which is provided by the **confidence interval**.  While a confidence interval is also a loaded description of a feature's relationship to the target, we can use it in a very practical way as a range of possible values for that weight. Here we see that the coefficient could be as low as `r rl_ci[1]` or as high as `r rl_ci[2]`, which we might interpret as a relatively narrow range, since the , giving us some 'confidence' that we should expect a non-zero value when assessing this particular relationship. Like statistical significance though, it is affected by many things, and without context, is limited in the information it can provide.  

Suffice it to say at this point that how much one focuses on prediction vs. explanation depends on the context and goals of the data endeavor.  There are cases where predictive capability is of utmost importance, and we care less about the explanation, but not to the point of ignoring it. Even with deep learning models for image classification, where the inputs are just RGB values, we'd still like to know what the (notably complex) model is picking up on, otherwise we may be classifying images based on background nonsense.  In some business settings, we are very or even mostly interested in the weights, which might indicate how to allocate resources in some fashion, but if they come from a model with no predictive power, this may be a fruitless endeavor.





### Prediction Error


#### Quick Review: Interpreting our results

At this point we understand a few things:

- Coefficients or weights allow us to understand how a feature relates to the target.
- R-squared is the correlation of the target and the predictions (squared). By squaring we understand the proportion of variance in the target explained by the model.
- The residual variance is a summary of how well our model fits the data, and is basically the part our model doesn't explain.
- The likelihood is an alternate way to assess the match of data and model, and allows us to compare the relative fits of models
- Estimation is a way of finding the best fitting model.

## Adding Complexity

We've seen how to fit a model with a single feature, but we'll always have more than one feature for a model except under some very specific circumstances, such as exploratory data analysis.  Let's see how we can do that.

### Multiple Features

We can add more features to our model very simply. Using the standard functions we just add them to the formala (both R and statsmodels)

```{python}
#| eval: false
'y ~ feature_1 + feature_2 + feature_3'
```

We might have a lot of features, and even for linear models this could be dozens in some scenarios.  How would we update our previous functions? This is where a little bit of matrix algebra comes in handy.  Please visit the section on \@matrix for a bit more detail, but really all you need to know is that this:

$$
y = X\beta 
$$ 

is the same as this:

$$
y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 \dots
$$

where $X$ is a matrix of features, and $\beta$ is a vector of coefficients. Matrix multiplication allows us an efficient way to get our predictions.  Knowing this, let's update our previous functions, using the ols function in particular. We use `X` to denote the matrix of features rather than `x` to denote a single feature.  We also use `par` to denote the vector of parameters, which in this case is just the set of coefficients.

NOTE: CHANGE THE FOLLOWING TO USE STANDARD FUNCTIONS, DELETE/CHANGE ANY PREVIOUS REFERENCES TO USING THE OLS FUNCTIONS




:::{.panel-tabset}

##### R

```{r}
ols <- function(X, y, par, sum_sq = FALSE) {
    # Calculate the predicted values
    y_hat <- X %*% par # this is literally the only change needed; %*% is matrix multiplication

    # Calculate the error
    error <- y - y_hat

    # Calculate the value as sum or mean squared error
    value <- crossprod(error) # crossprod is matrix multiplication

    if (!sum_sq) {
        value <- value / nrow(X)
    }

    # Return the value
    return(value)
}

```

To test our result, let's compare with the `lm` function

```{r}
X = df_reviews |>
    select(review_length, number_of_reviews, year_of_release) |>
    as.matrix()
X = cbind(1, X) # add a column so we can estimate the intercept


model_reviews = lm(
    rating ~ review_length + number_of_reviews + year_of_release,
    data = df_reviews
)

our_estimate = ols(X = X, y = df_reviews$rating, par = coef(model_reviews))
lm_estimate = mean(resid(model_reviews)^2)
```


##### Python

```{python}
def ols(X, y, par, sum_sq = False):
    # Calculate the predicted values
    y_hat = X @ par  # this is literally the only change needed; @ is matrix multiplication
    
    # Calculate the error
    error = y - y_hat
    
    # Calculate the value as sum or mean squared error
    value = error.T @ error # @ is matrix multiplication

    if not sum_sq: 
        value =  value / X.shape[0]

    # Return the value
    return(value)
```

To test our result, let's compare with the `ols` function from statsmodels

```{python}
X = df_reviews[['review_length', 'number_of_reviews', 'year_of_release']].to_numpy()
X = np.hstack((np.ones((X.shape[0], 1)), X)) # add a column so we can estimate the intercept

model_reviews = smf.ols(
    'rating ~ review_length + number_of_reviews + year_of_release', 
    data = df_reviews
).fit()

our_estimate = ols(X = X, y = df_reviews['rating'].to_numpy(), par = model_reviews.params.to_numpy())

# sm provides the mean squared error, but uses a slightly different 
# denominator which won't matter with any sample size of significance; 
# sm_estimate = model_reviews.mse_resid 
sm_estimate = model_reviews.ssr / model_reviews.nobs
```


```{r}
#| echo: false
# pd.DataFrame({'our_estimate': our_estimate, 'sm_estimate': sm_estimate}, index = ['mse'])

tibble(
    ` ` = "mse",
    `our estimate` = our_estimate,
    `lm estimate` = lm_estimate
) |>
    gt() |>
    fmt_number(decimals = 5)
```




:::




## Assumptions and More

## Commentary

- Opinions
- Limitations/Failure points
- Summary