# The Foundation

> It is the chief characteristic of data science that it works.
â€• Isaac Asimov (paraphrased)

```{r}
#| label: setup-lm
#| include: false
source("load_packages.R")
source("setup.R")
source("functions/utils.R")

reticulate::use_condaenv("book-of-models")
```



:::{.aside}

Packages needed or useful for this chapter include:

:::{.panel-tabset}

##### R

```{r}
#| echo: true
#| eval: false
#| label: packages

library(tidyverse)
```

##### Python

```{python}
#| label: modules
#| 
import numpy as np
import pandas as pd
import statsmodels.formula.api as smf
```

:::


```{r}
#| echo: false
#| label: r-display-setup

options(digits = 4) # number of digits of precision for floating point output
```


```{python}
#| echo: false
#| label: py-display-setup

np.set_printoptions(precision = 4, suppress=True) # suppress is for whether to use scientific notation for otherwise rounded to zero numbers
pd.set_option('display.precision', 4) # number of digits of precision for floating point output
```


## Introducing the Greatest Of All Time

Now that you have some idea of what you're getting into, it's time to dive in! We'll start things off by covering the building block of all modeling, and a solid understanding here will provide you the basis for just about anything that comes after, no matter how complex it gets. The **linear model** is our starting point. At first glance, it may seem like a very simple model, but it's actually quite powerful and flexible, able to take in different types of inputs, handle nonlinear relationships, temporal and spatial relations, clustering, and more. Linear models have a long history, with even the formal and scientific idea behind correlation and linear regression being well over a century old[^corbypeirce]! And in that time, the linear model is far and away the most used model out there. But before we start talking about the *linear* model, we need to talk about what a **model** is in general.


[^corbypeirce]: Peirce & Bowditch were well ahead of Pearson and Galton [@rovine2004peirce].


### What is a Model?

At its core, a model is just an **idea**. It's a way of thinking about the world, about how things work, how things change over time, how things are different from each other, and how they are similar. The underlying thread here is that **a model expresses relationships** about things in the world around us. One can also think of a **model as a tool**, one that allows us to take information, in the form of data, and act on it in some way. Just like other ideas (and tools), models have consequences in the real world, and they can be used wisely or foolishly.  

On a practical level, a model is expressed through a particular language, math, but don't let that worry you if you're not so inclined. As it's still just an idea at its core, the idea is the most important thing to understand about a model. The **math is just a way of expressing the idea** in a way that can be communicated and understood by others in a standard way, as well as helping make the idea precise. But in everyday terms, we're trying to understand things like how the amount of sleep relates to cognitive functioning, how the weather affects the number of people who visit a park, how much money to spend on advertising to increase sales, how to detect fraud, and so on.  Any of these  could form the basis of a model, as they stem from scientifically testable ideas, and they all express relationships between things we are interested in, possibly even with an implication of causal relations. 

If you wanted to run a linear model to understand the relationship between sleep and cognitive functioning, you might express it in code as:

:::{.panel-tabset}

##### R

```{r}
#| eval: false
#| label: lm-sleep-cog-func
lm(cognitive_functioning ~ sleep)
```

##### Python

```{python}
#| eval: false
#| label: ols-sleep-cog-func
from statsmodels.formula.api import ols

model = ols('cognitive_functioning ~ sleep', data=df).fit()
```

:::


Very easy! But that's all it takes to express a simple linear relationship.  In this case, we're saying that cognitive functioning is a linear (function) of sleep.  By the end of this chapter you'll also know why R's function is `lm` (linear model) and the [statsmodels]{.pack} function is `ols`, but both are doing the same thing. 

<!-- 
We can also express this as a simple equation:

$$ 
\textrm{cog\_func} = \beta_0 + \beta_1 \cdot \textrm{sleep}
$$

But we'll get more into that later. -->


## Key ideas

We can pose a few concepts key to understanding models. This is not an exhaustive list, but it's a good start.  We'll cover each of these in turn.

- What a model is: The model as an idea
- Features, targets, and input-output mappings: how do we get from input to output?
- Prediction: how do we use a model?
- Interpretation: what does a model tell us?
    - Prediction underlies all interpretation
- Assumptions, probabilistic outcomes, uncertainty: how do we know if we can trust a model?

As we go along and cover these concepts, be sure that you feel you have the 'gist' of what we're talking about.  Almost everything of what comes after linear models builds on these ideas, so it's important to have a firm grasp before climbing to new heights.


## What goes into a model? Features and Targets

In the context of a model, how we specify the nature of the relationship depends on the context. In the interest of generality, we'll refer to the **target** as what we want to explain, and **features** as those aspects of the data we will use to explain it. Because people come at data from a variety of contexts, they often use different terminology to mean the same thing. Some of these actually suggest a particular type of relationship (e.g., a causal relationship, an experimental setting), but we'll ignore that for now.  The table below shows some of the common terms used to refer to features and targets.

```{r tbl-feat-target, cache=FALSE}
#| echo: false
#| label: tbl-feature-target-names
#| tbl-cap: Common Terms for Features and Targets


tbl_feat_targ = tibble(
    Feature = c("independent variable", "predictor variable", "explanatory variable", "covariate", "x", "input", "right-hand side"),
    Target  = c("dependent variable", "response", "outcome", "label", "y", "output", "left-hand side"),
) |>
    gt() |>
    rm_caption() # does nothing

tbl_feat_targ
```

We may use any or all of these words to describe things so that you are comfortable with the terminology, but we'll stick with **features** and **targets** for the most part.  In our opinion, this terminology has the least hidden assumptions/implications.


### Expressing Relationships


As noted,  a model is a way of expressing a relationship between a set of features and a target, and one way of expressing them is as inputs and outputs. But how can we go from input to output?  Well to begin, we assume that the features and target are **correlated**, i.e. that there is some relationship between the x and y.  If so, then we can ultimately use the features to **predict** the target.  In the simplest setting a correlation implies a  relationship where x and y typically move up and down together (left plot) or they move in opposite directions where x goes up and y goes down.

```{r corr-plot, cache.rebuild=TRUE}
#| echo: false
#| label: fig-corr-plot
#| fig-cap: Correlation

p_dat = tibble(
    x = rnorm(50),
    y = .75 * x + rnorm(50, sd = .5),
    yneg = -.75 * x + rnorm(50, sd = .5)
)

p1 = ggplot(p_dat, aes(x, y)) +
    geom_point() +
    labs(subtitle = "Positive Correlation")

p2 = ggplot(p_dat, aes(x, yneg)) +
    geom_point() +
    labs(subtitle = "Negative Correlation")

p1 + p2
```

In addition, the typical correlation suggests a linear relationship.  There are many types of correlation metrics, but the most common one, the Pearson correlation, is explicitly a measure of the linear relationship between two variables.  It's expressed as a number between -1 and 1, where 0 means there is no linear relationship. As we move closer to 1, we would see a tighter scatterplot like the one on the left, until it became a straight line.  The same happens for the negative relationship as we get closer to a value of -1.  If we have only one feature and target, the Pearson correlation reflects the exact result of the linear model we'd conduct.  Even with multiple features, we often lean a version of the Pearson R to help us understand how the features account for the target's variability.

```{r}
#| echo: false
#| label: fig-corr-line-plot
p1 = p1 + geom_smooth(method = "lm", se = FALSE)
p2 = p2 + geom_smooth(method = "lm", se = FALSE)
p1 + p2
```



## *THE* Linear Model


The linear model is perhaps the simplest *functional* model we can use to express a relationship between features and targets. And because of that, it's possibly still the most common model used in practice, and it is the basis for many types of other models.  Why don't we run one now?

The following dataset has individual [movie reviews][app-data-review] and contains the rating (1-5 stars scale), along with features pertaining to the review (e.g., word count, etc.), those that regard the reviewer (e.g., age) and features about the movie (e.g., genre, release year).  We'll use the linear model to predict the rating from the length of the review in terms of word count.

```{r}
#| echo: false
#| eval: false
#| cache: false
#| label: import-review
library(tidyverse)

df_reviews = read_csv("data/movie_reviews.csv")

skimr::skim(df_reviews)
```

For our first linear model, we'll keep things simple.  Let's predict the rating from the length of the review in terms of word count.  We'll use the `lm()` function in R and the `ols()` function in Python[^smfolsR] to fit the model.  Both functions take a formula as the first argument, which is a way of expressing the relationship between the features and target.  The formula is expressed as `y ~ x1 + x2 + ...`, where `y` is the target name and `x` are the feature names. We also need to specify what the data object is, typically a data frame. 

[^smfolsR]: We actually are using the `smf.ols` approach because it is modeled on the R approach.

:::{.panel-tabset}

##### R

```{r my-first-model}
#| label: r-my-first-model
df_reviews = read_csv("data/movie_reviews.csv")

model_reviews = lm(rating ~ word_count, data = df_reviews)

summary(model_reviews)
```

```{r}
#| echo: false
#| eval: false
#| label: save-r-model_reviews
# only run as needed

save(model_reviews, file = "data/model_reviews.RData")
```

##### Python

```{python}
#| label: py-my-first-model
#| 
df_reviews = pd.read_csv('data/movie_reviews.csv')

model_reviews = smf.ols('rating ~ word_count', data = df_reviews).fit()

model_reviews.summary()
```

```{python}
#| echo: false
#| eval: false
#| label: save-py-model_reviews

# only run as needed
model_reviews.save("linear_models/data/model_reviews.pkl")
```

:::

```{r}
#| echo: false
#| label: my-first-model-coefs-etc
intercept = round(coef(model_reviews)[1], 2)
wc_coef = round(coef(model_reviews)[2], 2)
sd_y = round(sd(df_reviews$rating), 2)
sd_x = round(sd(df_reviews$word_count), 2)
rl_ci = round(confint(model_reviews)[2, ], 3)
n_words = round(mean(df_reviews$word_count))
model_rmse = round(summary(model_reviews)$sigma, 2)
model_rsq = round(summary(model_reviews)$r.squared, 2)
```



For such a simple model, we certainly have a lot to unpack here! Don't worry, you'll eventually come to know what it all means. But it's nice to know how easy it is to get the results!

We'll start with the fact that the linear model posits a **linear combination** of the features.  A linear combination is just a sum of the features, each of which has been multiplied by some specific value.  That value is often called a **coefficient** or possibly **weight**.  The linear model is expressed as (math incoming!):

$$
y = b_0 + b_1x_1 + b_2x_2 + ... + b_nx_n
$$

- $y$ is the target.
- $x_1, x_2, ... x_n$ are the features.
- $b_0$ is the intercept, which is kind of like a baseline value or offset. If we had no inputs at all it would just be the mean of the target.
- $b_1, b_2, ... b_n$ are the coefficients or weights for each feature.

But lets start with something simpler, let's say you want to take a sum of several features. In math you would write it as:

$$
x_1 + x_2 + ... + x_n
$$

In the previous equation, x is the feature and n is the number of features. $x_1$ is the first feature, $x_2$ the second, and so on.  $x$ is an arbitrary designation, you could use any letter, symbol you want, or better would be the actual name. Now look at the linear model.


$$
y = x_1 + x_2 + ... + x_n
$$


In this case, the function is *just a sum*, something so simple we do it all the time. In the linear model sense though, we're actually saying a bit more. Another way to understand that equation is that *y is a function of x*.  We don't show any coefficients, but technically it's as if each coefficient was a value of 1.  In other words, for this simple linear model, we're saying that each feature contributes in an identical fashion to the target. 

In practice, features will not contribute in the same ways. If we want to relate some feature, x1, and some other feature, x2, to target y, we probably would not assume that they both contribute in the same way from the beginning.  We might give relatively more weight to x1 than x2. In the linear model, we express this by multiplying each feature by a different coefficient. So the linear model is really just a sum of the features multiplied by their coefficients. In fact, we're saying that each feature contributes to the target in proportion to the coefficient. So if we have a feature x1 and a coefficient b1, then the contribution of x1 to the target is b1\*x1. If we have a feature x2 and a coefficient b2, then the contribution of x2 to the target is b2\*x2. And so on.  So the linear model is really just a sum of the features multiplied by their coefficients.  

For our model, here is the mathematical representation:

$$
\textrm{rating} = b_0 + b_1 \cdot \textrm{word\_count}
$$

And with the actual results of our model:

UPDATE THIS ONCE DATA IS FINALIZED

$$
\textrm{rating} = 5.2 - .25 \cdot \textrm{word\_count}
$$

Not too complicated we hope!  But let's make sure we see what's going on here just a little bit more. 

- Our *idea* is that the length of the review is in some way related to the eventual rating given to the movie. 
- Our *target* is rating, and the *feature* is the word count
- We *map the feature to the target* via the linear model, which provides an initial understanding of how the feature is related to the target.  In this case, we start with a baseline of `r intercept`.  This value makes sense only in the case of a rating with no review, but we have reviews for every observation, so it's not very meaningful as is.  We'll talk about ways to get a more meaningful intercept later, but for now, that is our starting point. Moving on, if we add a single word to the review, we expect the rating to go down by `r wc_coef` stars.  So if we had a review that was `r n_words` words long, i.e., the mean word count, we would predict a rating of `r paste(intercept)` + `r glue("{n_words}*{wc_coef}")` = `r round(predict(model_reviews, newdata = data.frame(word_count = n_words)), 1)` stars.

GRAPHICAL DEPICTION OF THE LINEAR MODEL: where to put?

```{dot}
//| echo: false
//| cache: false
//| label: graph-lm
//| fig-cap: Linear Model as a Graph
//| fig-width: 4
//| file: linear_models/misc/graphical_lm.dot

```

```{mermaid}
%%| echo: false
%%| fig-cap: Linear Model as a Graph
graph LR
    n1((X<sub>1</sub>)) --> |b<sub>1</sub>| n4((y))
    n2((X<sub>2</sub>)) --> |b<sub>2</sub>| n4((y))
    n3((X<sub>3</sub>)) --> |b<sub>3</sub>| n4((y))

    style n1 fill:#f9f,stroke:#333,stroke-width:4px
    %% style e3 font-size:50%
```


At this point you have the basics of what a linear model is and how it works. But there is a lot more to it than that. Just getting the model is easy enough, but we need to be able to use it and understand the details better, so we'll get into that now!

> In chapter 0, maybe say something about science = prediction, but maybe also stay away from academic treatment

::: {.callout-note collapse="true" appearance="minimal"}
## Matrix Representation of a Linear Model

Here we'll show the matrix representation form of the linear model, for the typical case where we have more than one feature in the model. In the following, y is a vector of all target observations, and likewise each x is a (row) vector of all observations for that feature.  The b vector is the vector of coefficients.  The 1 serves as a means to incorporate the intercept. It's just a feature that always has a value of 1.  The matrix multiplication is just a compact way of expressing the sum of the features multiplied by their coefficients.  We can even do it more

Here is y as a vector of observations, n x 1.

$$
\textbf{y} = \begin{bmatrix}
y_1 \\
y_2 \\
\vdots \\
y_n
\end{bmatrix}
$$ {#eq-lm-mat-y}

Here is the vector for x, including the intercept:

$$
\textbf{X} = \begin{bmatrix}
1 & x_{11} & x_{12} & \dots & x_{1p} \\
1 & x_{21} & x_{22} & \dots & x_{2p} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
1 & x_{n1} & x_{n2} & \dots & x_{np}
\end{bmatrix}
$$ {#eq-lm-mat-x}

And finally, here is the vector of coefficients:

$$
\textbf{b} = \begin{bmatrix}
b_0 \\
b_1 \\
\vdots \\
b_p
\end{bmatrix}
$$ {#eq-lm-mat-b}


Putting it all together, we get the linear model in matrix form:

$$
\textbf{y = Xb }
$$ {#eq-lm-mat-mult}

:::



## What do we do with a model? 

Once we have a working model, there are two primary ways we can use it.  One way to use a model is to help us understand the relationships between the features and our outcome of interest. In this way the focus can be said to be on **explanation**, or interpreting the model results. The other way to use a model is to use it to make estimates about the outcome for specific observations, often ones we haven't seen in our data. In this way the focus is on **prediction**. In practice, we often do both, but the focus is usually on one or the other. We'll cover both in detail here, starting with prediction.

It may not seem like much at first, but a model is of no use if it can't be used to make predictions about what we can expect in the world around us. Once our model has been *fit*, we can obtain our predictions by plugging in values for the features that we are interested in, and, using the corresponding weights and other parameters that have been estimated, come to a guess about a specific observation. Let's go back to our results, starting with a simpler depiction.

```{r}
#| echo: false
#| label: my-first-model-output-2

broom::tidy(model_reviews, conf.int = TRUE) |>
    janitor::clean_names() |>
    rename(feature = term) |>
    mutate(feature = ifelse(feature == "(Intercept)", "intercept", feature)) |>
    gt()
```

The table shows the **coefficient** for each feature including the intercept, which is our starting poing.  In this case, the coefficient for word count is `r wc_coef`, which means that for every additional word in the review, the rating goes `r ifelse(sign(wc_coef) == 1, 'up', 'down')` by `r wc_coef` stars.  So if we had a review that was `r n_words` words long, we would *predict* a rating of `r paste(intercept)` + `r glue("{n_words}*{wc_coef}")` = `r round(predict(model_reviews, newdata = data.frame(word_count = n_words)), 1)` stars.  


When we're talking about predictions for a linear model, we usually will see this as the following mathematically:

$$
\hat{y} = b_0 + b_1x_1 + b_2x_2 + ... + b_nx_n
$$

What is $\hat{y}$? The hat over the $y$ just means that it's a predicted value of the model, rather than the one we actually observe. In fact, we were missing something in our previous depictions of the linear model.  We need to add what is usually referred to as an **error term**, $\epsilon$, to account for the fact that our predictions will not be perfect[^perfect_prediction].  So the full linear model is:

[^perfect_prediction]: In most circumstances, if you ever have perfect prediction, or even near perfect prediction, the usual issues are that you have either asked a rather obvious/easy question of your data (e.g., predicting whether an image is of a human or a car), or have accidentally included the target in your features (or a combination of them) in some way.

$$
y = b_0 + b_1x_1 + b_2x_2 + ... + b_nx_n + \epsilon
$$

The error term is a random variable that represents the difference between the actual value and the predicted value.  We can't know what the error term is, but we can estimate it.  We'll talk more about that in the section on [estimation][#estimation].


### What kinds of predictions can we get?

What predictions we get depends on the type of model we are using.  For the linear model, we can get predictions for the target, which is a **continuous variable**.  Very commonly, we also can get predictions for a **categorical target**, such as whether the rating is 'good' or 'bad'. This simple breakdown pretty much covers everything, as we typically would be predicting a continuous variable or a categorical variable, or more of them, like multiple continuous variables, or a target with multiple categories, or sequences of categories (e.g. words). In our case, we can get predictions for the rating, which is a number between 1 and 5.

We saw a prediction for a single observation, but we can also get predictions for multiple observations at once.  In fact, we can get predictions for all observations in our dataset. Besides that, we can also get predictions for observations that we don't have data for. The following shows how we can get predictions for all data, and for a single observation with a word count of 5.

::: {.panel-tabset}

##### R

```{r}
#| label: my-first-model-predictions-r

all_predictions = predict(model_reviews)
single_prediction = predict(model_reviews, newdata = data.frame(word_count = 5))
```

##### Python

```{python}
#| label: my-first-model-predictions-py

all_predictions   = model_reviews.predict()
single_prediction = model_reviews.predict(pd.DataFrame({'word_count': [5]}))
```

:::



Here is a plot of our predictions versus the actual ratings[^jitter]. The reference line is where the points would fall if we had perfect prediction. We can see that the predictions are definitely not perfect, but they are not completely off base either. We'll talk about how to assess the quality of our predictions later, but we can at least get a sense that we have a correspondence relationship between our predictions and target, which is definitely better than not having a relationship at all!

[^jitter]: Word count is **discrete**- it can only take whole numbers like 3 or 20, and it is our only feature.  Because of this, we can only make very limited predicted rating values, while the observed rating can take on many other values. Because of this, the true plot would show a more banded result with many points overlapping, so we use a technique called **jittering** to move the points around a little bit so we can see them all.  The points are still roughly in the same place, but they are moved around a little bit so we can see them all.  

```{r}
#| echo: false
#| label: my-first-model-predictions-plot
#| fig-cap: Predictions vs. Actual Ratings\

df_reviews |>
    mutate(pred = all_predictions) |>
    ggplot(aes(pred, rating)) +
    # geom_point() +
    geom_point(position = position_jitter()) +
    geom_abline(intercept = 0, slope = 1, color = okabe_ito[2], size = 2) +
    labs(
        x = "Predicted Rating",
        y = "Actual Rating",
        title = "Predictions vs. Actual Ratings",
        caption = "Points have been jittered for better visibility."
    ) +
    theme(
        plot.caption = element_text(vjust = -2)
    )
```

Now let's look at what our prediction looks like for a single observation, and we'll add in a few more- one for 10 words, and one for a 50 word review, which is beyond the length of any review in this dataset, and one for 12.3 words, which isn't even possible for this data.

```{r}
#| echo: false
#| label: tbl-predictions
#| tbl-cap: Predictions for Specific Observations

tibble(
    word_count = c(5, 10, 12.3, 50),
    pred = predict(model_reviews, newdata = data.frame(word_count = word_count))
) |>
    rename("Word Count" = word_count, "Predicted Rating" = pred) |>
    gt() |>
    fmt_number(decimals = 1)
```

The values reflect the `r word_sign(wc_coef, c('positive', 'negative'))` coefficient from our model, reflecting a `r word_sign(wc_coef, c('increasing', 'decreasing'))` relationship. Further more, we see the power of the model's ability to make predictions for what we can't see. Maybe we limited our data review size, but we know there are 50 word reviews out there, and we can still make a guess as to what the rating would be for such a review.  Maybe in another case, we know a group of people who have on average 12.3 word reviews, and we can make a guess as to what the average rating would be for that group. Our model doesn't know anything about the context of the data, but we can use our knowledge to make predictions about the world around us. This is a very powerful capability, and it's one of the main reasons we use models in the first place.  


### Prediction Error

As we have seen, predictions are not perfect, and an essential part of the modeling endeavor is to better understand these errors and why they occur. In addition, error assessment is the fundamental way in which we assess a model's performance, and, by extension, compare that performance to other models. In general, prediction error is the difference between the actual value and the predicted value or some function of it, and in statistical models, is also often called the **residual**. In the case of the linear model we've been looking at, we can express this in a single metric as the sum or mean of our (squared) errors, the latter of which is a very commonly used modeling metric (MSE or mean squared error), or also, its square root (RMSE or root mean squared error). 

If we look back at our results, we can see this expressed as the part of the output or as an attribute of the model[^sigmanotrmse]. The RMSE is more interpretable, and it tells us that we can expect an average prediction error for rating of `r model_rmse`.  Given that the rating is on a 1-5 scale, this maybe isn't bad, but we could definitely hope to do better than get within roughly half a point on this scale.  We'll talk about ways to improve this later.


[^sigmanotrmse]: The actual divisor for linear regression output depends on the complexity of the model, and in this case the sum of the squared errors is divided by N-2 (due to estimating the intercept and coefficient) instead of N. This is a technical detail that would only matter for data too small to make much of in the first place, and not important for our purposes here.

:::{.panel-tabset}

##### R

```{r}
#| label: my-first-model-mse-r
summary(model_reviews) # 'Residual standard error' is approx RMSE
```

##### Python


```{python}
#| label: my-first-model-mse-py
np.sqrt(model_reviews.scale)   # RMSE
```

:::

It's also good to look at the distribution of errors visually, and often the modeling package you use will have this as a default plotting method when doing a standard linear regression, so it's wise to take advantage of it. Here we see a couple things. First, the distribution is roughly normal, which is a good thing, since statistical linear regression assumes our prediction error is normally distributed. Second, we see that the mean of the errors is zero, which is a consequence of linear regression, and the reason we look at the mean squared error rather than the mean error when assessing model performance.  Of more practical concern however, is that we don't see extreme values or clustering that might indicate a failure on the part of the model to pick up certain segments of the data.  It still is good to look at the extremes just in case we can pick up on some aspect of the data that we could potentially incorporate into the model.    

```{r}
#| echo: false
#| label: my-first-model-error-plot
#| fig-cap: Distribution of Prediction Errors
tibble(
    error = resid(model_reviews)
) |>
    ggplot(aes(error)) +
    geom_histogram() +
    geom_vline(xintercept = 0, color = okabe_ito[2], size = 2) +
    labs(
        x = "Error",
        y = "Count",
        # title = "Distribution of Prediction Errors",
    ) +
    theme(
        plot.caption = element_text(vjust = -2)
    )
```


```{r}
#| echo: false
#| label: get_worst_prediction
#|
worst_prediction = df_reviews |>
    mutate(prediction = predict(model_reviews)) |>
    slice(which.max(abs(resid(model_reviews)))) |>
    select(rating, prediction, word_count)
```

Looking at our worst prediction, we see the observation has a typical word count, and so our simple model will just predict a fairly typical rating. But the actual rating is `r worst_prediction$rating`, which is `r abs(round(worst_prediction$rating - worst_prediction$prediction, 1))` away from our prediction, a very noticeable difference.

```{r}
#| echo: false
#| label: tbl-worst-prediction
#| tbl-cap: Worst Prediction

worst_prediction |>
    gt() |>
    fmt_number(c(rating, prediction), decimals = 1) |>
    fmt_number(word_count, decimals = 0)
```

At this point you have the gist of prediction and prediction error, but there is a lot more to it. More detail can be found in the @sec-estimation chapter, since we often estimate the parameters of our model by picking those that will reduce the prediction error the most.    For now, let's move on to the other main use of models, which is to help us understand the relationships between the features and the target, or **explanation**.

:::{.callout-note title='R-squared' collapse="true" appearance="minimal"}

A very popular metric for linear models is R-squared which we can think of in a variety of ways, but here are two important ones. 

- The squared correlation of our predictions and true target values. 
- 1 - the ratio of the sum of squared errors to the sum of squared deviations from the mean of the target, i.e. 1 - proportion of error variance.

In both cases it tells us the proportion of the variance in the target that is explained by the model, but the latter makes clear the relationship to prediction error. In our model, the R-squared is `r model_rsq`, which means that our model explains `r round(model_rsq* 100, 0)`% of the variance in the target. This suggests there is plenty of work left to do!
:::


## Interpretation

When it comes to interpreting the results of our model, there are a lot of tools at our disposal, though many of the tools we can ultimately use depend on the specifics of the model we have employed. In general though, we can group our understanding approach to that of the **feature level** and the **model level**. A feature level understanding is concerned with the relationship between a single feature and the target, and also attempts comparisons of feature importance. Model level interpretation is concerned with assessments of model fit and performance. We'll start with the feature level, and then move on to the model level.

### Feature Level 

As mentioned, at the feature level, we are primarily concerned with the relationship between a single feature and the target. More specifically, we are interested in the direction and magnitude of the relationship, but in general, it all boils down to how a feature induces change in the target. For numeric features, we are curious about the change in the target given some amount of change in the feature. For categorical features it's the same, but often we like to express the change in terms of group mean differences or something similar, since the order of categories is not usually meaningful.  Key to the feature level interpretation is the specific predictions made at key feature values.

#### Basics

Let's start with the basics by looking again at our coefficient table from the model output.

```{r}
#| echo: false
#| label: my-first-model-output

broom::tidy(model_reviews, conf.int = TRUE) |>
    janitor::clean_names() |>
    rename(feature = term) |>
    mutate(feature = ifelse(feature == "(Intercept)", "intercept", feature)) |>
    gt()
```

Here, the main thing to look at are the actual feature coefficients and the direction of their relationship, positive or negative.  We saw before that the coefficient for word count is `r wc_coef`, and this means that for every additional word in the review, the rating goes `r ifelse(sign(wc_coef) == 1, 'up', 'down')` by `r wc_coef`.  So if we had a review that was `r n_words` words long, we would predict a rating of `r paste(intercept)` + `r glue("{n_words}*{wc_coef}")` = `r round(predict(model_reviews, newdata = data.frame(word_count = n_words)), 1)` stars.

This interpretation gives us directional information, but how would we interpret the magnitude of the coefficient? Let's try and use some context to help us. The value for the coefficient is `r round(wc_coef, 2)`, and the standard deviation of the target, i.e. how much it moves around naturally on its own, is `r sd_y`.  So the coefficient is about `r round(abs(wc_coef) / sd_y * 100, 0)`% of the standard deviation of the target.  In other words, the addition of a single word to a review results in an expected `r word_sign(wc_coef, c('increase', 'decrease'))` of `r round(abs(wc_coef) / sd_y * 100, 0)`% of what the review would normally bounce around in value. We probably wouldn't consider this negligible, but also, a single word change isn't much. What would be a significant change in word count? Let's consider the standard deviation of the feature. In this case it's `r sd_x` for word count.  So if we increase the word count by one standard deviation, we expect the rating to `r word_sign(wc_coef, c('increase', 'decrease'))` by `r glue('{wc_coef} * {sd_x} = {round(wc_coef * sd_x, 2)}')`, and this translates in to a change of `r round(wc_coef * sd_x, 2)`/`r sd_y` = `r  round(sd_x/sd_y * wc_coef, 2)` standard deviation units of the target. Without additional context, many would think that's a significant change [CITATION], or at the very least, that the coefficient is not negligible, and that the feature is indeed related to the target.  But we can also see that the coefficient is not so large that it's not believable.

::: {.callout-tip title='Standardized Coefficients'}
The calculation we just did results in what's often called a 'standardized' or 'normalized' coefficient. In the case of the simplest model with only one feature like this, it is identical to the Pearson r correlation metric, which we invite you to check and confirm on your own, which should roughly equal our calculation using rounded values. In the case of multiple features, it represents a (partial) correlation between the target and the feature, after adjusting for the other features. But before you start thinking of it as a measure of *importance*, it is not. It provides some measure of the feature-target linear relationship, but that doesn't not entail *practical* importance, nor is it useful in the presence of nonlinear relationships, interactions, and a host of other interesting things that are typical to data and models.
:::


After assessing the coefficients, next up in our table is the **standard error**. The standard error is a measure of how much the coefficient varies from sample to sample.  If we collected the data multiple times, even under identical circumstances, we wouldn't get the same value each time- it would bounce around a bit, and the standard error is an estimate of how much it would bounce around. In other words, it's a measure of **uncertainty**, and along with the coefficients, it's used to calculate everything else in the table.  The statistic, here a t-statistic from the student t distribution[^coefstats], is the ratio of the coefficient to the standard error. With that in hand, we  used to calculate the p-value, which is the probability of seeing a coefficient as large as the one we have, *if* we assume the true value of the coefficient is zero.  In this case, the p-value is `r round(summary(model_reviews)$coefficients[2, 4], 4)`, which is very small.  We can conclude that the coefficient is statistically different from zero, and that the feature is related to the target, at least statistically.


[^coefstats]: Most statistical tables of this sort will use a t (student t distribution), Z (normal distribution), or F (F distribution) statistic.  It doesn't really matter for your purposes which is used by default, they provide the p-value of interest to claim statistical significance. But for t and Z, any value over two would be associated with a p-value that would be statistically significant by typical standards.

Aside from the coefficients, the most important output is the **confidence interval** (CI). The CI is a range of values that encapsulates the unceratinty we have in our guess about the coefficients.  While our best guess for the effect of word count on rating is `r round(wc_coef, 2)`, we know it's not exactly that, and the CI gives us a range of reasonable values we might expect the effect to be based on the data at hand and the model we've employed. In this case, the default is a 95% confidence interval, and we can think of the confidence interval like [throwing horseshoes](https://en.wikipedia.org/wiki/Horseshoes_(game)). If we kept collecting data and running models, 95% of our CIs would capture the true value, and this is one of them. That's the technical definition, which is a bit abstract[^whatisaci], but we can also think of it more simply as a range of values that are good guesses for the true value.  In this case, the CI is `r paste(rl_ci[1])` to `r paste(rl_ci[2])`, and we can be 95% confident that a good range for the coefficient is between those values. We can also see that the CI is relatively narrow, which is good, as it implies that we have a good idea of what the coefficient is.  If it was very wide, we would have a lot of uncertainty about the coefficient, and we would not likely not want to base important decisions regarding it.

[^whatisaci]: The interpretation regarding the CI is even more nuanced than this, but we'll leave that for another time. For now, we'll just say that the CI is a range of values that are good guesses for the true value. Your authors have used frequentist and Bayesian statistics for many years, we are fine with both of them, because they both work well enough in the real world. Despite where this ranged estimate comes from, the vast majority use CIs in the same way, and they are a useful tool for understanding the uncertainty in our estimates.

:::{.aside}

```{r}
#| echo: false
#| label: fig-ci-plot
#| fig-cap: Confidence Intervals as Horseshoes
# from https://en.wikipedia.org/wiki/Horseshoes_(game)
knitr::include_graphics("img/horseshoes.jpeg")
```

:::

Keep in mind that your model has a lot to say about what you'll be able to say at the feature level. As an example, as we get into machine learning models, you won't have as easy a time with coefficients and their confidence intervals. For now we'll stop here, but there is a lot more to the story when it comes to feature level interpretation, and we'll continue to return to the topic. But first, let's take a look at interpreting things in another way.


### Model Level

```{r}
#| echo: false
#| label: model-metrics
rsq = round(summary(model_reviews)$r.squared, 2)
rsq_perc = 100 * rsq
rmse = round(performance::rmse(model_reviews), 2)
mae = round(performance::mae(model_reviews), 2)
aic = round(AIC(model_reviews), 2)
```

Thus far, we've focused on interpretation at the feature level. But knowing the interpretation of a feature doesn't do you much good if the model itself is poor! In that case, we also need to assess the model as a whole, and as with the feature level, we can do this in a few ways. First, we can start with the predictions of our model. As noted previously, how well the predictions and target line up is a measure of how well the model fits the data. Most model-level interpretation involves assessing and comparing model fit and variations on this theme. One of the better ways to assess model fit is visually, so let's look at our predictions vs. the target.

::: {.panel-tabset}

##### R

```{r}
#| echo: true
#| eval: false
predictions = predict(model_reviews)
y = df_reviews$rating
```

##### Python

```{python}
#| echo: true
#| eval: false

predictions = model_reviews.predict()
y = df_reviews.rating
```

:::


```{r}
#| echo: false
#| label: fig-pp-scatter
#| fig-cap: Predictions vs. Observed Ratings


p1 <- df_reviews |>
    mutate(pred = predict(model_reviews)) |>
    ggplot(aes(pred, rating)) +
    geom_point() +
    # geom_point(position = position_jitter()) +
    geom_abline() +
    labs(
        x = "Predicted Rating",
        y = "Observed Rating",
        subtitle = "Raw values"
    )

p2 <- df_reviews |>
    mutate(pred = predict(model_reviews)) |>
    ggplot(aes(pred, rating)) +
    geom_point(position = position_jitter()) +
    geom_abline() +
    labs(
        x = "Predicted Rating",
        y = "Observed Rating",
        subtitle = "Jittered values"
    )

p1 + p2

```

The one on the left is using the raw target and predictions- very stripey! The reason is that our ratings are only at the single decimal place precision, and our word count is at the integer level precision, so we have a lot of ties. The right side jitters the data randomly a bit so we can see a better pattern, but is otherwise the same.  In general, the closer to a line this plot becomes the better, so we can tell already there is still a lot of noise left to explain beyond our model.

#### Predictive Distribution

As another view, let's plot our predictive *distribution* versus the observed distribution.  
```{r}
#| echo: false
#| label: fig-pp-dists
#| fig-cap: Predicted vs. Observed Distributions

df_reviews |>
    select(rating) |>
    mutate(prediction = predict(model_reviews)) |>
    pivot_longer(cols = c(rating, prediction), names_to = "type", values_to = "value") |>
    ggplot(aes(value, fill = type, color = type)) +
    geom_density(color = NA, alpha = .5) +
    geom_vline(
        data = . %>% filter(type == "rating"),
        aes(xintercept = mean(value)),
        color = okabe_ito[3],
        size = 2
    ) +
    geom_vline(
        data = . %>% filter(type == "prediction"),
        aes(xintercept = mean(value)),
        color = okabe_ito[3],
        size = 2
    ) +
    scale_color_manual(values = okabe_ito, aesthetics = c("color", "fill")) +
    labs(
        x = "Rating",
        y = "",
        caption = "Lines are means",
        subtitle = "Predicted vs. Observed Distributions"
    ) +
    theme(
        axis.ticks.y = element_blank(),
        axis.text.y = element_blank()
    )

```

Again, the lumpiness of our feature and it's discrete predictions is borne out visually. We can see more clearly the shortcomings of this model as well. Our feature doesn't capture enough of the variation in our target, as our predicted distribution is more narrow than the observed distribution.  We can also see that our average prediction is the same as the observed mean rating, and this is a consequence of linear regression.

#### Model Metrics

We've already discussed mean-squared error, but there are other metrics we can use to assess model fit.  It is a very popular measure for continuous targets and gives a sense of how much our predictions miss the target on average.  In our case, the value was `r rmse`.  Another metric we can use in this particular situation is the mean absolute error, which is similar to the mean squared error, but instead of squaring the errors, we just take the absolute value.  This is a bit more interpretable, and in our case, the value is `r mae`.  



Finally, we can look at the **R-squared** of the model, possibly the most popular measure of model performance with linear regression and linear models in general. It represents the square of the correlation of the predictions on the current data and the observed target values. Before squaring, it's just the correlation of the values that we saw in the previous plot (@fig-pp-scatter). When we square it, it provides a measure of how much of the variance in the target is explained by the model. In this case, our model shows the R-squared is `r rsq`, which is pretty good for a single feature model, and tells us that `r rsq_perc`% of the target is explained by our model.

For a basic linear regression, R-squared is one of the most popular measures we look at. With it we get a sense of the variance shared between *all* features in the model and the target, however complex the model gets. As long as we use it descriptively as a simple correspondence assessment of our predictions and target, it's a fine metric. For various reasons, it's not a great metric for comparing models, but again, as long as you don't get carried away, it's fine.


### Prediction vs. Explanation

In your humble authors' views, one can't stress enough the importance of a model's ability to predict the target. It can be a poor model, maybe because the data is not great, or we're exploring a new area, but we'll always be interested in how well a model **fits** the observed data, and predicts new data.

As strange as it may sound, you can read whole journal articles, news articles, and business reports in many fields with hardly any mention of prediction. Instead, the focus is almost entirely on the **explanation** of the model, and this is where the coefficients come in. 

<!---
 They are used to explain how the features are related to the target, and we may use terms like saying a feature 'strongly' correlates, or 'negatively' correlates to the target.  In our case, we can say that for every additional word in the review, we *expect* the rating to go up by `r wc_coef` stars.  We might even use our background knowledge and context to say this is a 'strong' or 'weak' relationship.   
--->

Additionally, the extra information in the table shows that word count is **statistically significant**, which is a very loaded description. For our purposes right now, we'll interpret it as follows: if we expected the coefficient to be zero, the probability of seeing a coefficient as large as or larger than the one in our model is very small. In fact, the probability is so small, we might conclude that the coefficient is not zero, and that the feature is related to the target. Unfortunately, statistical significance is affected by other things besides the size of the coefficient, and without an understanding of the context of the features (like how long typical reviews are, what their range is, what variability of ratings is, etc.), the information it provides is extremely limited, and many would argue, not even useful at all.

In the past, and unfortunately even the present, statistical significance is focused on a great deal, even to the point that a much hullabaloo is made about models that have no predictive power at all. In those settings, statistical significance is often used as a proxy for importance, which it never should be. If we are very interested in the coefficient, it is better to focus on the range of possible values, which is provided by the **confidence interval**.  While a confidence interval is also a loaded description of a feature's relationship to the target, we can use it in a very practical way as a range of possible values for that weight. Here we see that the coefficient could be as low as `r rl_ci[1]` or as high as `r rl_ci[2]`, which we might interpret as a relatively narrow range, since the , giving us some 'confidence' that we should expect a non-zero value when assessing this particular relationship. Like statistical significance though, it is affected by many things, and without context, is limited in the information it can provide.  

Suffice it to say at this point that how much one focuses on prediction vs. explanation depends on the context and goals of the data endeavor.  There are cases where predictive capability is of utmost importance, and we care less about the explanation, but not to the point of ignoring it. Even with deep learning models for image classification, where the inputs are just RGB values, we'd still like to know what the (notably complex) model is picking up on, otherwise we may be classifying images based on background nonsense.  In some business settings, we are very or even mostly interested in the weights, which might indicate how to allocate resources in some fashion, but if they come from a model with no predictive power, this may be a fruitless endeavor.








## Adding Complexity

We've seen how to fit a model with a single feature, but we'll always have more than one feature for a model except under some very specific circumstances, such as exploratory data analysis.  Let's see how we can do that.

### Multiple Features

We can add more features to our model very simply. Using the standard functions we just add them to the formula (both R and statsmodels) as follows.

```{python}
#| eval: false
'y ~ feature_1 + feature_2 + feature_3'
```

In other cases, additional features will just be the additional input columns. We might have a lot of features, and even for linear models this could be dozens in some scenarios.  How would we update our previous functions? This is where a little bit of matrix algebra comes in handy.  Please visit the section on \@matrix section for a bit more detail, but really all you need to know is that this:

$$
y = X\beta 
$$ 

is the same as this:

$$
y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 \dots
$$

where $X$ is a matrix of features, and $\beta$ is a vector of coefficients. Matrix multiplication allows us an efficient way to get our predictions.  Knowing this, let's update our previous functions, using the ols function in particular. We use `X` to denote the matrix of features rather than `x` to denote a single feature.  We also use `par` to denote the vector of parameters, which in this case is just the set of coefficients.


MOVE SHAP TO MULTIPLE FEATURES??

#### SHAP Values

Some models are more complicated than can be explained by a simple coefficient, e.g. nonlinear effects in generalized additive models, or may not even have coefficients, like gradient boosting models. Such models typically won't come with statistical output like standard errors and confidence intervals either. But we'll still have some tricks up our sleeve, though they won't come without some caveats. 

A very common interpretation tool is called a **SHAP value**. SHAP stands for **SHapley Additive exPlanations**, and it provides a means to understand how much each feature contributes to a specific prediction. It's based on a concept from game theory called the **Shapley value**, which is a way to understand how much each player contributes to the outcome of a game. While the computations can be tedious, the basic idea is relatively straightforward- for a given prediction at a specific feature's value, we can calculate the difference between the prediction and the average prediction. This is the **local effect** of the feature.  However, we must also consider doing this for all possible values of other features that might be in a model, as well as considering whether other features are present for the prediction or not. The initial Shapley approach is to average the local effects over all possible combinations of features, which is computationally intractable for all but the simplest models. The SHAP approach offers more computationally feasible methods for estimation which, while still computationally intensive, is doable for many models.  The SHAP approach also has the benefit of being able to be applied to any model, and it's the approach we'll use here.

For a linear regression model, the SHAP value is the same as the coefficient, but for more complicated models, it's not so simple.  Let's look at the SHAP values for our model.  We'll start with a single observation, and then look at the average SHAP values for all observations.



```{r}
#| echo: false
#| eval: false
#|
explainer = DALEX::explain(model_reviews)

word_count_15 = iBreakDown::break_down_uncertainty(
    model_reviews,
    data = df_reviews,
    new_observation = tibble(word_count = 15)
)

# plot(word_count_15)

pred = predict(model_reviews, newdata = tibble(word_count = 15))

pred - mean(predict(model_reviews))

# coef(model_reviews)["word_count"]*15 - coef(model_reviews)["word_count"]*mean(df_reviews$word_count)

map_dbl(
    unique(df_reviews$word_count),
    \(x) predict(model_reviews, newdata = tibble(word_count = x)) - mean(predict(model_reviews))
) |>
    abs() |>
    mean()
# iml::FeatureImp()

# shapper also from DrWhy group but uses shap py module, very poorly so evidently, and doesn't appear to be recently updated
# word_count_15 <- DALEX::explain(
#     model_reviews,
#     data = df_reviews |> dplyr::select(word_count),
#     y = df_reviews$rating
# )
# shapper::shap(explainer, new_observation = tibble(word_count = 15))
```

```{r}
mod_test = lm(
    rating ~
        word_count
        + release_year
        + length_minutes,
    data = df_reviews
)
coef(mod_test)
explainer <- DALEX::explain(mod_test)
word_count_15 <- iBreakDown::break_down_uncertainty(
    mod_test,
    data = df_reviews,
    new_observation = tibble(word_count = 15, release_year = 2015, length_minutes = 100)
)
plot(word_count_15)
```

```{r}
pp = DALEX::predict_parts(
    explainer,
    tibble(word_count = 15, release_year = 2015, length_minutes = 100),
    B = 5
)
pp |> plot()

# shapviz works with the DALEX explainer so we can get all the usual shap plots in ggplot format
library(shapviz)
sv_force(shapviz(pp)) +
    xlab("Breakdown")
sv_waterfall(shapviz(pp)) +
    labs(
        x = "Contribution to Prediction",
        title = "SHAP Waterfall"
    )
sv_importance(shapviz(pp)) +
    xlab("Importance")
```


We can ultimately get a shap value for every value of a feature, and take the average. If we do this for each feature, we can use it as a measure of **feature importance**.


#### Feature Importance

How important is a feature? It's a common question, and one that is often asked of models, but the answer ranges from 'it depends' and 'it doesn't matter'. Let's start with some hard facts:

- There is no single definition of importance.
- There is no single metric for *any* model that will definitively tell you how important a feature is.
- There are many metrics for a given model that are equally valid, but may come to different conclusions.
- Many metrics of importance fail to adequately capture interactions and deal with correlated features.
- All measures of importance are measured with uncertainty, and the uncertainty can be large.


To show just how difficult measuring feature importance is, we only have to stick with our simple linear regression. Think again about R-squared: it tells us the proportion of the target explained by our features. An ideal measure of importance would be able to tell us how much each feature contributes to that proportion, or in other words, decomposes R-squared into the relative contributions of each feature. One of the most common measures of importance in linear regression is the standardized coefficient we demonstrated earlier. You know what it doesn't do? Decompose R-squared. 

Let's demonstrate things by adding more features to our model.

```{r}
#| echo: false
#| eval: false
#| label: fig-rsq-decomp
#| fig-cap: R-squared Decomposition

model_reviews_extra = lm(
    rating ~
        word_count
        + age
        + review_year
        + release_year
        + length_minutes
        + children_in_home
        + total_reviews,
    data = df_reviews
)

library(relaimpo)
metrics = c("lmg", "last", "first", "betasq", "pratt", "genizi", "car")
init_rela_false = relaimpo::calc.relimp(
    model_reviews_extra,
    type = metrics,
    rela = FALSE
)
init_rela_true = relaimpo::calc.relimp(
    model_reviews_extra,
    type = metrics,
    rela = TRUE
)

df_rela = map_df(metrics, \(x) slot(init_rela_false, x)) |>
    t()
colnames(df_rela) = metrics

decomposer2 = names(which(round(colSums(df_rela), 3) == round(summary(model_reviews_extra)$r.squared, 3)))

pdat_rela = df_rela |>
    as_tibble(rownames = "feature") |>
    pivot_longer(-feature, names_to = "metric", values_to = "value") |>
    mutate(
        metric = fct_inorder(metric),
        true_decompose = ifelse(metric %in% decomposer2, "Decomposes R-squared", "Does not")
    )

pdat_rela |>
    arrange(true_decompose, metric, value) |>
    ggplot(aes(value, fct_inorder(feature))) +
    geom_col(
        aes(
            alpha = I(ifelse(true_decompose == "Decomposes R-squared", 1, .25)),
            color = feature == "word_count",
            fill  = feature == "word_count",
            width = I(ifelse(feature == "word_count", .5, .25))
        ),
        show.legend = FALSE
    ) +
    scale_color_manual(
        values = c(okabe_ito[2], okabe_ito[1]),
        aesthetics = c("color", "fill")
    ) +
    ggnewscale::new_scale_color() +
    geom_point(
        aes(
            # alpha = I(ifelse(true_decompose == 'Decomposes R-squared', 1, .25)),
            color = feature == "word_count",
            size = I(ifelse(feature == "word_count", 5, 4))
        ),
        alpha = 1,
        show.legend = FALSE
    ) +
    scale_color_manual(
        values = c(okabe_ito[1], okabe_ito[2]),
        # aesthetics = c("color", "fill")
    ) +
    # guides(color = 'none', fill = 'none', alpha = 'none') +
    labs(
        x = "Contribution to R-squared",
        y = "",
        title = "Relative Importance"
    ) +
    facet_wrap(vars(true_decompose, metric), scales = "free", ncol = 2, dir = "v") +
    theme(
        axis.ticks.y = element_blank(),
        plot.caption = element_text(vjust = -2)
    )

ggsave("img/fig-rsq-decomp.png", width = 8, height = 6)
```

- It rarely makes sense to drop features based on importance alone, and will typically drop performance to do so.


It depends on what you mean by importance, and what you are trying to do with the information.  There are many ways to measure feature importance, and they all have their pros and cons.

- Relative to... what?
- What will you do with the information?
- 


Where to begin? Feature importance is a very popular topic, and there are many ways to measure it.  The most common way is to use the SHAP values, which we just discussed.  But there are many other ways, and they all have their pros and cons.  

#### Visualization

##### Partial Dependency and ALE Plots

We can also look at the **partial dependency plot** (PDP).  The partial dependency plot shows the relationship between the feature and the target, but it does so by holding all other features constant. In this case, we can see that the relationship between word count and rating is negative, and that it's linear as it should be. PDPs are interesting in that, for the linear model setting, they plot the SHAP value for each feature value.  Related to PDPs are **individual conditional expectation** (ICE) plots, which are just a PDP for a specific observation, i.e. the plot of the SHAP values for a specific observation, if it had different values of the feature. A similar plot to the PDP is the **accumulated local effect** (ALE) plot, which is a bit more robust to correlated features, but is not so great with categorical features.


Unfortunately, PDPs are not as straightforward to interpret when we have correlated features and, though still applicable, become more difficult to interpret with interactions. **ALE** plots are a bit more robust to correlated features, but they are not so great with categorical features, and likewise still difficult to interpret in complicated feature settings.  We'll talk more about these later, but for now, we can see that the relationship between word count and rating is negative, and that it's linear as it should be. 


## Model Comparison

We can also look at the **Akaike Information Criterion** (AIC), which is a measure of model fit that takes into account the complexity of the model.  In our case, the value is `r aic`.  We won't go into the details of the AIC here, but it's a very popular metric for model comparison with statistical models and is related to the log likelihood, which is discussed in the @sec-estimation.  The value by itself is meaningless, and can only be used to to compare one model to another. Suffice it to say that the lower the AIC, the better the model, and we can use it to compare models with different features.  



## Assumptions and More



## Commentary

- Opinions
- Limitations/Failure points
- Summary




## Discard Pile

#### Quick Review: Interpreting our results

At this point we understand a few things:

- Coefficients or weights allow us to understand how a feature relates to the target.
- R-squared is the correlation of the target and the predictions (squared). By squaring we understand the proportion of variance in the target explained by the model.
- The residual variance is a summary of how well our model fits the data, and is basically the part our model doesn't explain.
- The likelihood is an alternate way to assess the match of data and model, and allows us to compare the relative fits of models
- Estimation is a way of finding the best fitting model.