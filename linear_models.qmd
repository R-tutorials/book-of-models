# The Foundation {-}

```{r, include = FALSE}
source('load_packages.R')
source('setup.R')
```

Intro

Key ideas

- Features, targets
- Input-output mappings
- Model Estimation
- Prediction
- Assumptions/Probabilistic outcomes/Uncertainty

Estimation

- OLS/MSE
- Maximum Likelihood (including logistic/poisson)
- Estimation as ‘Learning’
- Other Loss Functions Quick Demo

Commentary

- Opinions
- Limitations/Failure points
- Summary

## Introducing the Linear Model  (SOMETHING SNAPPIER)


Now that you have some idea of what you're getting into, it's time to dive in! In this chapter we're going to cover the building block of all modeling, and a solid understanding here will provide you the basis for the rest of what comes after, no matter how complex it gets. The **linear model** is our starting point. At first glance, it may seem like a very simple model, but it's actually quite powerful and flexible, able to take in different types of inputs, handle nonlinear relationships, temporal and spatial relations and more.  But before we start talking about the linear model, we need to talk about what a **model** is.

### What is a Model?

At its core a model is just an **idea**. It's a way of thinking about the world, about how things work, about how things are related to each other, about how things change over time, how things are different from each other, how they are the same. The underlying thread here is that **a model expresses relationships**.  On a practical level, the model is expressed through a particular language, math, but don't let that worry you if you're not so inclined. As it's still just an idea at its core, that's the most important thing to understand about a model.  The **math is just a way of expressing the idea** in a way that can be communicated and understood by others in a standard way, helping make the idea precise.  In general, we're trying to understand how ~~A~~ relates to ~~B~~, ~~inputs~~ produce ~~outputs~~. CHANGE THESE TERMS TO EVERYDAY EXAMPLES


### Features and Targets

In the context of a model, how we specify the nature of the relationship depends on the context. In the interest of generality, we'll refer to the **target** as what we want to explain, and **features** as those aspects of the data we will use to explain it. Because people come at data from a variety of contexts, they often use different terminology to mean the same thing. Some of these actually suggest a particular type of relationship (e.g., a causal relationship, an experimental setting), but we'll ignore that for now.  The table below shows some of the common terms used to refer to features and targets.

```{r tbl-feat-target, cache=FALSE}
#| echo: false
#| label: tbl-feature-target-names
#| tbl-cap: Common Terms for Features and Targets


tbl_feat_targ = tibble(
    Feature = c('independent variable', 'predictor variable', 'explanatory variable', 'covariate', 'x', 'input'),
    Target  = c('dependent variable', 'response', 'outcome', 'label', 'y', 'output'),
    ) |> 
    gt() |> 
    rm_caption() # does nothing

tbl_feat_targ
```

We may use any or all of these words to describe things so that you are comfortable with the terminology, but we'll stick with **features** and **targets** for the most part.


## From Input to Output

### Correlation

As noted,  a model is a way of expressing a relationship between a set of features and a target, and one way of expressing them is as inputs and outputs. But how can we go from input to output?  Well to begin, we assume that the features and target are **correlated**, i.e. that there is some relationship between the x and y.  If so, then we can ulitmately use the features to **predict** the target.  In the simplest setting a correlation implies a  relationship where x and y typically move up and down together (left plot) or they move in opposite directions where x goes up and y goes down.

```{r corr-plot}
#| echo: false
#| label: fig-corr-plot
#| fig-cap: Correlation
p_dat = tibble(
    x = rnorm(50),
    y = .75*x + rnorm(50, sd = .5),
    yneg = -.75*x + rnorm(50, sd = .5)
)

p1 = ggplot(p_dat, aes(x, y)) + 
    geom_point() + 
    labs(subtitle = 'Positive Correlation')
p2 = ggplot(p_dat, aes(x, yneg)) + 
    geom_point() + 
    labs(subtitle = 'Negative Correlation')

p1 + p2
```

- correlation
- simple sum (and as math equation)
- simple regression equation

### *THE* Linear Model

The linear model is perhaps the simplest *functional* model we can use to express a relationship between features and targets.  It's possibly still the most common model used in practice, and it's the basis for many other models.  The linear model is a **linear combination** of the features.  A linear combination is just a sum of the features multiplied by some constant.  In the case of the linear model, the constant is called a **coefficient**.  The linear model is expressed as:
omg an equation!

$$y = b_0 + b_1x_1 + b_2x_2 + ... + b_nx_n$$

- y is the target
- x1, x2, ... xn are the features
- b0 is the intercept
- b1, b2, ... bn are the coefficients



But lets start with something simpler, let's say you want to take a sum of several features. In OMG MATH you would write it as:

$$x_1 + x_2 + ... + x_n$$

In the previous equation, x is the feature and n is the number of features. `x` is an arbitrary designation, you could use any letter, symbol you want, or better would be the actual name. Now look at the linear model.


$$y = x_1 + x_2 + ... + x_n$$

In this case, the function is *just a sum*, something so simple we do it all the time. In the linear model sense though, we're actually saying a bit more. Another way to understand the previous is that *y is a function of x*.  In the linear model, we're saying that each feature contributes to the target, but they don't necessarily contribute equally. If we want to relate some cool x1 and some other cool x2 to hot target y, we probably would not assume that they both contribute in the same way.  We might give more weight to cool x1 than cool x2.  In the linear model, we express this by multiplying each feature by a coefficient.  So the linear model is really just a sum of the features multiplied by their coefficients.  In math, we would write it as:

$$y = b_0 + b_1x_1 + b_2x_2 + ... + b_nx_n$$

For starters, features are typically on different scales

In fact, we're saying that each feature contributes to the target in proportion to the coefficient.  So if we have a feature x1 and a coefficient b1, then the contribution of x1 to the target is b1*x1.  If we have a feature x2 and a coefficient b2, then the contribution of x2 to the target is b2*x2.  And so on.  So the linear model is really just a sum of the features multiplied by their coefficients.  In math, we would write it as:


> In chapter 0, maybe say something about science = prediction, but maybe also stay away from academic treament


make a latex table of three rows and two columns


```{r tbl-feat-target, cache=FALSE}
#| echo: false
#| label: tbl-lm-math
#| tbl-cap: Common Ways to Express the Linear Model


tbl_lm_math = tibble(
    RHS = c('$ X\beta $')
    ) |>   
    gt()  |> 
    as_latex()

tbl_lm_math
```


Here is a matrix multiplication formula:
$$
\begin{bmatrix} 1 & x_1 & x_2 & ... & x_n \end{bmatrix} \begin{bmatrix} b_0 \\ b_1 \\ b_2 \\ ... \\ b_n \end{bmatrix}
$$ {#eq-lm-mat-mult}


Here is test reference to the previous table See @tbl-feature-target-names . But this equation reference wroks work in quarto (@eq-lm-mat-mult). does this figure @fig-corr-plot work?  How about this one @fig-corr-plot2?

```{r corr-plot2}
#| echo: false
#| label: fig-corr-plot2
#| fig-cap: Correlation


plot(rnorm(10))
```