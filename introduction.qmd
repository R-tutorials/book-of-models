---
jupyter: 
  jupytext:
    formats: ipynb,md
    text_representation:
      extension: .md
      format_name: markdown
      format_version: '1.1'
      jupytext_version: 1.10.3
  kernelspec:
    display_name: Python 3
    language: python
    name: book-of-models
  # language_info:
  #   codemirror_mode:
  #     name: ipython
  #     version: 3
  #   file_extension: .py
  #   mimetype: text/x-python
  #   name: Python
  #   nbconvert_exporter: python
  #   pygments_lexer: ipython3
  #   version: 3.8.8
  # nteract:
  #   version: 0.27.0
---


# Introduction

Regardless of background, and whether we're conscious of it or not, we are constantly inundated with data. It's inescapable, from our first attempts to understand the world around us, to our most recent efforts to figure out why we still don't understand it. But if you're here reading this, you are the type of person that wants to keep trying! So for seasoned professionals or perhaps just the data curious, we want to help you learn more about how to use data to answer the questions you have.

## Who Should Use This Book?

If you consider yourself a data scientist, an analyst, or a statistical hobbyist, you already know that the best part of a good dive into data is the modeling. No matter what part of the data world you find yourself living, models give us the possibility to answer questions, make recommendations, and understand what we're interested in a little bit better. But no matter who you are, it isn't always easy to understand *how the models work*. Even when you do get a good grasp, it can get complicated and there are a lot of details. Maybe you just have other things going on in your life and have forgotten a few things. We find that it's always good to remind yourself of the basics!

Your humble authors have struggled mightily themselves throughout the course of their data history, and still do! We were initially taught by those that weren't exactly experts, and often found it difficult to get a good grasp of statistical modeling and machine learning. We've had to learn how to use the tools, how to interpret the results, and possibly the most difficult, how to explain what we're doing to others! We've forgotten a lot, confused ourselves, and made some happy accidents in the process. That's okay! Our goal here is to help you avoid some of those pitfalls, help you understand the basics of how models work, and get a sense of how most modeling endeavors have a lot of things in common. 

Let's look at the following equation for Pearson's correlation coefficient:

$$
r = \frac{\Sigma(x_i-\bar{x})(y_i-\bar{y})}{\sqrt{\Sigma(x_i-\bar{x})^2\Sigma(y_i-\bar{y})^2}}
$$

You don't need to memorize this formula to interpret the correlation results. We are the first to admit that we have long dumped the ability to pull formulas out of our brain folds; however, knowing how those individual pieces work together only helps to deepen your understanding of the model. Diving into the internal workings of models also allows you to see how they connect to each other -- this provides a nice roadmap from chapter to chapter. 

## Book Structure

For each model that we cover, you can expect the following:

	- Overview
	
	- Why it's useful
	
	- Conceptual example and interpretation
	
	- Where the model lies in the grand scheme of topics that we will cover
  
  - Key ideas and concepts
  
  - Demonstration with data, estimation/model code, and results/visualizations
  
  - Commentary, cautions, and where to explore next

The demonstrations will provide you the opportunity to get your hands dirty. In our demonstrations, we will present the code in two ways:

  - standard functions (e.g., `lm` in R)

  - the complete steps to recreate the formula

We are taking this approach for one reason: so that you can go as deep as you wish. If you are looking for a quick tutorial on helpful models, then you might not find yourself going any deeper than the standard functions (or even getting into the code at all). If you want to really dive into these models, then you might find yourself working through the complete steps. Another approach is to allow yourself some time between the standard functions and complete steps. You could work through the standard functions of every chapter, give it some time to marinate, and then work back through the complete steps. While we certainly recommend working through the chapters in order, we want to give you the flexibility to choose your own depth within each.

We hope that this book can serve as a "choose your own adventure" statistical reference. Whether you want a surface-level understanding, a deeper dive, or just want to be able to understand what the analysts in your organization are talking about, you will find value in this book. While we assume that you have a basic familiarity with coding, that doesn't mean that you need to work through every line of code to understand the fundamental principles of every model.

## Code

You've probably noticed a trend in books, blogs, and courses of choosing R or Python. While practitioners, authors, scholars, and professors alike often take an opinionated approach towards teaching one over the other, we eschew dogmatic approaches and language flame wars. R and Python are great languages (and equally flawed in unique ways) and it is in your best interest to be able to flip back and forth between the two. Throughout this book, we will be presenting ideas in both R and Python; you can use both or take your pick, but we want to leave that choice up to you. Our goal isn't to obscure the ideas behind packages and specialty functions, but to show you the most basic functions behind big model ideas. 

### A Code Example: Correlation

Let's look at the correlation formula again:

$$
r = \frac{\Sigma(x_i-\bar{x})(y_i-\bar{y})}{\sqrt{\Sigma(x_i-\bar{x})^2\Sigma(y_i-\bar{y})^2}}
$$
Just as a reminder, this correlation will give us an idea about the linear relationship between two continuous variables; we will get a value between -1 and 1, with values closer to 0 indicating that there is no linear relationship between the two variables. As values get closer to 1, we have a *positive correlation* -- as values for one variable increase, values for the other variable increase along with it. As values get closer to -1, we have a *negative correlation* -- as values for one variable increase, values for the other variable decrease along with it. 

We can start by generating *x* as a random sample from a standard normal distribution:

```{r}
set.seed(seed = 1001)

x <- rnorm(n = 1000, mean = 0, sd = 1)
```

And we can create a vector for *y*:

```{r}
y <- rnorm(n = 1000, mean = 5, sd = 2)
```

Before we break it down into the long form, let's see what R's `cor` function gives us:

```{r}
cor(x, y)
```

We have a negative value, but with a value so close to 0, this correlation is certainly nothing impressive -- we wouldn't expect impressive results with random numbers! We already know the answer, so let's make sure that we can get the same answer by working through the formula.

Since we have our x and y vectors already, we can compute the averages -- representing $\bar{x}$ and $\bar{y}$ -- for both:

```{r}
x_bar <- mean(x)

y_bar <- mean(y)
```

Let's work through the numerator first. For the first term, $x_i-\bar{x}$, we can subtract `x_bar` from every value of `x`. This tells us how much each observation of `x` differs from the mean of `x`. 

```{r}
x_minus_x_mean <- x - x_bar
```

And then repeat that for `y`:

```{r}
y_minus_y_mean <- y - y_bar
```

We can wrap up the numerator by summing those multiplied values:

```{r}
numerator <- sum(x_minus_x_mean * y_minus_y_mean)

numerator
```

Shifting to the denominator, we see that we have already completed a large portion of the work; we can take our `x_minus_x_mean` and `y_minus_y_mean` values, square each to make make them positive, and then sum each:

```{r}
x_minus_x_mean_squared <- sum(x_minus_x_mean^2)

y_minus_y_mean_squared <- sum(y_minus_y_mean^2)
```

To finish up the denominator, all we need to do is to multiply those terms and take the square root:

```{r}
denominator <- sqrt(x_minus_x_mean_squared * y_minus_y_mean_squared)
```

And we can finish it up:

```{r}
numerator / denominator
```

While the long form certainly took more lines of code, we can see that breaking it down into smaller steps isn't difficult. 

For the sake of completeness, let's see how we would do the same thing with Python. We can start by generating our `x` and `y` vectors with `numpy`:

```{python}
import numpy as np

np.random.seed(seed=1001)

x = np.random.normal(loc=0, scale=1, size=1000)

y = np.random.normal(loc=5, scale=2, size=1000)
```

Now we can easily get the correlation value:

```{python}
np.corrcoef(x, y)
```

Now we can go through the same steps to get the numerator value:

```{python}
x_bar = x.mean()

y_bar = y.mean()

x_minus_x_mean = x - x_bar

y_minus_y_mean = y - y_bar

numerator = sum(x_minus_x_mean * y_minus_y_mean)
```

So far so good! Let's put everything together for the denominator:

```{python}
x_minus_x_mean_squared = np.sum(x_minus_x_mean**2)

y_minus_y_mean_squared = np.sum(y_minus_y_mean**2)

denominator = np.sqrt(x_minus_x_mean_squared * y_minus_y_mean_squared)
```

The last step is still to divide the numerator by the denominator:

```{python}
numerator / denominator
```

