
@misc{noauthor_dive_2023,
	title = {Dive into {Deep} {Learning} — {Dive} into {Deep} {Learning} 1.0.3 documentation},
	url = {https://d2l.ai/index.html},
	urldate = {2023-12-02},
	year = {2023},
	file = {Dive into Deep Learning — Dive into Deep Learning 1.0.3 documentation:/Users/micl/Zotero/storage/NYIE2FYU/index.html:text/html},
}

@book{barrett_causal_2023,
	title = {Causal {Inference} in {R}},
	url = {https://www.r-causal.org/},
	language = {en},
	urldate = {2023-12-02},
	author = {Barrett, Malcolm and McGowan, Lucy D’Agostino and Gerke, Travis},
	month = nov,
	year = {2023},
	file = {Snapshot:/Users/micl/Zotero/storage/SDSTQ6EG/www.r-causal.org.html:text/html},
}

@book{hastie_elements_2017,
	title = {Elements of {Statistical} {Learning}: data mining, inference, and prediction. 2nd {Edition}.},
	url = {https://hastie.su.domains/ElemStatLearn/},
	urldate = {2023-12-02},
	author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
	year = {2017},
	file = {Elements of Statistical Learning\: data mining, inference, and prediction. 2nd Edition.:/Users/micl/Zotero/storage/YC7KFXJY/ElemStatLearn.html:text/html},
}

@book{goodfellow_deep_2016,
	title = {Deep {Learning}},
	url = {https://www.deeplearningbook.org/},
	urldate = {2023-12-02},
	author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
	year = {2016},
	file = {Deep Learning:/Users/micl/Zotero/storage/GKAVKPVP/www.deeplearningbook.org.html:text/html},
}

@misc{noauthor_116_2023,
	title = {1.16. {Probability} calibration},
	url = {https://scikit-learn/stable/modules/calibration.html},
	abstract = {When performing classification you often want not only to predict the class label, but also obtain a probability of the respective label. This probability gives you some kind of confidence on the p...},
	language = {en},
	urldate = {2023-12-02},
	journal = {scikit-learn},
	year = {2023},
	file = {Snapshot:/Users/micl/Zotero/storage/J44HYBR6/calibration.html:text/html},
}

@book{james_introduction_2021,
	address = {New York, NY},
	series = {Springer {Texts} in {Statistics}},
	title = {An {Introduction} to {Statistical} {Learning}},
	volume = {103},
	isbn = {978-1-4614-7137-0 978-1-4614-7138-7},
	url = {http://link.springer.com/10.1007/978-1-4614-7138-7},
	urldate = {2023-12-02},
	publisher = {Springer New York},
	author = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
	year = {2021},
	doi = {10.1007/978-1-4614-7138-7},
	file = {Submitted Version:/Users/micl/Zotero/storage/KS3SIBUU/James et al. - 2013 - An Introduction to Statistical Learning.pdf:application/pdf},
}

@article{lang_mlr3_2019,
	title = {mlr3: {A} modern object-oriented machine learning framework in {R}},
	volume = {4},
	issn = {2475-9066},
	shorttitle = {mlr3},
	url = {https://joss.theoj.org/papers/10.21105/joss.01903},
	doi = {10.21105/joss.01903},
	number = {44},
	urldate = {2023-12-02},
	journal = {Journal of Open Source Software},
	author = {Lang, Michel and Binder, Martin and Richter, Jakob and Schratz, Patrick and Pfisterer, Florian and Coors, Stefan and Au, Quay and Casalicchio, Giuseppe and Kotthoff, Lars and Bischl, Bernd},
	month = dec,
	year = {2019},
	pages = {1903},
	file = {Full Text:/Users/micl/Zotero/storage/4ZZCSE38/Lang et al. - 2019 - mlr3 A modern object-oriented machine learning fr.pdf:application/pdf},
}

@book{gelman_regression_2020,
	edition = {1},
	title = {Regression and {Other} {Stories}},
	isbn = {978-1-139-16187-9 978-1-107-02398-7 978-1-107-67651-0},
	url = {https://www.cambridge.org/highereducation/product/9781139161879/book},
	abstract = {Most textbooks on regression focus on theory and the simplest of examples. Real statistical problems, however, are complex and subtle. This is not a book about the theory of regression. It is about using regression to solve real problems of comparison, estimation, prediction, and causal inference. Unlike other books, it focuses on practical issues such as sample size and missing data and a wide range of goals and techniques. It jumps right in to methods and computer code you can use immediately. Real examples, real stories from the authors' experience demonstrate what regression can do and its limitations, with practical advice for understanding assumptions and implementing methods for experiments and observational studies. They make a smooth transition to logistic regression and GLM. The emphasis is on computation in R and Stan rather than derivations, with code available online. Graphics and presentation aid understanding of the models and model fitting.},
	urldate = {2023-12-02},
	publisher = {Cambridge University Press},
	author = {Gelman, Andrew and Hill, Jennifer and Vehtari, Aki},
	month = jul,
	year = {2020},
	doi = {10.1017/9781139161879},
}

@book{wooldridge_introductory_2012,
	address = {Mason, OH},
	edition = {5th edition},
	title = {Introductory {Econometrics}: {A} {Modern} {Approach}},
	isbn = {978-1-111-53104-1},
	shorttitle = {Introductory {Econometrics}},
	abstract = {Discover how empirical researchers today actually think about and apply econometric methods with the practical, professional approach in Wooldridge�s INTRODUCTORY ECONOMETRICS: A MODERN APPROACH, 5E. Unlike traditional books on the subject, INTRODUCTORY ECONOMETRICS� unique presentation demonstrates how econometrics has moved beyond just a set of abstract tools to become a genuinely useful tool for answering questions in business, policy evaluation, and forecasting environments. Organized around the type of data being analyzed, the book uses a systematic approach that only introduces assumptions as they are needed, which makes the material easier to understand and ultimately leads to better econometric practices. Packed with timely, relevant applications, the text emphasizes incorporates close to 100 intriguing data sets in six formats and offers updates that reflect the latest emerging developments in the field.},
	language = {English},
	publisher = {Cengage Learning},
	author = {Wooldridge, Jeffrey M.},
	month = sep,
	year = {2012},
}

@misc{noauthor_machine_nodate,
	title = {Machine {Learning} {\textbar} {Google} for {Developers}},
	url = {https://developers.google.com/machine-learning},
	abstract = {Educational resources for machine learning.},
	language = {en},
	urldate = {2023-12-02},
}

@article{rovine_peirce_2004,
	title = {Peirce and {Bowditch}},
	volume = {58},
	issn = {0003-1305},
	url = {https://doi.org/10.1198/000313004X964},
	doi = {10.1198/000313004X964},
	abstract = {Henry Pickering Bowditch and Charles Sanders Peirce made important contributions to the ideas of regression and correlation. This is particularly interesting as these contributions came well before the work of Galton and Pearson. This article discusses the work of Bowditch related to the development of regression and presents Peirce's coefficient of the science of the method, an association coefficient for a 2 × 2 contingency table.},
	number = {3},
	urldate = {2023-12-02},
	journal = {The American Statistician},
	author = {Rovine, Michael J and Anderson, Douglas R},
	month = aug,
	year = {2004},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1198/000313004X964},
	keywords = {Association, Coefficient, History of statistics},
	pages = {232--236},
}

@book{grolemund_welcome_nodate,
	title = {Welcome {\textbar} {R} for {Data} {Science}},
	url = {https://r4ds.had.co.nz/},
	abstract = {This book will teach you how to do data science with R: You’ll learn how to get your data into R, get it into the most useful structure, transform it, visualise it and model it. In this book, you will find a practicum of skills for data science. Just as a chemist learns how to clean test tubes and stock a lab, you’ll learn how to clean data and draw plots—and many other things besides. These are the skills that allow data science to happen, and here you will find the best practices for doing each of these things with R. You’ll learn how to use the grammar of graphics, literate programming, and reproducible research to save time. You’ll also learn how to manage cognitive resources to facilitate discoveries when wrangling, visualising, and exploring data.},
	language = {en},
	urldate = {2023-12-02},
	author = {Grolemund, Hadley Wickham {and} Garrett},
	file = {Snapshot:/Users/micl/Zotero/storage/MKUR5VY6/r4ds.had.co.nz.html:text/html},
}

@book{noauthor_python_nodate,
	title = {Python {Data} {Science} {Handbook} {\textbar} {Python} {Data} {Science} {Handbook}},
	url = {https://jakevdp.github.io/PythonDataScienceHandbook/},
	urldate = {2023-12-02},
	file = {Python Data Science Handbook | Python Data Science Handbook:/Users/micl/Zotero/storage/8Q2C7ZGH/PythonDataScienceHandbook.html:text/html},
}

@book{greene_econometric_2017,
	title = {Econometric {Analysis} - 8th {Edition}},
	url = {https://pages.stern.nyu.edu/~wgreene/Text/econometricanalysis.htm},
	urldate = {2023-12-02},
	author = {Greene, William},
	year = {2017},
	file = {Econometric Analysis - 7th Edition:/Users/micl/Zotero/storage/RJAENYBP/econometricanalysis.html:text/html},
}

@misc{brownlee_gentle_2016,
	title = {Gentle {Introduction} to the {Bias}-{Variance} {Trade}-{Off} in {Machine} {Learning}},
	url = {https://machinelearningmastery.com/gentle-introduction-to-the-bias-variance-trade-off-in-machine-learning/},
	abstract = {Supervised machine learning algorithms can best be understood through the lens of the bias-variance trade-off. In this post, you will discover the Bias-Variance Trade-Off and how to use it to better understand machine learning algorithms and get better performance on your data. Let’s get started. Update Oct/2019: Removed discussion of parametric/nonparametric models (thanks Alex). Overview […]},
	language = {en-US},
	urldate = {2023-12-03},
	journal = {MachineLearningMastery.com},
	author = {Brownlee, Jason},
	month = mar,
	year = {2016},
	file = {Snapshot:/Users/micl/Zotero/storage/GBAUFIA6/gentle-introduction-to-the-bias-variance-trade-off-in-machine-learning.html:text/html},
}

@misc{noauthor_introduction_nodate,
	title = {Introduction {\textbar} {Machine} {Learning}},
	url = {https://developers.google.com/machine-learning/decision-forests},
	language = {en},
	urldate = {2023-12-03},
	journal = {Google for Developers},
	file = {Snapshot:/Users/micl/Zotero/storage/PJY3UGGV/decision-forests.html:text/html},
}

@misc{noauthor_nested_nodate,
	title = {Nested versus non-nested cross-validation},
	url = {https://scikit-learn/stable/auto_examples/model_selection/plot_nested_cross_validation_iris.html},
	abstract = {This example compares non-nested and nested cross-validation strategies on a classifier of the iris data set. Nested cross-validation (CV) is often used to train a model in which hyperparameters al...},
	language = {en},
	urldate = {2023-12-03},
	journal = {scikit-learn},
	file = {Snapshot:/Users/micl/Zotero/storage/9FV6VZX3/plot_nested_cross_validation_iris.html:text/html},
}

@misc{schmidhuber_annotated_2022,
	title = {Annotated {History} of {Modern} {AI} and {Deep} {Learning}},
	url = {http://arxiv.org/abs/2212.11279},
	doi = {10.48550/arXiv.2212.11279},
	abstract = {Machine learning is the science of credit assignment: finding patterns in observations that predict the consequences of actions and help to improve future performance. Credit assignment is also required for human understanding of how the world works, not only for individuals navigating daily life, but also for academic professionals like historians who interpret the present in light of past events. Here I focus on the history of modern artificial intelligence (AI) which is dominated by artificial neural networks (NNs) and deep learning, both conceptually closer to the old field of cybernetics than to what's been called AI since 1956 (e.g., expert systems and logic programming). A modern history of AI will emphasize breakthroughs outside of the focus of traditional AI text books, in particular, mathematical foundations of today's NNs such as the chain rule (1676), the first NNs (linear regression, circa 1800), and the first working deep learners (1965-). From the perspective of 2022, I provide a timeline of the -- in hindsight -- most important relevant events in the history of NNs, deep learning, AI, computer science, and mathematics in general, crediting those who laid foundations of the field. The text contains numerous hyperlinks to relevant overview sites from my AI Blog. It supplements my previous deep learning survey (2015) which provides hundreds of additional references. Finally, to round it off, I'll put things in a broader historic context spanning the time since the Big Bang until when the universe will be many times older than it is now.},
	urldate = {2023-12-03},
	publisher = {arXiv},
	author = {Schmidhuber, Juergen},
	month = dec,
	year = {2022},
	note = {arXiv:2212.11279 [cs]},
	keywords = {Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: 75 pages, over 500 references. arXiv admin note: substantial text overlap with arXiv:2005.05744},
	file = {arXiv Fulltext PDF:/Users/micl/Zotero/storage/G2XW58H9/Schmidhuber - 2022 - Annotated History of Modern AI and Deep Learning.pdf:application/pdf;arXiv.org Snapshot:/Users/micl/Zotero/storage/P75W945I/2212.html:text/html},
}

@article{welchowski_techniques_2022,
	title = {Techniques to {Improve} {Ecological} {Interpretability} of {Black}-{Box} {Machine} {Learning} {Models}},
	volume = {27},
	issn = {1537-2693},
	url = {https://doi.org/10.1007/s13253-021-00479-7},
	doi = {10.1007/s13253-021-00479-7},
	abstract = {Statistical modeling of ecological data is often faced with a large number of variables as well as possible nonlinear relationships and higher-order interaction effects. Gradient boosted trees (GBT) have been successful in addressing these issues and have shown a good predictive performance in modeling nonlinear relationships, in particular in classification settings with a categorical response variable.  They also tend to be robust against outliers. However, their black-box nature makes it difficult to interpret these models. We introduce several recently developed statistical tools to the environmental research community in order to advance interpretation of these black-box models. To analyze the properties of the tools, we applied gradient boosted trees to investigate biological health of streams within the contiguous USA, as measured by a benthic macroinvertebrate biotic index. Based on these data and a simulation study, we demonstrate the advantages and limitations of partial dependence plots (PDP), individual conditional expectation (ICE) curves and accumulated local effects (ALE) in their ability to identify covariate–response relationships. Additionally, interaction effects were quantified according to interaction strength (IAS) and Friedman’s \$\$H{\textasciicircum}2\$\$statistic. Interpretable machine learning techniques are useful tools to open the black-box of gradient boosted trees in the environmental sciences. This finding is supported by our case study on the effect of impervious surface on the benthic condition, which agrees with previous results in the literature. Overall, the most important variables were ecoregion, bed stability, watershed area, riparian vegetation and catchment slope. These variables were also present in most identified interaction effects. In conclusion, graphical tools (PDP, ICE, ALE) enable visualization and easier interpretation of GBT but should be supported by analytical statistical measures. Future methodological research is needed to investigate the properties of interaction tests.      Supplementary materials accompanying this paper appear on-line.},
	language = {en},
	number = {1},
	urldate = {2023-12-03},
	journal = {Journal of Agricultural, Biological and Environmental Statistics},
	author = {Welchowski, Thomas and Maloney, Kelly O. and Mitchell, Richard and Schmid, Matthias},
	month = mar,
	year = {2022},
	keywords = {Boosting, Interaction terms, Interpretable machine learning, Macroinvertebrates, Stream health},
	pages = {175--197},
	file = {Full Text PDF:/Users/micl/Zotero/storage/TPMT2XPT/Welchowski et al. - 2022 - Techniques to Improve Ecological Interpretability .pdf:application/pdf},
}

@book{burzykowski_explanatory_2020,
	title = {Explanatory {Model} {Analysis}},
	url = {https://ema.drwhy.ai/},
	abstract = {This book introduces unified language for exploration, explanation and examination of predictive machine learning models.},
	urldate = {2023-12-03},
	author = {Burzykowski, Przemyslaw Biecek {and} Tomasz},
	year = {2020},
	file = {Snapshot:/Users/micl/Zotero/storage/C3ZUM24Y/ema.drwhy.ai.html:text/html},
}
