
@book{barrett_causal_2023,
	title = {Causal {Inference} in {R}},
	url = {https://www.r-causal.org/},
	language = {en},
	urldate = {2023-12-02},
	author = {Barrett, Malcolm and McGowan, Lucy D’Agostino and Gerke, Travis},
	month = nov,
	year = {2023},
	file = {Snapshot:/Users/micl/Zotero/storage/SDSTQ6EG/www.r-causal.org.html:text/html},
}

@book{hastie_elements_2017,
	title = {Elements of {Statistical} {Learning}: data mining, inference, and prediction. 2nd {Edition}.},
	url = {https://hastie.su.domains/ElemStatLearn/},
	urldate = {2023-12-02},
	author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
	year = {2017},
	file = {Elements of Statistical Learning\: data mining, inference, and prediction. 2nd Edition.:/Users/micl/Zotero/storage/YC7KFXJY/ElemStatLearn.html:text/html},
}

@book{goodfellow_deep_2016,
	title = {Deep {Learning}},
	url = {https://www.deeplearningbook.org/},
	urldate = {2023-12-02},
	author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
	year = {2016},
	file = {Deep Learning:/Users/micl/Zotero/storage/GKAVKPVP/www.deeplearningbook.org.html:text/html},
}

@misc{scikit-learn_116_2023,
	title = {1.16. {Probability} calibration},
	url = {https://scikit-learn/stable/modules/calibration.html},
	abstract = {When performing classification you often want not only to predict the class label, but also obtain a probability of the respective label. This probability gives you some kind of confidence on the p...},
	language = {en},
	urldate = {2023-12-02},
	journal = {scikit-learn},
	author = {scikit-learn},
	year = {2023},
	file = {Snapshot:/Users/micl/Zotero/storage/J44HYBR6/calibration.html:text/html},
}

@book{james_introduction_2021,
	address = {New York, NY},
	series = {Springer {Texts} in {Statistics}},
	title = {An {Introduction} to {Statistical} {Learning}},
	volume = {103},
	isbn = {978-1-4614-7137-0 978-1-4614-7138-7},
	url = {http://link.springer.com/10.1007/978-1-4614-7138-7},
	urldate = {2023-12-02},
	publisher = {Springer New York},
	author = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
	year = {2021},
	doi = {10.1007/978-1-4614-7138-7},
	file = {Submitted Version:/Users/micl/Zotero/storage/KS3SIBUU/James et al. - 2013 - An Introduction to Statistical Learning.pdf:application/pdf},
}

@article{lang_mlr3_2019,
	title = {mlr3: {A} modern object-oriented machine learning framework in {R}},
	volume = {4},
	issn = {2475-9066},
	shorttitle = {mlr3},
	url = {https://joss.theoj.org/papers/10.21105/joss.01903},
	doi = {10.21105/joss.01903},
	number = {44},
	urldate = {2023-12-02},
	journal = {Journal of Open Source Software},
	author = {Lang, Michel and Binder, Martin and Richter, Jakob and Schratz, Patrick and Pfisterer, Florian and Coors, Stefan and Au, Quay and Casalicchio, Giuseppe and Kotthoff, Lars and Bischl, Bernd},
	month = dec,
	year = {2019},
	pages = {1903},
	file = {Full Text:/Users/micl/Zotero/storage/4ZZCSE38/Lang et al. - 2019 - mlr3 A modern object-oriented machine learning fr.pdf:application/pdf},
}

@book{gelman_regression_2020,
	edition = {1},
	title = {Regression and {Other} {Stories}},
	isbn = {978-1-139-16187-9 978-1-107-02398-7 978-1-107-67651-0},
	url = {https://www.cambridge.org/highereducation/product/9781139161879/book},
	abstract = {Most textbooks on regression focus on theory and the simplest of examples. Real statistical problems, however, are complex and subtle. This is not a book about the theory of regression. It is about using regression to solve real problems of comparison, estimation, prediction, and causal inference. Unlike other books, it focuses on practical issues such as sample size and missing data and a wide range of goals and techniques. It jumps right in to methods and computer code you can use immediately. Real examples, real stories from the authors' experience demonstrate what regression can do and its limitations, with practical advice for understanding assumptions and implementing methods for experiments and observational studies. They make a smooth transition to logistic regression and GLM. The emphasis is on computation in R and Stan rather than derivations, with code available online. Graphics and presentation aid understanding of the models and model fitting.},
	urldate = {2023-12-02},
	publisher = {Cambridge University Press},
	author = {Gelman, Andrew and Hill, Jennifer and Vehtari, Aki},
	month = jul,
	year = {2020},
	doi = {10.1017/9781139161879},
}

@book{wooldridge_introductory_2012,
	address = {Mason, OH},
	edition = {5th edition},
	title = {Introductory {Econometrics}: {A} {Modern} {Approach}},
	isbn = {978-1-111-53104-1},
	shorttitle = {Introductory {Econometrics}},
	abstract = {Discover how empirical researchers today actually think about and apply econometric methods with the practical, professional approach in Wooldridge�s INTRODUCTORY ECONOMETRICS: A MODERN APPROACH, 5E. Unlike traditional books on the subject, INTRODUCTORY ECONOMETRICS� unique presentation demonstrates how econometrics has moved beyond just a set of abstract tools to become a genuinely useful tool for answering questions in business, policy evaluation, and forecasting environments. Organized around the type of data being analyzed, the book uses a systematic approach that only introduces assumptions as they are needed, which makes the material easier to understand and ultimately leads to better econometric practices. Packed with timely, relevant applications, the text emphasizes incorporates close to 100 intriguing data sets in six formats and offers updates that reflect the latest emerging developments in the field.},
	language = {English},
	publisher = {Cengage Learning},
	author = {Wooldridge, Jeffrey M.},
	month = sep,
	year = {2012},
}

@misc{google_machine_nodate,
	title = {Machine {Learning} {\textbar} {Google} for {Developers}},
	url = {https://developers.google.com/machine-learning},
	abstract = {Educational resources for machine learning.},
	language = {en},
	urldate = {2023-12-02},
	author = {Google},
}

@article{rovine_peirce_2004,
	title = {Peirce and {Bowditch}},
	volume = {58},
	issn = {0003-1305},
	url = {https://doi.org/10.1198/000313004X964},
	doi = {10.1198/000313004X964},
	abstract = {Henry Pickering Bowditch and Charles Sanders Peirce made important contributions to the ideas of regression and correlation. This is particularly interesting as these contributions came well before the work of Galton and Pearson. This article discusses the work of Bowditch related to the development of regression and presents Peirce's coefficient of the science of the method, an association coefficient for a 2 × 2 contingency table.},
	number = {3},
	urldate = {2023-12-02},
	journal = {The American Statistician},
	author = {Rovine, Michael J and Anderson, Douglas R},
	month = aug,
	year = {2004},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1198/000313004X964},
	keywords = {Association, Coefficient, History of statistics},
	pages = {232--236},
}

@book{grolemund_welcome_nodate,
	title = {Welcome {\textbar} {R} for {Data} {Science}},
	url = {https://r4ds.had.co.nz/},
	abstract = {This book will teach you how to do data science with R: You’ll learn how to get your data into R, get it into the most useful structure, transform it, visualise it and model it. In this book, you will find a practicum of skills for data science. Just as a chemist learns how to clean test tubes and stock a lab, you’ll learn how to clean data and draw plots—and many other things besides. These are the skills that allow data science to happen, and here you will find the best practices for doing each of these things with R. You’ll learn how to use the grammar of graphics, literate programming, and reproducible research to save time. You’ll also learn how to manage cognitive resources to facilitate discoveries when wrangling, visualising, and exploring data.},
	language = {en},
	urldate = {2023-12-02},
	author = {Grolemund, Hadley Wickham {and} Garrett},
	file = {Snapshot:/Users/micl/Zotero/storage/MKUR5VY6/r4ds.had.co.nz.html:text/html},
}

@book{greene_econometric_2017,
	title = {Econometric {Analysis} - 8th {Edition}},
	url = {https://pages.stern.nyu.edu/~wgreene/Text/econometricanalysis.htm},
	urldate = {2023-12-02},
	author = {Greene, William},
	year = {2017},
	file = {Econometric Analysis - 7th Edition:/Users/micl/Zotero/storage/RJAENYBP/econometricanalysis.html:text/html},
}

@misc{brownlee_gentle_2016,
	title = {Gentle {Introduction} to the {Bias}-{Variance} {Trade}-{Off} in {Machine} {Learning}},
	url = {https://machinelearningmastery.com/gentle-introduction-to-the-bias-variance-trade-off-in-machine-learning/},
	abstract = {Supervised machine learning algorithms can best be understood through the lens of the bias-variance trade-off. In this post, you will discover the Bias-Variance Trade-Off and how to use it to better understand machine learning algorithms and get better performance on your data. Let’s get started. Update Oct/2019: Removed discussion of parametric/nonparametric models (thanks Alex). Overview […]},
	language = {en-US},
	urldate = {2023-12-03},
	journal = {MachineLearningMastery.com},
	author = {Brownlee, Jason},
	month = mar,
	year = {2016},
	file = {Snapshot:/Users/micl/Zotero/storage/GBAUFIA6/gentle-introduction-to-the-bias-variance-trade-off-in-machine-learning.html:text/html},
}

@misc{google_introduction_nodate,
	title = {Introduction {\textbar} {Machine} {Learning}},
	url = {https://developers.google.com/machine-learning/decision-forests},
	language = {en},
	urldate = {2023-12-03},
	journal = {Google for Developers},
	author = {Google},
	file = {Snapshot:/Users/micl/Zotero/storage/PJY3UGGV/decision-forests.html:text/html},
}

@misc{scikit-learn_nested_nodate,
	title = {Nested versus non-nested cross-validation},
	url = {https://scikit-learn/stable/auto_examples/model_selection/plot_nested_cross_validation_iris.html},
	abstract = {This example compares non-nested and nested cross-validation strategies on a classifier of the iris data set. Nested cross-validation (CV) is often used to train a model in which hyperparameters al...},
	language = {en},
	urldate = {2023-12-03},
	journal = {scikit-learn},
	author = {scikit-learn},
	file = {Snapshot:/Users/micl/Zotero/storage/9FV6VZX3/plot_nested_cross_validation_iris.html:text/html},
}

@misc{schmidhuber_annotated_2022,
	title = {Annotated {History} of {Modern} {AI} and {Deep} {Learning}},
	url = {http://arxiv.org/abs/2212.11279},
	doi = {10.48550/arXiv.2212.11279},
	abstract = {Machine learning is the science of credit assignment: finding patterns in observations that predict the consequences of actions and help to improve future performance. Credit assignment is also required for human understanding of how the world works, not only for individuals navigating daily life, but also for academic professionals like historians who interpret the present in light of past events. Here I focus on the history of modern artificial intelligence (AI) which is dominated by artificial neural networks (NNs) and deep learning, both conceptually closer to the old field of cybernetics than to what's been called AI since 1956 (e.g., expert systems and logic programming). A modern history of AI will emphasize breakthroughs outside of the focus of traditional AI text books, in particular, mathematical foundations of today's NNs such as the chain rule (1676), the first NNs (linear regression, circa 1800), and the first working deep learners (1965-). From the perspective of 2022, I provide a timeline of the -- in hindsight -- most important relevant events in the history of NNs, deep learning, AI, computer science, and mathematics in general, crediting those who laid foundations of the field. The text contains numerous hyperlinks to relevant overview sites from my AI Blog. It supplements my previous deep learning survey (2015) which provides hundreds of additional references. Finally, to round it off, I'll put things in a broader historic context spanning the time since the Big Bang until when the universe will be many times older than it is now.},
	urldate = {2023-12-03},
	publisher = {arXiv},
	author = {Schmidhuber, Juergen},
	month = dec,
	year = {2022},
	note = {arXiv:2212.11279 [cs]},
	keywords = {Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: 75 pages, over 500 references. arXiv admin note: substantial text overlap with arXiv:2005.05744},
	file = {arXiv Fulltext PDF:/Users/micl/Zotero/storage/G2XW58H9/Schmidhuber - 2022 - Annotated History of Modern AI and Deep Learning.pdf:application/pdf;arXiv.org Snapshot:/Users/micl/Zotero/storage/P75W945I/2212.html:text/html},
}

@article{welchowski_techniques_2022,
	title = {Techniques to {Improve} {Ecological} {Interpretability} of {Black}-{Box} {Machine} {Learning} {Models}},
	volume = {27},
	issn = {1537-2693},
	url = {https://doi.org/10.1007/s13253-021-00479-7},
	doi = {10.1007/s13253-021-00479-7},
	abstract = {Statistical modeling of ecological data is often faced with a large number of variables as well as possible nonlinear relationships and higher-order interaction effects. Gradient boosted trees (GBT) have been successful in addressing these issues and have shown a good predictive performance in modeling nonlinear relationships, in particular in classification settings with a categorical response variable.  They also tend to be robust against outliers. However, their black-box nature makes it difficult to interpret these models. We introduce several recently developed statistical tools to the environmental research community in order to advance interpretation of these black-box models. To analyze the properties of the tools, we applied gradient boosted trees to investigate biological health of streams within the contiguous USA, as measured by a benthic macroinvertebrate biotic index. Based on these data and a simulation study, we demonstrate the advantages and limitations of partial dependence plots (PDP), individual conditional expectation (ICE) curves and accumulated local effects (ALE) in their ability to identify covariate–response relationships. Additionally, interaction effects were quantified according to interaction strength (IAS) and Friedman’s \$\$H{\textasciicircum}2\$\$statistic. Interpretable machine learning techniques are useful tools to open the black-box of gradient boosted trees in the environmental sciences. This finding is supported by our case study on the effect of impervious surface on the benthic condition, which agrees with previous results in the literature. Overall, the most important variables were ecoregion, bed stability, watershed area, riparian vegetation and catchment slope. These variables were also present in most identified interaction effects. In conclusion, graphical tools (PDP, ICE, ALE) enable visualization and easier interpretation of GBT but should be supported by analytical statistical measures. Future methodological research is needed to investigate the properties of interaction tests.      Supplementary materials accompanying this paper appear on-line.},
	language = {en},
	number = {1},
	urldate = {2023-12-03},
	journal = {Journal of Agricultural, Biological and Environmental Statistics},
	author = {Welchowski, Thomas and Maloney, Kelly O. and Mitchell, Richard and Schmid, Matthias},
	month = mar,
	year = {2022},
	keywords = {Boosting, Interaction terms, Interpretable machine learning, Macroinvertebrates, Stream health},
	pages = {175--197},
	file = {Full Text PDF:/Users/micl/Zotero/storage/TPMT2XPT/Welchowski et al. - 2022 - Techniques to Improve Ecological Interpretability .pdf:application/pdf},
}

@book{burzykowski_explanatory_2020,
	title = {Explanatory {Model} {Analysis}},
	url = {https://ema.drwhy.ai/},
	abstract = {This book introduces unified language for exploration, explanation and examination of predictive machine learning models.},
	urldate = {2023-12-03},
	author = {Burzykowski, Przemyslaw Biecek {and} Tomasz},
	year = {2020},
	file = {Snapshot:/Users/micl/Zotero/storage/C3ZUM24Y/ema.drwhy.ai.html:text/html},
}

@misc{bycroft_llm_nodate,
	title = {{LLM} {Visualization}},
	url = {https://bbycroft.net/llm},
	urldate = {2023-12-03},
	author = {Bycroft, Brendan},
	file = {LLM Visualization:/Users/micl/Zotero/storage/46AYQEXH/llm.html:text/html},
}

@article{kunzel_metalearners_2019,
	title = {Metalearners for estimating heterogeneous treatment effects using machine learning},
	volume = {116},
	url = {https://www.pnas.org/doi/abs/10.1073/pnas.1804597116},
	doi = {10.1073/pnas.1804597116},
	abstract = {There is growing interest in estimating and analyzing heterogeneous treatment effects in experimental and observational studies. We describe a number of metaalgorithms that can take advantage of any supervised learning or regression method in machine learning and statistics to estimate the conditional average treatment effect (CATE) function. Metaalgorithms build on base algorithms—such as random forests (RFs), Bayesian additive regression trees (BARTs), or neural networks—to estimate the CATE, a function that the base algorithms are not designed to estimate directly. We introduce a metaalgorithm, the X-learner, that is provably efficient when the number of units in one treatment group is much larger than in the other and can exploit structural properties of the CATE function. For example, if the CATE function is linear and the response functions in treatment and control are Lipschitz-continuous, the X-learner can still achieve the parametric rate under regularity conditions. We then introduce versions of the X-learner that use RF and BART as base learners. In extensive simulation studies, the X-learner performs favorably, although none of the metalearners is uniformly the best. In two persuasion field experiments from political science, we demonstrate how our X-learner can be used to target treatment regimes and to shed light on underlying mechanisms. A software package is provided that implements our methods.},
	number = {10},
	urldate = {2023-12-09},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Künzel, Sören R. and Sekhon, Jasjeet S. and Bickel, Peter J. and Yu, Bin},
	month = mar,
	year = {2019},
	note = {Publisher: Proceedings of the National Academy of Sciences},
	pages = {4156--4165},
	file = {Full Text PDF:/Users/micl/Zotero/storage/MKR9FWQJ/Künzel et al. - 2019 - Metalearners for estimating heterogeneous treatmen.pdf:application/pdf},
}

@article{pearl_causal_2009,
	title = {Causal inference in statistics: {An} overview},
	volume = {3},
	issn = {1935-7516},
	shorttitle = {Causal inference in statistics},
	url = {https://projecteuclid.org/journals/statistics-surveys/volume-3/issue-none/Causal-inference-in-statistics-An-overview/10.1214/09-SS057.full},
	doi = {10.1214/09-SS057},
	abstract = {This review presents empirical researchers with recent advances in causal inference, and stresses the paradigmatic shifts that must be undertaken in moving from traditional statistical analysis to causal analysis of multivariate data. Special emphasis is placed on the assumptions that underly all causal inferences, the languages used in formulating those assumptions, the conditional nature of all causal and counterfactual claims, and the methods that have been developed for the assessment of such claims. These advances are illustrated using a general theory of causation based on the Structural Causal Model (SCM) described in Pearl (2000a), which subsumes and unifies other approaches to causation, and provides a coherent mathematical foundation for the analysis of causes and counterfactuals. In particular, the paper surveys the development of mathematical tools for inferring (from a combination of data and assumptions) answers to three types of causal queries: (1) queries about the effects of potential interventions, (also called “causal effects” or “policy evaluation”) (2) queries about probabilities of counterfactuals, (including assessment of “regret,” “attribution” or “causes of effects”) and (3) queries about direct and indirect effects (also known as “mediation”). Finally, the paper defines the formal and conceptual relationships between the structural and potential-outcome frameworks and presents tools for a symbiotic analysis that uses the strong features of both.},
	number = {none},
	urldate = {2023-12-10},
	journal = {Statistics Surveys},
	author = {Pearl, Judea},
	month = jan,
	year = {2009},
	note = {Publisher: Amer. Statist. Assoc., the Bernoulli Soc., the Inst. Math. Statist., and the Statist. Soc. Canada},
	keywords = {causal effects, causes of effects, confounding, counterfactuals, graphical methods, mediation, policy evaluation, potential-outcome, structural equation models},
	pages = {96--146},
	file = {Full Text PDF:/Users/micl/Zotero/storage/QEVSNU5U/Pearl - 2009 - Causal inference in statistics An overview.pdf:application/pdf},
}

@misc{pearl_causal_nodate,
	title = {Causal {Inference}: {History}, {Perspectives}, {Adventures}, and {Unification} ({An} {Interview} with {Judea} {Pearl})},
	url = {https://muse.jhu.edu/pub/56/article/867087/summary},
	urldate = {2023-12-10},
	author = {Pearl, Judea},
	file = {Project MUSE - Causal Inference\: History, Perspectives, Adventures, and Unification (An Interview with Judea Pearl):/Users/micl/Zotero/storage/AESKD3IT/summary.html:text/html},
}

@article{morgan_counterfactuals_2014,
	title = {Counterfactuals and {Causal} {Inference}: {Methods} and {Principles} for {Social} {Research}, 2nd {Edition}},
	shorttitle = {Counterfactuals and {Causal} {Inference}},
	url = {https://stars.library.ucf.edu/etextbooks/298},
	author = {Morgan, Stephen and Winship, Christopher},
	month = jan,
	year = {2014},
	file = {"Counterfactuals and Causal Inference\: Methods and Principles for Socia" by Stephen L. Morgan and Christopher Winship:/Users/micl/Zotero/storage/U74QKHFA/298.html:text/html},
}

@misc{facure_alves_causal_2022,
	title = {Causal {Inference} for {The} {Brave} and {True} — {Causal} {Inference} for the {Brave} and {True}},
	url = {https://matheusfacure.github.io/python-causality-handbook/landing-page.html},
	urldate = {2023-12-10},
	author = {Facure Alves, Matheus},
	year = {2022},
	file = {Causal Inference for The Brave and True — Causal Inference for the Brave and True:/Users/micl/Zotero/storage/ZMZM2BRJ/landing-page.html:text/html},
}

@book{molnar_interpretable_nodate,
	title = {Interpretable {Machine} {Learning}},
	url = {https://christophm.github.io/interpretable-ml-book/},
	abstract = {Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners to make machine learning decisions interpretable.},
	urldate = {2023-12-10},
	author = {Molnar, Christoph},
	file = {Snapshot:/Users/micl/Zotero/storage/F775EJWY/interpretable-ml-book.html:text/html},
}

@misc{pok_how_nodate,
	title = {How uplift modeling works {\textbar} {Blogs}},
	url = {https://ambiata.com/blog/2020-07-07-uplift-modeling/},
	urldate = {2023-12-10},
	author = {Pok, Wilson},
	file = {How uplift modeling works | Blogs:/Users/micl/Zotero/storage/QJDBLXPQ/2020-07-07-uplift-modeling.html:text/html},
}

@misc{shevchenko_types_nodate,
	title = {Types of customers — scikit-uplift 0.3.1 documentation},
	url = {https://www.uplift-modeling.com/en/v0.5.1/user_guide/introduction/clients.html},
	urldate = {2023-12-10},
	author = {Shevchenko, Maksim},
	file = {Types of customers — scikit-uplift 0.3.1 documentation:/Users/micl/Zotero/storage/6RB8SG2A/clients.html:text/html},
}

@misc{zhang_dive_nodate,
	title = {Dive into {Deep} {Learning} — {Dive} into {Deep} {Learning} 1.0.3 documentation},
	url = {https://d2l.ai/index.html},
	urldate = {2023-12-10},
	author = {Zhang, Aston and Lipton, Zack and Li, Mu and Smola, Alex},
	file = {Dive into Deep Learning — Dive into Deep Learning 1.0.3 documentation:/Users/micl/Zotero/storage/FYUH75CV/index.html:text/html},
}

@misc{vanderplas_python_2016,
	title = {Python {Data} {Science} {Handbook} [{Book}]},
	url = {https://www.oreilly.com/library/view/python-data-science/9781491912126/},
	abstract = {For many researchers, Python is a first-class tool mainly because of its libraries for storing, manipulating, and gaining insight from data. Several resources exist for individual pieces of this data … - Selection from Python Data Science Handbook [Book]},
	language = {en},
	urldate = {2023-12-10},
	author = {VanderPlas, Jake},
	year = {2016},
	note = {ISBN: 9781491912058},
	file = {Snapshot:/Users/micl/Zotero/storage/PS9825NX/9781491912126.html:text/html},
}

@book{cunningham_causal_2023,
	title = {Causal {Inference} {The} {Mixtape}},
	url = {https://mixtape.scunning.com/},
	urldate = {2023-12-10},
	author = {Cunningham, Scott},
	year = {2023},
	file = {Causal Inference The Mixtape:/Users/micl/Zotero/storage/5FEHRQ3X/mixtape.scunning.com.html:text/html},
}
