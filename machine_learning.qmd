
# Machine Learning

```{r}
#| include: False
#| label: setup-ml
source("load_packages.R")
source("setup.R")

library(tidyverse)
reticulate::use_condaenv("book-of-models")
```

Packages needed or useful for this chapter include:

```{r}

```


Machine Learning 

- Intro 
    - ML as a modeling focus
- Key Ideas
    - Performance, Loss functions, Tuning/Cross-validation
- Models Demonstrated
    -Penalized Approaches
        -Lasso
    -Boosting
        -Gblinear demo
    -RL needs to be mentioned somewhere
    -Deep Learning with Neural Networks
       -Fancier curve fitting and more!
        -MLP
- Commentary



Autoencoders?

## Introduction

Machine learning is used everywhere, and allows us to do things that would have been impossible just a couple decades ago. It is used in everything from self-driving cars to medical diagnosis to predicting the next word in your text message. The ubiquity of it is such that it, and related terms like artificial intelligence, are used as buzzwords, and it is not always clear what it meant by the user. In this chapter we hope you'll come away with a better understanding of what machine learning is, and how it can be used in your own work. Because whatever it is, it sure is fun!

Machine learning is a branch of data analysis with a primary focus on predictive performance.  Honestly, that's pretty much it from a practical standpoint. It is not a subset of particular types of models, it does not preclude statistical models, it doesn't mean that a program spontaneously learns without human involvement, it doesn't necessarily have anything to do with 'machines' outside of laptop, and it doesn't even mean that the model is particularly complex. It is a set of tools and a modeling approach that attempts to maximize and generalize performance on new data, and compare models based on that performance[^generalize]. This is a different focus than statistical approaches that put much more emphasis on interpreting coefficients and uncertainty, but it is not an exclusive one. Some implementations of machine learning include models that have their basis in traditional statistics, while others are often sufficiently complex that they are scarcely interpretable at all, or in contexts where it simply isn't important. In this chapter, we will explore some of the more common models used machine learning and related techniques. 

[^generalize]: Generalization in statistical analysis is more about generalizing from our sample of data to the population from which it's drawn.

Machine learning came about in some ways as an outgrowth of statistics, and many papers in the past reinvented the wheel with some new metric that already had a long history in statistics.  Even after you conduct your modeling via machine learning, you may still fall back on statistical analysis of some parts of the results.  For example, you may want to know if the model is significantly better than a baseline model, or if the model is significantly better than another model. Or at least you should.   In any event, here we will also discuss some of the key ideas in machine learning, such as performance, loss functions, and cross-validation.

::: {.callout-note}
## ML by any other name
AI, statistical learning, data mining, predictive analytics, data science, BI
:::


This is a very different focus than the one we have taken so far in this book.  In this chapter, we will explore some of the most common machine learning models and techniques.  We will also discuss some of the key ideas in machine learning, such as performance, loss functions, and cross-validation.


Any model may be used in machine learning, from a standard linear model to a deep neural network.  The focus is on performance, not on inference.  This means that the modeler is less concerned with the interpretation of the model, but rather with the ability of the model to predict well on new data.  This is a very different focus than the one we have taken so far in this book.  In this chapter, we will explore some of the most common machine learning models and techniques.  We will also discuss some of the key ideas in machine learning, such as performance, loss functions, and cross-validation.

## Key ideas

- Machine learning is not a set of modeling techniques

## Common Models

### Penalized Approaches
#### Lasso
#### Ridge
#### Elastic Net

### Boosting
#### Gblinear demo

### Deep Learning and Neural Networks
#### MLP


## Other aspects of ML
### Unsupervised Learning
### Reinforcement Learning

## Commentary

When machine learning began to take off, it seemed most of the field of statistics sat on their laurels, and often scoffed at these techniques that didn't bother to test their assumptions[^riprip]! ML was, after all, mostly just a rehash of old ideas right? But the machine learning community was able to make great strides in predictive performance, and The application of machine learning continues to enable us to push the boundaries of what is possible. Statistics wasn't going to provide ChatGPT or self-driving cars, but it remains vitally important whenever we need to understand the uncertainty of our predictions, or when we need to make inferences about the world. The field of statistics has much to learn from the machine learning community, and vice versa. The two fields are not mutually exclusive, and the best data scientists will be able to draw from both. A more general field of **data science** became the way people used statistics *and* machine learning to solve their data challenges. In the end, use the best tool for the job, worry less about what it's called or whether it's the hot thing right now, and have fun!


[^riprip] To paraphrase provocatively, 'machine learning is statistics minus any checking of models and assumptions'. Brian D. Ripley useR! 2004, Vienna (May 2004)  Want to know what's even crazier than that statement? It was said by the guy that literally wrote the book on neural networks before anyone used them! Pattern Recognition and Neural Networks.