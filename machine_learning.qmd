
# Machine Learning

```{r}
#| include: False
#| label: setup-ml
source("load_packages.R")
source("setup.R")

library(tidyverse)
reticulate::use_condaenv("book-of-models")
```

Packages needed or useful for this chapter include:

**Python**

- sklearn
- 

**R**

- glmnet


Machine Learning 

- Intro 
    - ML as a modeling focus
- Key Ideas
    - Performance, Loss functions, Tuning/Cross-validation
- Models Demonstrated
    -Penalized Approaches
        -Lasso
    -Boosting
        -Gblinear demo
    -RL needs to be mentioned somewhere
    -Deep Learning with Neural Networks
       -Fancier curve fitting and more!
        -MLP

- Using R and Python in ML
- Commentary

Autoencoders?

## Introduction

**Machine learning** is used everywhere, and allows us to do things that would have been impossible just a couple decades ago. It is used in everything from self-driving cars, to medical diagnosis, to predicting the next word in your text message. The ubiquity of it is such that it, and related adventures like artificial intelligence, are used as buzzwords, and it is not always clear what it meant by the one speaking them. In this chapter we hope you'll come away with a better understanding of what machine learning is, and how it can be used in your own work. Because whatever it is, it sure is fun!

Machine learning is a branch of data analysis with a primary focus on predictive performance.  Honestly, that's pretty much it from a practical standpoint. It is not a subset of particular types of models, it does not preclude using statistical models, it doesn't mean that a program spontaneously learns without human involvement[^machinelearn], it doesn't necessarily have anything to do with 'machines' outside of laptop, and it doesn't even mean that the model is particularly complex. Machine learning, at its core, is a set of tools and a modeling approach that attempts to maximize and generalize performance on new data, and compare models based on that performance[^generalize]. This is a different focus than statistical approaches that put much more emphasis on interpreting coefficients and uncertainty, but it is not an exclusive one. Some implementations of machine learning include models that have their basis in traditional statistics, while others are often sufficiently complex that they are scarcely interpretable at all, or in contexts where it simply isn't important. In this chapter, we will explore some of the more common models used machine learning and related techniques. 

[^generalize]: Generalization in statistical analysis is more about generalizing from our sample of data to the population from which it's drawn. In order to do that well or precisely, one needs to meet certain assumptions about the model. In machine learning, generalization is more about how well the model will perform on new data, and is often referred to as 'out-of-sample' performance.

[^machinelearn]: Although this is implied by the name, it always felt like this description of ML was created by people who did not do applied machine learning. In fact, many of the most common models in machine learning are not capable of learning on their own, and require a human to specify the model and its parameters, set up the search through that parameter space, etc. We only very recently, post 2020, have developed the capability where something like a large language model can actually write a working program that would then be able to run on its own. The LLM itself was developed by humans though, so it's still not clear how this definition applies, or else it would apply to anything that uses optimization to 'learn' the best parameters for a model.

Machine learning came about in some ways as an outgrowth of statistics, and many papers in the past reinvented the wheel with some new metric that already had a long history in statistics.  Even after you conduct your modeling via machine learning, you may still fall back on statistical analysis of some parts of the results.  For example, you may want to know if the model is significantly better than a baseline model, or if the model is significantly better than another model. Or at least you should.   In any event, here we will also discuss some of the key ideas in machine learning, such as performance, loss functions, and cross-validation.

::: {.callout-note}
## ML by any other name
AI, statistical learning, data mining, predictive analytics, data science, BI
:::


This is a very different focus than the one we have taken so far in this book.  In this chapter, we will explore some of the most common machine learning models and techniques.  We will also discuss some of the key ideas in machine learning, such as performance, loss functions, and cross-validation.


Any model may be used in machine learning, from a standard linear model to a deep neural network.  The focus is on performance, not on inference.  This means that the modeler is less concerned with the interpretation of the model, but rather with the ability of the model to predict well on new data.  This is a very different focus than the one we have taken so far in this book.  In this chapter, we will explore some of the most common machine learning models and techniques.  We will also discuss some of the key ideas in machine learning, such as performance, loss functions, and cross-validation.

## Key ideas

- Machine learning is not a set of modeling techniques, but rather a focus on predictive performance, and a set of tools to achieve that.
- Models used in machine learning are typically more complex and difficult to interpret than those used in standard statistical analysis.
- There are many performane metrics used in machine learning, and care should be taken to choose the appropriate one for your situation.
- Objective functions likewise should be chosen for the situation, and are often different than the performance metric.
- Cross-validation is a method that allows us to select parameters and hyperparameters for our models, and to compare models to one another by assessing a model's performance on data that was not used to fit the model.
- Machine learning models are often used in situations where the primary goal is to predict well on new data, and not necessarily to understand the underlying process.
<!-- - For tabular data, you don't need many models in your toolbox to do very well
- For non-tabular data such as natural language and images, you need specific types of models and metrics appropriate to those -->


### Why this matters

Machine learning applications help define the modern world and how we interact with it.  There are few aspects of modern society that have not been touched by machine learning in some way. By understanding the basic ideas behind machine learning, you will be able to understand the models and techniques that are used in these applications, and be able to apply them to your own work. You'll also be able to understand the limitations of these models.



## General Approach

- Define the problem
- Define the performance metric(s)
- Select the model(s) to be used, including one baseline model
- Define the search space (parameters, hyperparameters) for those models
- Define the search method (optimization)
- Implement some sort of cross-validation technique
- Evaluate the results on unseen data

As an example using ridge regression:

- Define the problem: predict the price of a house
- Define the performance metric(s): RMSE
- Select the model(s) to be used: ridge regression, standard regression with no penalty as baseline
- Define the search space (parameters, hyperparameters) for those models: penalty parameter 
- Define the search method (optimization): grid search
- Implement some sort of cross-validation technique: 5-fold cross-validation
- Evaluate the results on unseen data: RMSE on test data

The key difference separating ML from other traditional statistical approaches is the assessment on unseen data, which we actually do twice. In the validation stage we usually will separate data into training and validation sets. We will go into details later, but the gist is that we select the model that performs best on the validation set, and then we evaluate that model on a test set that has not been used at all in the modeling process as our final assessment of performance. This is the only way to get an unbiased estimate of how well the model will perform on new data. We repeat this process for each model we are considering, and then select the model that performs best on the test set. 



```{r}
#| eval: false
#| 
# Option 1: tidytuesdayR package
## install.packages("tidytuesdayR")

tuesdata <- tidytuesdayR::tt_load("2023-08-15")
## OR
tuesdata <- tidytuesdayR::tt_load(2023, week = 33)

spam <- tuesdata$spam

# Option 2: Read directly from GitHub

spam <- readr::read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-08-15/spam.csv")

# tidytuesdayR::tt_datasets(2023)
```

## Data setup
    
```{r}
#| eval: false
#| label: setup-data

spam <- readr::read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-08-15/spam.csv")

spam <- spam %>% 
  mutate(is_spam = ifelse(type == "spam", 1, 0)) %>% 
  select(-type)
```


## Concepts


### Objective Functions

We've implmented a variety of objective functions in other chapters, such as mean squared error for numeric targets and log loss for binary targets.  As we have also noted elsewhere, the objective function is not necessarily the same as the performance metric we ultimately use to select a model. For example, we may use log loss as the objective function, but then use accuracy as the performance metric. In that setting, the log loss provides a 'smooth' objective function to search the parameter space over, while the accuracy is a straightforward and more interpretable metric for stakeholders. The objective function is used to optimize the model, while the performance metric is used to evaluate the model. In some cases, the objective function and performance metric are the same, and even if not, they might have selected the same 'best' model, but this is not always the case.

### Performance Metrics

There are many performance metrics used in machine learning, and care should be taken to choose the appropriate one for your situation. Typically we have a standard set we might use for the type of predictive problem. Here is a table of some commonly used ones:

TODO: add wiki link https://en.wikipedia.org/wiki/Confusion_matrix
```{r}
#| echo: false
#| label: tbl-performance-metrics
#| tbl-cap: Common Performance Metrics

library(gt)

performance_metrics <- tribble(
  ~Problem_Type, ~Metric, ~Description, ~`Other Names/Notes`,
  "Regression", "RMSE", "Root mean squared error", 'MSE (before square root)',
  "Regression", "MAE", "Mean absolute error", '',
  "Regression", "MAPE", "Mean absolute percentage error",  '',
  "Regression", "RMSLE", "Root mean squared log error", '',
  "Regression", "R-squared", "Amount of variance shared by predictions and observed target", 'Coefficient of determination',
  "Regression", "Deviance/AIC", "Generalization of sum of squared error for non-continuous/gaussian settings", 'Also "deviance explained" for similar R-sq interpretation',
  'Regression', 'Pinball Loss', 'Quantile loss function', 'MAE if estimating median',
  "Classification", "Accuracy", "Percent correct", 'Error rate is 1 - Accuracy',
  "Classification", "Precision", "Percent of positive predictions that are correct", 'Positive Predictive Value',
  "Classification", "Recall", "Percent of positive samples that are predicted correctly", 'Sensitivity, True Positive Rate',
  "Classification", "Specificity", "Percent of negative samples that are predicted correctly", 'True Negative Rate',
  'Classification', 'Negative Predictive Value', 'Percent of negative predictions that are correct', '',
  "Classification", "F1", "Harmonic mean of precision and recall", 'F-Beta',
  'Classification', 'Lift', 'Ratio of percent positive predictions to percent positive samples', '',
  "Classification", "AUC", "Area under the ROC curve", '',
  "Classification", 'Type I Error', 'False Positive Rate', 'alpha',
  "Classification", 'Type II Error', 'False Negative Rate', 'beta',
  "Classification", 'False Discovery Rate', 'Percent of positive predictions that are incorrect', '',
  "Classification", 'False Omission Rate', 'Percent of negative predictions that are incorrect', '',
  "Classification", 'False Positive Rate', 'Percent of negative samples that are predicted incorrectly', '',
  "Classification", 'False Negative Rate', 'Percent of positive samples that are predicted incorrectly', '',
  "Classification", 'Phi', 'Correlation between predicted and actual', "Matthews Correlation",
  "Classification", "Log loss", "Negative log likelihood of the predicted probabilities", ''
)



performance_metrics %>% 
  group_by(Problem_Type) %>%
  gt() %>% 
  tab_header(
    title = "",
    subtitle = "This is a table of some commonly used performance metrics in machine learning. "
  ) |> 
  tab_footnote(
    footnote = "Beta = 1 for F1",
    locations = cells_body(columns = vars(`Other Names/Notes`), rows = Metric == 'F1')
  )  |> 
  tab_footnote(
    footnote = "Not sure which fields refer to Matthews Correlation outside of CS, since 'phi' had already been widely used for over 60 years before Matthews forgot to cite it in their paper, and was literally the first correlation coefficient provided by Pearson.",
    locations = cells_body(columns = vars(`Other Names/Notes`), rows = Metric == 'Phi')
  )  |> 
  tab_options(
    footnotes.font.size = 8,
  )

```


#### Using Metrics for Model Evaluation and Selection

As we've seen, there are many performance metrics to choose from, and the choice of metric depends on the type of problem. For example, for a problem for numeric targtes, we might use RMSE, while for a classification problem, we might use accuracy.  However, it turns out that assessing the metric on the data we used to fit the model does not give us the best assessment of that metric.  This is because the model will always do better on the data it was trained on than on new data, and we can generally always improve that metric by making the model more complex.  However, in most modeling situations, this complexity comes at the expense of generalization, and the model will not perform as well on new data, something we'll discuss in more detail shortly. So we really want to assess the performance metric on new data, and not the data we used to fit the model. At that point, we can also compare multiple models to one another, and select the one that performs best on the new data.

It's rather easy to get started down this path. For starters, we simply split our data into two sets, a **training set** and a **test set**, which the latter typically a smaller subset.  We fit the model on the training set, and then evaluate the performance metric on the test set. This is known as the **holdout method**.  There limitations to doing it this simply, but conceptually this is an important idea.

DEMO METRIC with holdout method??


### Regularization

A key aspect of the machine learning approach is to predict well on new data. This is known as generalization. One way to improve generalization is through the use of **regularization**, which is a general appraoch to penalize complexity in a model, and is typically used to prevent **overfitting**.  Overfitting occurs when a model fits the training data very well, but does not generalize well to new data, and this is often due to the model being too complex, and thus fitting to noise in the training data that isn't present in other data. Note that the converse can also happen, and is often the case with simpler models, where the model does not fit the training data well, and thus does not generalize well to new data either, and this is known as **underfitting**[^underfit].

[^underfit]: Underfitting is a notable problem in many academic disciplines, where the models are often too simple to capture the complexity of the underlying process. Typically the models are often linear, and the underlying process may be anything but. These disciplines were slow to adopt machine learning techniques, as they are often more difficult to interpret, and so seen as not as useful for understanding the underlying process. However, one could make the obvious argument that 'understanding' an unrealistic result is not very useful either, and that the goal should be to understand the underlying process however we can, and not just the model. 

We have already seen one example of regularization in the ridge regression model, where we add a penalty term to the objective function. This penalty term is a function of the coefficients, and is usually a function of the sum of the squares of the coefficients.  This is known as an L2 penalty, and is the most common type of regularization.  Another common type of regularization is the L1 penalty, which is a function of the sum of the absolute values of the coefficients.  This is used in the lasso model.  There are other types of regularization as well, such as the elastic net, which is a combination of the L1 and L2 penalties.  The relative size of the two penalties is controlled by a mixing parameter, and the optimal value of that parameter is determined by cross-validation.

It turns out that regularization comes in many forms. Here is a quick rundown of some examples. 

- GAMs also use penalized regression for estimation, where the coefficients used in the basis functions are penalized (typically with L2).  This tends the 'wiggly' part of the GAM to be less wiggly, tending toward a linear effect. 

- Similarly, the variance estimate of a random effect, e.g. for intercept or slope, is inversely related to penalty on the fixed effects for the corresponding group-specific effects.  The more penalty, the less variance, and the more the random effect is shrunk toward the overall mean[^mixedpenalty]. 
[^mixedpenalty]: One more reason to prefer a random effects approach over so-called fixed effects models, as the latter are not penalized at all, and thus are more prone to overfitting.

- Still another form of regularization occurs in the form of priors in Bayesian models. For example, the variance on the prior for regression coefficients could be very large, which amounts to a result where there is little influence of the prior on the posterior, or it could be very small, which amounts to a result where the prior has a lot of influence on the posterior, shrinking it toward the prior mean, which is typically zero. In fact, ridge regression is a frequentist form of standard Bayesian linear regression.

- As a final example of regularization, **dropout** is a technique used in neural networks to prevent overfitting. It works by randomly dropping out some of the nodes in intervening/hidden layers in the network during training. This tends to force the network to learn more robust features, and prevents it from overfitting to the training data.

In short, regularization comes in many forms across the modeling landscape, and is a key aspect of machine learning and traditional statistical modeling alike. In general, we can add the same sort of penalty to any number of models, such as logistic regression, neural networks, recommender systems etc. The primary goal again is to hopefully increase our ability to generalize the selected model to new data. 


### Tuning & Cross-validation


#### Bias-Variance Tradeoff

<!-- In most of our modeling endeavors, we are concerned with reducing uncertainty in our knowledge of some phenomenon. The more we know about the factors involved or related to some outcome of interest, the better we can predict that outcome upon the influx of new information. The initial step is to take the data at hand, and determine how well a model or set of models fit the data. In many applications however, this part is also more or less the end of the game as well[^bigdata].

[^bigdata]: Note that we don't make any particular claim about the quality of such analysis. In many situations, the cost of data collection is very high, and for all the enamorment with 'big' data, a lot of folks will never have access to big data for their situation (e.g. certain clinical populations) In these situations, getting new data or splitting data to make predictions on holdout sets is difficult.

While often not reported, in many scientific, business, and other situations, many models are actually tested behind the scenes, among which the 'best' is then provided in detail in the end report. Unfortunately, fitting models to a single dataset set does not give a very good sense of the generalization any of those models, i.e. the performance we would see with new data. Without some sort of check on a model's generalization, such performance will be optimistic when it comes to new data. -->

In the following discussion, you can think of a standard linear model scenario, e.g. with squared-error loss function and perhaps some regularization, and a data set where we split some of the observations in a random fashion into a **training set**, for initial model fitting, and a **test set**, which will be kept separate and independent, and used to measure generalization performance. We note **training error** as the (average) loss over the training set, and **test error** as the (average) prediction error obtained when a model resulting from the training data is fit to the test data.  So, in addition to the previously noted goal of finding the 'best' model (**model selection**), we are interested further in estimating the prediction error with new data (**model performance**).

##### Bias & Variance

We've found that most discussions of **bias** and **variance** and their 'tradeoff' can be difficult to digest, and misleading at worst. Part of it is due to terminology (like using a word like *bias* that already has been used to mean different things) and talking in terms of 'averages' when typically we only have one score in practice. Also in play is that the discussion doesn't actually hit on all sources of error, and that error occurs in both the target and the features before we ever even have a model.

Let's start with the classic weight scale example. We take a measure of our dog Chewie, who weighs 30 pounds. The scale says 28 pounds 1 day, 31 the next and so on, but, if we take the average, it's 30 pounds, which is what Chewie weighs. It bounces around because, some days Chewie might eat a little more, or go to the dog park and run around, which makes his weight fluctuate a bit. This is known as **random error**.  Now we have a scale that's off, and after several tries we get an average weight for Chewie of 28 pounds. This scale is *biased*, and contains what is more specifically referred to as **measurement error**. Note that we haven't even talked about a model yet. The weight could be our target or a feature in a model, we aren't even concerned with parameter estimates yet, and we already have potentially two sources of 'error'.

Now consider a modeling situation where we have the usual situation of splitting data into training and test sets.  We run the model on the training set, but we are more interested in generalization error, or how well it predicts on the test set.  We can think of the test error as the average error over many such splits of the data into training and test sets.  This is the **generalization error**. Given this scenario, let's look at the following visualization inspired by @hastie_elements_2009.

![](img/biasvar2.svg)


Prediction error on the test set is a function of several components, and the terms bias and variance generally refer to two of those components. One thing to note is that even if we had the 'true' model given the features specified correctly, there would still be prediction error.

As the model complexity increases, the bias, which is the difference in our average prediction and the true model prediction, decreases, but this only continues for training error, where eventually our model fits the data perfectly. For test error, as the model complexity increases, the bias decreases, but the variance eventually increases because we get too close to the training data and do poorly when we try to generalize beyond it. In a nutshell this is the tradeoff - we can reduce one source of error at the expense of the other, but not both at the same time.  In other words, we can reduce bias by increasing model complexity, but this will increase variance. We can reduce variance by reducing model complexity, but this will increase bias. The goal is to find the sweet spot where we have a model that is complex enough to capture the underlying process, but not so complex that it overfits to the training data. 

```{r}
#| echo: false
#| eval: false
#| label: bias-variance-tradeoff
# create a function with input N that fits increasingly complex models in an attempt to estimate some model. X is a set of five features, and y the target, both of size N. y is a nonlinear function of three of those features. The error in train and test sets should be estimated, and the ultimate result should be a plot of the test error and training error on the y axis with the x axis the model complexity or degrees of freedom.
set.seed(123)

# Define the function
fit_models <- function(N = 500, max_degree = 10, n_boot = 10) {
    library(mgcv)

    # Generate data
    d = gamSim(n = N)
    X = d |> select(x0:x3)
    y = d$y

    degs = c(3:12, 25, 50)

    # Define function to fit models of increasing complexity
    fit_model <- function(X_train, y_train, X_test, y_test, degree) {
      # Fit model
      model <- gam(
        y_train ~
            s(x0, k = degree)
          + s(x1, k = degree)
          + s(x2, k = degree)
          + s(x3, k = degree),
        data = X_train
      )

      # Calculate train error
      y_train_pred <- fitted(model)
      train_error <- mean((y_train - y_train_pred)^2)

      # Calculate test error
      y_test_pred <- predict(model, newdata = X_test)
      test_error <- mean((y_test - y_test_pred)^2)

      return(list(train_error = train_error, test_error = test_error))
    }
    
    # Fit models of increasing complexity n_boot times
    train_error <- matrix(0, nrow = n_boot, ncol = length(degs))
    test_error <- matrix(0, nrow = n_boot, ncol = length(degs))

    for (boot in 1:n_boot) {
      # Resample data
      train_idx <- sample(N, N / 2, replace = TRUE)
      test_idx <- setdiff(1:N, train_idx)
      X_train <- X[train_idx, ]
      y_train <- y[train_idx]
      X_test <- X[test_idx, ]
      y_test <- y[test_idx]
      
      for (degree in seq_along(degs)) {
        # Fit model and calculate errors
        errors <- fit_model(X_train, y_train, X_test, y_test, degs[degree])

        # Store errors
        train_error[boot, degree] <- errors[["train_error"]]
        test_error[boot, degree] <- errors[['test_error']]
      }
    }

    # Reshape errors into long format
    df <- data.frame(
      degree = rep(degs, each = n_boot),
      boot = rep(1:n_boot, times = length(deg)),
      train_error = as.vector(train_error),
      test_error = as.vector(test_error)
    )

    # Plot train and test error vs model complexity
    df <- df %>%
      pivot_longer(
        cols = c(train_error, test_error), 
        names_to = "error_type", 
        values_to = "error"
      ) %>%
      mutate(error_type = factor(error_type))

    df |> 
    ggplot(aes(x = degree, y = error)) +
      geom_line(aes(group = interaction(boot, error_type), color = error_type),      alpha = .2
    ) +
      stat_summary(
        geom = 'line', 
        fun = mean, 
        aes(color = error_type),
        linewidth = 1
      ) +
      scale_x_continuous(breaks = 1:max_degree) +
      labs(x = "Model complexity (degrees of freedom)", y = "Error") +
      theme(
        axis.text = element_blank(),
        axis.ticks = element_blank()
      )
}

fit_models(400, max_degree = 20, n_boot = 100)

```

Higher bias can occur when we have a model that is too simple to capture the complexity of the underlying process. Our model predictions are simply not that good. Higher variance occurs when we have a model that is too complex, and thus fits the noise in the training data, and does not generalize well to new data. The bias-variance tradeoff is the idea that we can reduce one source of error at the expense of the other, but not both at the same time.  In other words, we can reduce bias by increasing model complexity, but this will increase variance. We can reduce variance by reducing model complexity, but this will increase bias. The goal is to find the sweet spot where we have a model that is complex enough to capture the underlying process, but not so complex that it overfits to the training data.  This is the model that will generalize best to new data.

Consider the following image, inspired by @hastie_elements_2009.  Let's say we repeatedly estimate a model, get an estimate of error on the training set, ao that we can get an average error over many such estimates.  We can do the same for the test set.  The average error on the training set is the **training error**, and the average error on the test set is the **test error**.  The difference between the two we'll call the **generalization error**, and is the error we would expect to see with new data.  The training error is the error we would expect to see with the same data, but a different model. The test error is the error we would expect to see with new data, but the same model. The generalization error is the sum of the bias and variance, and is the error we are most interested in.  The bias-variance tradeoff is the idea that we can reduce one source of error at the expense of the other, but not both at the same time.  In other words, we can reduce bias by increasing model complexity, but this will increase variance. We can reduce variance by reducing model complexity, but this will increase bias. The goal is to find the sweet spot where we have a model that is complex enough to capture the underlying process, but not so complex that it overfits to the training data.  This is the model that will generalize best to new data.


![](img/biasvar2.svg)
![](img/biasvartarget.svg)


[^csbias]: Folks in CS and engineering should not be allowed to use the word bias. 

Keep in mind the following:

- Prediction from a model ($M$)
- Prediction error, the difference from our model's prediction and the observed value, i.e. what this is all about. More specifically we can think of the average. 
- A dataset $X$ which can be randomly produced, and so produce different estimates of $M$
- A target $y$ which is also random, and which is observed
- True value of the target $y$ (unknown)

$$y = f(X) + \epsilon$$

$$\textrm{Prediction Error} = \textrm{Other Error} + \textrm{Bias} + \textrm{Variance}$$

- Other error: $\epsilon$ (random)
- Bias: $E(\hat f) - f$ (observed)
- Variance: $Var(\hat f)$ (random)

> Measurement error is a concept almost entirely ignored in computer and data science/engineering, but is the possibly the largest source of model inadequacy. It not only occurs in the target, but also the features, the latter of which is never shown in the bias-variance tradeoff equation. It's often labeled as 'irreducible', but in fact, a whole endeavor of mathematical, statistical, and psychological efforts have been devoted to providing ways to do so almost 100 years[^mindvector]. It's not easy, but it's not impossible either in many situations.  We will not discuss it further here, but it is important to keep in mind that it is a source of error that is not addressed by a model by default. Even if we had a perfect model, our target would still vary in its measurement, and thus we would still have error in our prediction.  To make the example concrete, consider predicting a person's income. You can have a perfect model, but some people will lie about it.

[^mindvector]: https://en.wikipedia.org/wiki/The_Vectors_of_Mind https://en.wikipedia.org/wiki/Psychometrics

We start[^biasvarref] with a true data generating process for some target $y$, expressed as a function of features $X$. We can specify the true model as 

$$y = f(X) + \epsilon$$

where $f(x)$ is the expected value of $y$ given $X$, i.e. $f(x) = E(y|X)$. The expected value of the error, $E(\epsilon)=0$, has some variance, $\textrm{Var}(\epsilon) = \sigma^2_\epsilon$.  In other words, we are talking about the standard regression model we all know and love.  Now we can conceptually think of the *expected prediction error* at a specific input $X = x_*$ as: 


$$\text{Error}_{x_*} = \text{Irreducible Error} + \text{Bias}^2 + \text{Variance}$$


To better understand this, think of training models over and over, each time with new training data, but testing each model at input $x_*$. The $\text{Error}_{x_*}$ is the average, or expected value of the prediction error in this scenario, or $E[(y - \hat f(x))^2|X=x_*]$, with $\hat f$ our current estimate of the true underlying data generating function $f$. We can note three components to this general notion of prediction error: 

**Irreducible error**: The variance of the (new test) target ($\sigma^2_\epsilon$). This is unavoidable, since our $y$ is measured with error. Think of linear regression where the true model is y = 1 + 2x + e. Even if we estimate the coefficients exactly, we will still have error in our prediction due to the error in the target.  This is the irreducible error.  It's sometimes called **measurement error**, but measurement error 

**Bias^2^**: the amount the *average* of our estimate varies from the true (but unknown) value ($E(\hat f) - f$). This is often the result of trying to model the complexity of nature with something much simpler that the human brain can understand. While the simpler might make us feel good, it may not work very well.

**Variance**: the amount by which our prediction would change if we had estimated it using a different training data set ($Var(\hat f)$). Even with unbiased estimates, we could still see a high mean squared error due to high variance.

Slightly more formally, we can present this as follows, with $h_*$ our estimated (hypothesized) value at $x_*$:


$$\text{Error}_{x_*} = \text{Var}(\epsilon) + (\text{E}[h_*] - f(x_*))^2 + \text{Var}(h_*)$$

The latter two components make up the mean squared error in our previous demonstration.  While they are under our control, they compete with one another such that oftentimes we improve one at the detriment of the other. In other words, *bias and variance are not independent*.



##### The Tradeoff

Outlining a general procedure, we start by noting the prediction error on a training data set with multiple models of varying complexity (e.g. increasing the number of predictor variables, adding polynomial terms, including interactions), and then assess the performance of the chosen models in terms of prediction error on the test set.  We then perform the same activity for a total of 100 simulated data sets, for each level of complexity.

The results from this process might look like the following, taken from @hastie_elements_2009.  With regard to the training data, we have $\mathrm{error}_{\mathrm{train}}$ for one hundred training sets for each level of model complexity.  The bold blue line notes this average error over the 100 sets by model complexity, and we can see that more complex models fit the data better.  The bold red line is the average test error ($\mathrm{error}_{\mathrm{test}}$) across the 100 test data sets, and it tells a different story. When models get too complex, the test error starts to *increase*.

<img src="img/biasvar2.svg" style="display:block; margin: 0 auto;" width=50%>

Ideally we'd like to see low bias and (relatively) low  variance, but things are not so easy. One thing we can see clearly is that $\mathrm{error}_{\mathrm{train}}$ is not a good estimate of $\mathrm{error}_{\mathrm{test}}$, which is now our focus in terms of performance. If we think of the training error as what we would see in typical research where one does everything with a single data set, we are using the same data set to fit the model and assess error.  As the model is adapted to that data set specifically, it will be overly optimistic in the estimate of the error, that optimism being the difference between the error rate we see based on the training data versus the average of what we would get with many test data sets.  We can think of this as a problem of overfitting to the training data.  Models that do not incorporate any regularization or validation process of any kind are likely overfit to the data presented.

Generally speaking, the more complex the model, the lower the bias, but the higher the variance, as depicted in the graphic.  Specifically however, the situation is more nuanced, where the type of problem (classification with 0-1 loss vs. continuous response with squared error loss[^biasvardimen]) and technique (a standard linear model vs. regularized fit) will exhibit different bias-variance relationships.




#### Diagnosing Bias-Variance Issues *&* Possible Solutions

The following can serve as a visual summary of the concepts just outlined (figure adapted from @domingos_few_2012).

<img src="img/biasvartarget.svg" style="display:block; margin: 0 auto;" width=50%> <br> 

Now let's assume a regularized linear model with a standard data split into training and test sets.  We will describe different scenarios with possible solutions.


##### Worst Case Scenario

Starting with the worst case scenario, poor models may exhibit high bias and high variance.  One thing that will not help this situation (perhaps contrary to intuition) is adding more data.  You can't make a silk purse out of a sow's ear ([*usually*](https://libraries.mit.edu/archives/exhibits/purse/)), and adding more data just gives you a more accurate picture of how awful your model is. One might need to rework the model, e.g. adding new predictors or creating them via interaction terms, polynomials, or other smooth functions as in additive models, or simply collecting better and/or more relevant data.


##### High Variance

When variance is a problem, our training error is low while test error is relatively high (overfitting problem). Implementing more shrinkage or other penalization to model complexity may help with the issue.  In this case more data may help as well.

##### High Bias

With bias issues, our training error is high and test error is not too different from training error (underfitting problem).  Adding new predictors/features, e.g. interaction terms, polynomials etc., can help here.  Additionally, reducing the penalty parameter $\lambda$ would also work with even less effort, though generally it should be estimated rather than explicitly set.


Here is another visualization to drive the point home.

<img class='imgbigger' src="img/biasvar_gp.svg" style="display:block; margin: 0 auto;" width=50%> 

<br> 

The figure is inspired by @murphy_machine_2012 (figure 6.5) showing the bias-variance trade-off.  Sample (left) and average (right) fits of linear regression using a Gaussian radial basis function expansion. The blue line represents the true x-y relationship. The top row shows low variance between one fit and the next (left) but notable bias (right) in that the average fit is off.  Compare to the less regularized (high variance, low bias) situation of the bottom row.  See the <span class="pack">kernlab</span> package for the fitting function used, and the [appendix][Appendix] for the code used to produce the graph.



#### Bias-Variance Summary

One of the key ideas any applied researcher can take from machine learning concerns the bias-variance trade-off and issues of overfitting in particular.  Typical applied practice involves potentially dozens of models fit to the same data set without any validation whatsoever, yet only one or two are actually presented in publication.  Many disciplines report nothing but the statistical significance, and yet one can have statistically significant predictors and have predictive capability that is no different from guessing.  Furthermore, very complex models are often fit to small data sets, compounding the problem.

It is very easy to describe ***science*** without ever talking about statistical significance.  It is impossible to talk about science without talking about prediction.  The bias-variance trade-off is one way to bring the concerns of prediction to the forefront, and any applied researcher can benefit from thinking about its implications[^bvclass]. 



#### Cross-validation




#### Pipelines

## Common Models

### Baseline

The baseline model should serve as a way to gauge how much better your model performs over one that is simpler and more interpretable, or one that is sufficiently complex to capture something about the data you are exploring, but not as complex as the models you're primarily interested in. Take a classification model for example. We could compare it to a logistic regression, which is as simple as it gets, but is often too simple to be very predictive for many situations. 

#### Why do we do this?

You can actually find articles in which deep learning models do not even beat a logistic regression on some datasets, but which did not stop the authors writing several pages hyping the more complex technique. Probably the most important reason to have a baseline is so that you can avoid wasting time and resources implementing more complex tools. It is probably rare, but sometimes relationships for the chosen features are mostly or nearly linear and have little interaction, and no amount of fancy modeling will make it come about.  If our baseline is a complex linear model that actually incorporates nonlinear relationships and interactions (e.g. a GAMM), you'll often find that the more complex models don't improve on the baseline by much, if at all. Furthermore, you may find that the initial baseline model is good enough for the time being and you can then move on to other problems to solve.  This is especially true if you are working in a business setting where you have limited time and resources.

A final note. In many (most?) settings, it often isn't enough to merely beat the baseline model. You should look for doing statistically better. For example, if your complex model accuracy is 75% and your baseline is 73%, that's great, but you should check to see if that difference is statistically significant[^ifonlystatdiff], because those metrics are *estimates*, and they have uncertainty, which means you can get a range for them as well as test whether they are different from one another. If it is not, then you should probably stick with the baseline model or try something else, because the next time you run the model, the baseline may actually perform better, or at least you can't be sure that it won't.

That said, in some situations any performance increase is worth it, and even if we can't be certain a result is statistically better, any sign of improvement is worth pursuing. For example, if you are trying to predict the next word in a sentence, and your baseline is 10% accurate, and your complex model is 11% accurate, that's a 10% increase in accuracy, which may be a big deal depending on the situation. You should still work to show that this is a consistent increase and not a fluke.

[^ifonlystatdiff]: There would be far less hype and wasted time if those in ML and DL research simply did this rather than just reporting the chosen metric of their model 'winning' against other models. It's not that hard to do, yet most do not provide any ranged estimate for their metric, let alone test statistical difference from other models. You don't even have to bootstrap the metric estimates for binary classification! It'd also be nice if they used a more meaningful baseline than logistic regression, but that's a different story.


### Penalized Approaches

We show explictly how to estimate models like lasso and ridge regression in the estimation chapter. Those work well as a baseline and so should be in your ML toolbox. Remember also that they can be applied to any linear model setting, where your objective is MSE, log likelihood of a count or binary target, or what have you. It's good to keep them in mind.

#### Elastic Net

Another common approach is **elastic net**, which is a combination of lasso and ridge.  We will not show how to estimate elastic net here, but all you have to know is that it has two penalties, the same ones for lasso and one for ridge, along with the standard objective for a numeric or categorical target. The relative size of the two penalties is controlled by a mixing parameter, and the optimal value of that parameter is determined by cross-validation.  So for example, you might end up with a 75% lasso penalty and 25% ridge penalty.  

Let's apply this to the movie review data. We'll used the 'processed version' which has some standardized versions of the data and a binary version of the target review as 'Good' or 'Bad'.

:::{.panel-tabset}

##### Python

```{python}
#| label: elasticnet-py

import pandas as pd
import numpy as np
from sklearn.linear_model import ElasticNetCV, LogisticRegressionCV
from sklearn.metrics import accuracy_score

df_movies = pd.read_csv('data/movie_reviews_processed.csv')

X = df_movies.filter(regex='_sc$')
y = df_movies['rating_good']

# Fit model

model = LogisticRegressionCV(
  penalty   = 'elasticnet', 
  l1_ratios = np.arange(.0, 1, .05), 
  cv     = 5, 
  solver = 'saga', 
  random_state = 123,
  verbose = False
)

model.fit(X, y)

# Predict

y_pred = model.predict(X)

# Evaluate

print(
  'Training accuracy: ', accuracy_score(y, y_pred),
  'Baseline Prevalence: ', np.mean(y)
)
```

##### R

```{r}
#| label: elasticnet-r
library(glmnet)

df_movies <- read_csv('data/movie_reviews_processed.csv')

X <- df_movies %>%
  select(ends_with('_sc')) %>%
  as.matrix()

y <- df_movies %>%
  pull(rating_good)


# Fit model

model <- cv.glmnet(
  x = X,
  y = y,
  alpha = 0.5,
  family = 'binomial',
  nfolds = 5,
  type.measure = 'class',
  gamma = seq(0, 1, 0.05)
)

# Predict

y_pred <- predict(model, newx = X, type = 'class')

# Evaluate

glue('Training Accuracy: {round(mean(y_pred == y), 3)}\nBaseline Prevalence: {round(mean(y), 3)}')
```

:::





#### Strengths & Weaknesses

##### Strengths

- Intuitive approach.  In the end, it's still just a standard regression model you're already familiar with.
- Widely used for many problems.  Would be fine to use in any setting you would use standard/logistic regression.

##### Weaknesses

- Does not automatically seek out interactions and non-linearity, and as such will generally not be as predictive as other techniques.
- Variables have to be scaled or results will largely reflect data types.
- May have issues with correlated predictors


#### Additional Thoughts

Incorporating regularization would be fine as your default method, and something to strongly consider.  Furthermore, these approaches will have better prediction on new data than their standard complements.  As such they are a nice balance between staying interpretable while enhancing predictive capability. However, in general they are not going to be as strong of a method as others in the ML universe, and possibly not even competitive without a lot of feature engineering.  If prediction is all you care about for a particular modeling setting, you'll want to try something else.





### Tree-based methods

Let's move beyond a standard linear models and get into a notably different approach. Tree-based methods are a class of models that are very popular in machine learning. Consider the following classification example where we want to predict a binary target as yes or no. We have two numeric features, $X_1$ and $X_2$. At the start we take $X_1$ and make a split at the value of 5. Any observation less than 5 on $X_1$ goes to the right with a prediction of *No*. Any observation greater than or equal to 5 goes to the left, where we then split based on values of $X_2$, and specifically at 3. Any observation less than 3 goes to the right with a prediction of *Yes*. Any observation greater than or equal to 3 (and greater than or equal to 5 on $X_1$) goes to the left with a prediction of *No*. 


```{dot}
#| echo: false
#| label: tree-graph

digraph tree {
graph [rankdir = TD  bgcolor="#fffff8"]

node [shape = rectangle, style=filled, fillcolor=white, color=gray, width=.75]

node [fontcolor=gray25 fontname=Roboto fixedsize=true fontsize=5]
X1[width=.25 height=.25 label = <X<sub>1</sub> >];
X2 [width=.25 height=.25 label = <X<sub>2</sub> >]; 
No1 [label="No" shape=circle color="#E69F00" width=.25]; 
No2 [label="No" shape=circle color="#E69F00" width=.33]; 
Yes [ shape=circle color="#56B4E9" width=.33];

edge [color=gray50 arrowhead=dot]
X1 -> No1 [label = " < 5", fontcolor="gray50" fontsize=5.5 color="#E69F00"];
X1 -> X2 [label = " >= 5", fontcolor="gray50" fontsize=5.5];
X2 -> No2 [label = " >= 3", fontcolor="gray50" fontsize=5.5 color="#E69F00"];
X2 -> Yes [label = " < 3", fontcolor="gray50" fontsize=5.5 color="#56B4E9"];

}
```


This is a simple example, but it illustrates the basic idea of a tree-based model, where the **tree** reflects the total process, and **branches** the splits going down, ultimately ending at **leaves**. We can think of the tree as a series of if-then statements, where we start at the top and work our way down until we reach a leaf node, which is a prediction for all observations that qualify for that leaf.

If we just had a single tree, this would be the most interpretable model we could probably come up with, and it incorporates nonlinearities (multiple branches on a single feature), interactions (branches across features), and feature selection all in one.  However, it's unfortunately not a very stable model, and does not generalize well. For example, just a slight change in data, or even just starting with a different feature, might produce a very different tree[^cartbase]. The solution is merely to come up with a bunch of trees, get predictions for each observation from each tree, and then average the predictions. This is the concept behind both **random forests** and **gradient boosting**, which can be seen as different algorithms to produce a bunch of trees and then average the predictions.  The also fall under the heading of **ensemble models**, which are models that combine the predictions of multiple models, in this case individual trees, to produce a single prediction. 

Random forests and boosting methods are very easy to implement, to a point. However, there are typically a few hyperparameters to consider for tuning. Here are few to think about:

- Number of trees
- Learning rate (GB)
- Maximum **depth** of each tree
- Minimum number of observations in each leaf
- Number of features to consider at each tree/split
- Regularization parameters (GB)
- Out-of-bag sample size (RF)


Those are the ones that you'll usually be trying to figure out via cross-validation for boosting, but there are others. The number of trees and learning rate kind of play off of each other, where more trees allows for a smaller rate, which might work better but will usually take longer to train, but can lead to overfitting if other steps are not taken. The depth of each tree refers to the number of levels down the branches we allow the model to go (as well as how wide we let things get in some implementations). This is important because it controls the complexity of each tree, and thus the complexity of the overall model- less depth helps to avoid overfitting, but too little depth and you won't be able to capture the nuances of the data. The minimum number of observations in each leaf is also important for the same reason. It's generally a good idea to take a random sample of features (or even data) to also help reduce overfitting. The regularization parameters are typically less important, but in general you'll want to use them to reduce overfitting.


[^cartbase]: This actually could serve as a decent baseline model, especially given the interepretability.


<!-- <img src="img/tree1.png" style="display:block; margin: 0 auto;" width=25%> -->

Example with the moview review data. Although boosting methods are available in sklearn for Python, in general we recommend lightgbm or xgboost packages directly for boosting implementation, which have a sklearn API anyway (as demonstrated). Also, they both provide R and Python implementations of the package, making it easy to not lose your place when switching between languages.  We'll use xgboost here, but lightgbm is also a very good option.  Some also prefer catboost[^nomeow], but we'll leave that for the reader to investigate.  We'll also use the processed version of the data, which has some standardized versions of the data and a binary version of the target review as 'Good' or 'Bad'.

[^nomeow]: The authors have not actually been able to practically implement catboost in a setting where it was more predictive or as efficient/speedy as xgboost or lightgbm, but that's not to say it's not a good option, and some have had notable success with it. It's just not one we've used much, or is as popular in general.

:::{.panel-tabset}

##### Python

```{python}
#| label: boost-py

import pandas as pd
import numpy as np
from sklearn.ensemble import HistGradientBoostingClassifier
import xgboost as xgb
from sklearn.metrics import accuracy_score

df_movies = pd.read_csv('data/movie_reviews_processed.csv')

X = df_movies.filter(regex='_sc$|genre|release_year_0|season|children_in_home')

X = X.assign(
  genre  = pd.Categorical(X['genre']),
  season = pd.Categorical(X['season'])
)

y = df_movies['rating_good']

# Fit model

model = xgb.XGBClassifier(
  max_iter = 250,
  learning_rate = .1,
  max_depth = 5,
  min_samples_leaf = 10,
  objective = 'binary:logistic',
  tree_method = 'hist',
  enable_categorical = True,
  random_state = 123
)

model.fit(X, y)


# Predict

y_pred = model.predict(X)

# Evaluate

print(
  'Training accuracy: ', accuracy_score(y, y_pred),
  'Baseline Prevalence: ', np.mean(y)
)
```

##### R

Note that as of writing, the xgboost package does not support categorical variables in R, so we'll just one-hot encode them.



```{r}
#| label: boost-r

library(xgboost)

# df_movies <- read_csv('data/movie_reviews_processed.csv')

X <- df_movies %>%
  select(matches("_sc$|genre|release_year_0|season|children_in_home")) |> 
  mutate(
    genre = as.factor(genre),
    season = as.factor(season)
  ) |>
  data.table::as.data.table() |> 
  mltools::one_hot() |> 
  as.matrix()


y <- df_movies %>%
  pull(rating_good)


# Fit model
set.seed(123)

model <- xgboost(
  data = X,
  label = y,
  nrounds = 250,
  params = list(
    eta = .1,
    max_depth = 5,
    min_child_weight = 10
  ),
  objective = 'binary:logistic',
  verbose = 0
)

# Predict

y_pred <- as.integer(predict(model, X, type = 'class')>.5)

# Evaluate

glue('Training Accuracy: {round(mean(y_pred == y), 3)}\nBaseline Prevalence: {round(mean(y), 3)}')
```




#### Gblinear demo

MOVE TO APPENDIX




Random forests and boosting methods, though not new, are still 'state of the art' in terms of performance on tabular data like the type we've been using for our demos here. 

### Strengths & Weaknesses

**Strengths**

- A single tree is highly interpretable.
- Easily incorporates features of different types (scale of features, categoricals, doesn't matter).
- Tolerance to irrelevant features.
- Some tolerance to correlated inputs.
- Handling of missing values. Missing values are just another value to potentially split on.

**Weaknesses**

- Honestly few, but like all techniques, it might be relatively less predictive in certain situations. 
- It does take more effort to tune relative to linear model methods.


### Deep Learning and Neural Networks
#### MLP


### Others

When you look up models used in machine learning, you'll potentially see a lot of different kinds.  Popular methods from the past include *k*-nearest neighbors, support vector machines, and more. You don't see these used in practice much though, as these have mostly been made obsolete due to not being as predictive as other options in general (k-nn), or maybe only working well with 'pretty' data situations (SVM), or just being less interpretable. While they might work well in certain situations, when you have tools that can handle a lot of data complexity and predict very well (and typically better) like tree-based methods, there's not much reason to use the historical alternatives these days. If you're interested in learning more about them or think one of them is just 'neat'[^svm], you could potentially use it as a baseline model.

[^svm]: Mathy folk should love SVMs.

There are also many other methods that are more specialized, such as those for text, image, and audio data.  We will not discuss those here, but they are worth looking into if you have a need for them.


## Other aspects of ML
### Unsupervised Learning

PCA DEMO


Traditionally **unsupervised learning** falls under dimension reduction, such that we reduce features to a smaller **latent**, or hidden, or unobserved, subset that accounts for most of the (co-)variance of the larger set, or reduce the rows to a small number of hidden, or unobserved, clusters. For example, we start with 100 features and reduce them to 10, or we classify each observation as belong to 2-3 clusters. Either way, the goal is to reduce the dimensionality of the data, not predict an explicit target.

More info about latent variables and latent linear models...

It turns out that these are the same thing, though apparently no one knows this outside of statisticians. You can discretize anything, e.g. from a nicely continuous feature to a coarse couple of categories, and this goes for latent variables as well as those we actually see in our data. For example, if I do a factor analysis with one latent feature, I can either convert it to a probability of some class with an appropriate transformation, or just say that scores higher than some cutoff are in cluster A and the others are in cluster B, and indeed, there is a whole class of clustering models called **mixture models** that do just that (i.e. estimate the latent probability of class membership). The only real difference between the two approaches is the objective (function), and for any k latent features I come up with I can create (at least) k + 1 clusters before taking into account interactions of the latent variables. 

https://stats.stackexchange.com/questions/122213/latent-class-analysis-vs-cluster-analysis-differences-in-inferences

::: {.callout-tip}
In general, do not use a dimension reduction technique as a preprocessing step for a supervised learning problem.  Instead, use a supervised learning technique that can handle high-dimensional data, has a built-in way to reduce features (e.g. lasso, boosting), or use a dimension reduction technique that is specifically designed for supervised learning (e.g. partial least squares). Creating a reduced set of features without regard to the target will generally be suboptimal for the supervised learning problem.
:::

Say something about Recommender Systems

### Reinforcement Learning

### Non-Tabular Data

## Commentary

When machine learning began to take off, it seemed most of the field of statistics sat on their laurels, and often scoffed at these techniques that didn't bother to test their assumptions[^riprip]! ML was, after all, mostly just a rehash of old ideas right? But the machine learning community was able to make great strides in predictive performance, and the application of machine learning continues to enable us to push the boundaries of what is possible. Statistics wasn't going to provide ChatGPT or self-driving cars, but it remains vitally important whenever we need to understand the uncertainty of our predictions, or when we need to make inferences about the world. The field of statistics has much to learn from the machine learning community, and vice versa. The two fields are not mutually exclusive, and the best data scientists will be able to draw from both. A more general field of **data science** became the way people used statistics *and* machine learning to solve their data challenges. In the end, use the best tool for the job, worry less about what it's called or whether it's the hot thing right now, and have fun!


[^riprip]: To paraphrase provocatively, 'machine learning is statistics minus any checking of models and assumptions'. Brian D. Ripley useR! 2004, Vienna (May 2004)  Want to know what's even crazier than that statement? It was said by the guy that literally wrote the book on neural networks before anyone   was even using them! Pattern Recognition and Neural Networks.


## Using R and Python in ML

### Python

Python is the king of ML

Pros:

- powerful tools all in one library
- much more efficient on memory and faster
- many other packages try to use the sklearn API[^sklearnapi]
- easy pipeline/reproducibility setup

[^sklearnapi]: Note to developers, just having a fit and predict method is not an API.


Cons:

- everything beyond prediction is like pulling teeth: plotting, extracting key estimated parameters
- Python ML developers just figured out recently that keeping feature names attached to the model object was a useful thing to do, so you can imagine the other sorts of functionality that are still missing
- Depending on packages used, documentation is generally bad to nonexistent, even for some important model aspects of the model, and non-standard. Demos may work or not, and you may have to dig into the source code to figure out what's actually going on.  This hopefully will be alleviated in the future with modern AI tools.

### R

R is actually great at ML

Pros:

- easy to use objects that contain the things you'd need to use for further processing
- saving models does not require any special effort
- easy post-processing with many packages designed to work with the output of other modeling packages (e.g. broom, tidybayes, etc.)
- Documentation is standardized for any CRAN and most non-CRAN packages, and will only improve with AI tools.

Cons:

- relativley slow
- memory intensive
- pipeline/reproducibility has only recently been of focus
  - tidymodels is a great but very non-standard way of doing things
  - mlr3 is much more sklearn-like fast and memory efficient
- developers don't do enough testing




## Refs

ridge as Bayesian  WIKILINK: https://en.wikipedia.org/wiki/Ridge_regression#Bayesian_interpretation

dropout
https://d2l.ai/chapter_multilayer-perceptrons/dropout.html

bv tradeoff

https://hastie.su.domains/Papers/ESLII.pdf

https://machinelearningmastery.com/gentle-introduction-to-the-bias-variance-trade-off-in-machine-learning/

RF/boosting
https://developers.google.com/machine-learning/decision-forests